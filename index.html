<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/favicon_16.png?v=2.6.1" type="image/png" sizes="16x16"><link rel="icon" href="/images/favicon_32.png?v=2.6.1" type="image/png" sizes="32x32"><meta name="google-site-verification" content="O5CNgi37yYXs3qQp7Xz61oL_AmGiwM28d7hRt5yh2to"><meta name="baidu-site-verification" content="pnKVynCWMP"><meta name="description" content="记录技术成长路上的点点滴滴">
<meta property="og:type" content="website">
<meta property="og:title" content="指  间">
<meta property="og:url" content="https://plotor.github.io/index.html">
<meta property="og:site_name" content="指  间">
<meta property="og:description" content="记录技术成长路上的点点滴滴">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="zhenchao">
<meta property="article:tag" content="王振超, 字节, 字节跳动, 小米, 阿里, 阿里巴巴, 武汉大学, 指间, 指间生活">
<meta name="twitter:card" content="summary"><title>指  间</title><link ref="canonical" href="https://plotor.github.io/index.html"><link rel="alternate" href="/atom.xml" type="application/atom+xml"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"carbon","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: {"avoidBanner":false},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user-circle"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-search"><span class="header-nav-search__icon"><i class="fas fa-search"></i></span><span class="header-nav-search__text">搜索</span></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">指  间</div><div class="header-banner-info__subtitle">记录技术成长路上的点点滴滴</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><div class="sticky-top" data-popover="置顶文章" data-popover-pos="up"><span class="sticky-top__icon"><i class="fas fa-thumbtack"></i></span></div><h1 class="post-title"><a class="post-title__link" href="/2020/04/23/java/theia/">Theia：可扩展的注解式配置注入组件</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-04-23</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">3.7k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">18分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>Theia 是一个 java 语言编写的，支持自定义扩展的注解式配置加载与注入组件，旨在以注解的方式加载任何可以被表示成 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://docs.oracle.com/cd/E23095_01/Platform.93/ATGProgGuide/html/s0204propertiesfileformat01.html">Properties</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 对象的配置，并注入给目标对象，同时支持当配置内容发生变更时回调更新。配置文件的来源可以是本地文件、网络，以及第三方配置系统。Theia 默认支持从 ClassPath 加载本地配置文件，并支持以 SPI 的方式扩展以支持更多的配置来源，例如从 ZK 加载配置等。</p>
<p>特性一览：</p>
<ul>
<li>支持以注解的方式加载多种配置数据源，并注入给配置对象。</li>
<li>支持预注入，预注入会校验配置的合法性，如果不合法则会放弃注入，避免配置出错影响服务的正常运行。</li>
<li>支持配置变更时回调更新，默认关闭，并允许用户配置是否启用。</li>
<li>内置基本类型转换器，用于将 String 类型配置项转换成目标类型对象。</li>
<li>支持自定义类型转换器，以实现一些定制化的类型转换。</li>
<li>支持以原生字符串或 Properties 对象的形式注入。</li>
<li>支持监听注入过程（InjectEventListener）和更新过程（UpdateEventListener）。</li>
<li>支持加载系统环境变量，并注入给配置对象。</li>
<li>支持 <code>${}</code> 占位符替换，使用指定的配置项替换占位符。</li>
<li>支持以 SPI 的方式扩展以支持更多类型的配置数据源。</li>
<li>对于 Spring 应用，支持自动扫描、加载并初始化配置对象。</li>
</ul></div><div class="post-readmore"><a class="post-readmore__link" href="/2020/04/23/java/theia/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2025/03/31/ray/triton-inference-server/">Ray Serve 集成 Triton Inference Server 构建模型在线推理服务</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-03-31</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">2.8k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">13分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>Triton Inference Server（下文简称 Triton）是 Nvidia 推出的高性能推理服务器，支持多种模型框架和硬件平台，关于 Triton 的更多内容可以参考 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/introduction/index.html">官方文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>。Triton 与 Ray Serve 在功能定位方面存在相同之处，不过二者也各有优势：</p>
<ul>
<li><p>Ray Serve 原生支持以分布式模式运行，可在单一应用中编排并部署多个模型，以满足复杂推理场景的需求。同时，其内置的弹性伸缩特性能够有效平衡用户请求与资源开销。</p>
</li>
<li><p>Triton 主要专注于单机推理场景，兼容多种硬件平台。通过引入动态批处理、多 GPU 并行推理等技术以提升模型推理性能，同时支持以 Backend 插件形式集成多种模型框架，并对外提供统一的接入 API。</p>
</li>
</ul>
<p>如下图所示，通过将 Ray Serve 与 Triton 集成能够让二者的优势形成互补：</p>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2025/ray-serve-triton-infer-server.png" alt="image">
      </p>
<p>在集成模式下，Ray Serve 主要承担流量承接、负载均衡、模型编排以及弹性伸缩等职责；而 Triton 则作为单机版的模型推理服务，具备模型加载、推理及加速等功能。本文首先介绍如何基于 Triton 部署大语言模型，然后介绍如何将 Ray Serve 与 Triton 集成，使用的环境信息如下：</p>
<blockquote>
<ul>
<li>镜像：nvcr.io/nvidia/tritonserver:25.02-trtllm-python-py3</li>
<li>模型：Llama-3.2-1B-Instruct</li>
<li>显卡：Nvidia A10 (24GB), CUDA 12.2</li>
</ul>
</blockquote>

        <h2 id="基于-Triton-部署单机版在线推理服务">
          <a href="#基于-Triton-部署单机版在线推理服务" class="heading-link"><i class="fas fa-link"></i></a>基于 Triton 部署单机版在线推理服务</h2>
      <p>Triton 支持多种类型的 Backend，这里我们以 TensorRT-LLM Backend 为例。要让 Triton 能够部署 Llama 模型，主要包含如下两步操作：</p>
<ol>
<li><p>编译模型文件生成 TensorRT 引擎，利用 TensorRT-LLM Backend 实现与 Triton 集成。</p>
</li>
<li><p>通过 TensorRT-LLM Backend 内置的模型集成配置模板对 Triton 模型仓库进行配置，以便 Triton 能够识别并部署该模型。</p>
</li>
</ol>
<p>简单起见，这里我们基于 Nvidia 提供的镜像创建 Docker 容器进行操作，以避免复杂的 CUDA 驱动安装，以及一系列环境配置等。为了让 Docker 容器能够访问宿主机 GPU 资源，你需要参考 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">官方文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 安装配置 NVIDIA Container Toolkit，然后基于 <code>nvcr.io/nvidia/tritonserver:25.02-trtllm-python-py3</code> 镜像创建并进入 Docker 容器：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it \</span><br><span class="line">  --name triton-infer-server \</span><br><span class="line">  --gpus all \</span><br><span class="line">  --ipc=host \</span><br><span class="line">  -p8000:8000 -p8001:8001 -p8002:8002 -p:8080:8080 -p8265:8265 \</span><br><span class="line">  -v /opt/workspace:/opt/workspace \</span><br><span class="line">  -w /opt/workspace \</span><br><span class="line">  -d nvcr.io/nvidia/tritonserver:25.02-trtllm-python-py3 \</span><br><span class="line">  /bin/bash</span><br><span class="line"></span><br><span class="line">$ docker <span class="built_in">exec</span> -it 859b7c7d53dd /bin/bash</span><br></pre></td></tr></tbody></table></div></figure>

        <h3 id="编译模型">
          <a href="#编译模型" class="heading-link"><i class="fas fa-link"></i></a>编译模型</h3>
      <p>以 <code>Llama-3.2-1B-Instruct</code> 模型为例，参考 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama">官方文档</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 将其转换成 TensorRT 引擎格式：</p>
<ul>
<li><p>脚本 <code>convert_checkpoint.py</code> 用于将 HF 权重转换为 TensorRT-LLM 检查点。</p>
</li>
<li><p>命令 <code>trtllm-build</code> 用于从 TensorRT-LLM 检查点构建 TensorRT-LLM 引擎。</p>
</li>
</ul>
<p>操作示例：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/workspace/nvidia &amp;&amp; git <span class="built_in">clone</span> https://github.com/NVIDIA/TensorRT-LLM.git</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ./TensorRT-LLM &amp;&amp; pip install --upgrade -r requirements.txt</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ./examples/llama</span><br><span class="line"></span><br><span class="line">python convert_checkpoint.py \</span><br><span class="line">  --model_dir /opt/workspace/models/llama/Llama-3.2-1B-Instruct \</span><br><span class="line">  --output_dir /opt/workspace/models/llama/tllm_checkpoint_1gpu_tp1 \</span><br><span class="line">  --dtype float16 \</span><br><span class="line">  --tp_size 1</span><br><span class="line"></span><br><span class="line">trtllm-build \</span><br><span class="line">  --checkpoint_dir /opt/workspace/models/llama/tllm_checkpoint_1gpu_tp1 \</span><br><span class="line">  --output_dir /opt/workspace/models/llama/trt_engines \</span><br><span class="line">  --gemm_plugin auto</span><br></pre></td></tr></tbody></table></div></figure>
<p>这一步完成后将在 <code>trt_engines</code> 目录下生成如下 2 个文件：</p>
<ul>
<li><p><code>rank0.engine</code>：该文件包含嵌入模型权重的可执行操作图。</p>
</li>
<li><p><code>config.json</code>：该文件包含模型的详细信息，例如结构、精度，以及引擎中集成的插件列表等。</p>
</li>
</ul>
<p>此时，我们可以通过 TensorRT-LLM 提供的脚本进行推理验证：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/workspace/nvidia/TensorRT-LLM/examples/llama</span><br><span class="line"></span><br><span class="line">python ../run.py \</span><br><span class="line">  --input_text <span class="string">"What is Nvidia Triton Inference Server"</span> \</span><br><span class="line">  --max_output_len=1024 \</span><br><span class="line">  --tokenizer_dir /opt/workspace/models/llama/Llama-3.2-1B-Instruct \</span><br><span class="line">  --engine_dir=/opt/workspace/models/llama/trt_engines</span><br></pre></td></tr></tbody></table></div></figure>
<p>没有问题的话，上述请求会返回模型的推理结果。</p>

        <h3 id="部署模型">
          <a href="#部署模型" class="heading-link"><i class="fas fa-link"></i></a>部署模型</h3>
      <p>TensorRT-LLM Backend 内置了模型集成配置模板以简化模型的集成操作（位于 <code>all_models/inflight_batcher_llm</code> 目录，内容如下），模板主要包含 4 个模块，分别对应模型执行过程的不同阶段。</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">|-- ensemble</span><br><span class="line">|   |-- 1</span><br><span class="line">|   `-- config.pbtxt</span><br><span class="line">|-- postprocessing</span><br><span class="line">|   |-- 1</span><br><span class="line">|   |   `-- model.py</span><br><span class="line">|   `-- config.pbtxt</span><br><span class="line">|-- preprocessing</span><br><span class="line">|   |-- 1</span><br><span class="line">|   |   `-- model.py</span><br><span class="line">|   `-- config.pbtxt</span><br><span class="line">`-- tensorrt_llm</span><br><span class="line">    |-- 1</span><br><span class="line">    |   |-- config.json</span><br><span class="line">    |   |-- model.py</span><br><span class="line">    |   `-- rank0.engine</span><br><span class="line">    `-- config.pbtxt</span><br></pre></td></tr></tbody></table></div></figure>
<p>说明：</p>
<ul>
<li><p>模块 preprocessing 包含用于对输入进行 tokenizing 的脚本，支持将用户输入的 prompt 字符串转换成 input_id 列表。</p>
</li>
<li><p>模块 postprocessing 包含用于对输出进行 de-tokenizing 的脚本，支持将模型输出的 output_id 列表转换成用户能够理解的字符串。</p>
</li>
<li><p>模块 tensorrt_llm 包含用户编译生成的模型文件，负载加载并调用用户自定义模型完成推理操作。</p>
</li>
<li><p>模块 ensemble  用于将 preprocessing、tensorrt_llm 和 postprocessing 模块串联在一起，指导 Triton 如何在这些模块之间传输数据以构建一个完整的推理流程。</p>
</li>
</ul>
<p>你可以从 Github 下载模型仓库配置模板，并将前面编译得到的模型文件拷贝到模型仓库中：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/workspace/nvidia &amp;&amp; git <span class="built_in">clone</span> https://github.com/triton-inference-server/tensorrtllm_backend.git</span><br><span class="line"></span><br><span class="line">cp -r ./tensorrtllm_backend/all_models/inflight_batcher_llm/* /opt/workspace/models/llama/triton</span><br><span class="line"></span><br><span class="line">cp /opt/workspace/models/llama/trt_engines/* /opt/workspace/models/llama/triton/tensorrt_llm/1</span><br></pre></td></tr></tbody></table></div></figure>
<p>然后执行如下命令修改相关配置：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">TOKENIZER_DIR=/opt/workspace/models/llama/Llama-3.2-1B-Instruct</span><br><span class="line">TOKENIZER_TYPE=auto</span><br><span class="line">ENGINE_DIR=/opt/workspace/models/llama/triton/tensorrt_llm/1</span><br><span class="line">DECOUPLED_MODE=<span class="literal">false</span></span><br><span class="line">MODEL_FOLDER=/opt/workspace/models/llama/triton</span><br><span class="line">MAX_BATCH_SIZE=4</span><br><span class="line">INSTANCE_COUNT=1</span><br><span class="line">MAX_QUEUE_DELAY_MS=10000</span><br><span class="line">TRITON_BACKEND=tensorrtllm</span><br><span class="line">LOGITS_DATATYPE=<span class="string">"TYPE_FP32"</span></span><br><span class="line">FILL_TEMPLATE_SCRIPT=/opt/workspace/nvidia/tensorrtllm_backend/tools/fill_template.py</span><br><span class="line">python3 <span class="variable">${FILL_TEMPLATE_SCRIPT}</span> -i <span class="variable">${MODEL_FOLDER}</span>/preprocessing/config.pbtxt tokenizer_dir:<span class="variable">${TOKENIZER_DIR}</span>,tokenizer_type:<span class="variable">${TOKENIZER_TYPE}</span>,triton_max_batch_size:<span class="variable">${MAX_BATCH_SIZE}</span>,preprocessing_instance_count:<span class="variable">${INSTANCE_COUNT}</span></span><br><span class="line">python3 <span class="variable">${FILL_TEMPLATE_SCRIPT}</span> -i <span class="variable">${MODEL_FOLDER}</span>/postprocessing/config.pbtxt tokenizer_dir:<span class="variable">${TOKENIZER_DIR}</span>,tokenizer_type:<span class="variable">${TOKENIZER_TYPE}</span>,triton_max_batch_size:<span class="variable">${MAX_BATCH_SIZE}</span>,postprocessing_instance_count:<span class="variable">${INSTANCE_COUNT}</span></span><br><span class="line">python3 <span class="variable">${FILL_TEMPLATE_SCRIPT}</span> -i <span class="variable">${MODEL_FOLDER}</span>/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:<span class="variable">${MAX_BATCH_SIZE}</span>,decoupled_mode:<span class="variable">${DECOUPLED_MODE}</span>,bls_instance_count:<span class="variable">${INSTANCE_COUNT}</span>,logits_datatype:<span class="variable">${LOGITS_DATATYPE}</span></span><br><span class="line">python3 <span class="variable">${FILL_TEMPLATE_SCRIPT}</span> -i <span class="variable">${MODEL_FOLDER}</span>/ensemble/config.pbtxt triton_max_batch_size:<span class="variable">${MAX_BATCH_SIZE}</span>,logits_datatype:<span class="variable">${LOGITS_DATATYPE}</span></span><br><span class="line">python3 <span class="variable">${FILL_TEMPLATE_SCRIPT}</span> -i <span class="variable">${MODEL_FOLDER}</span>/tensorrt_llm/config.pbtxt triton_backend:<span class="variable">${TRITON_BACKEND}</span>,triton_max_batch_size:<span class="variable">${MAX_BATCH_SIZE}</span>,decoupled_mode:<span class="variable">${DECOUPLED_MODE}</span>,engine_dir:<span class="variable">${ENGINE_DIR}</span>,max_queue_delay_microseconds:<span class="variable">${MAX_QUEUE_DELAY_MS}</span>,batching_strategy:inflight_fused_batching,encoder_input_features_data_type:TYPE_FP16,logits_datatype:<span class="variable">${LOGITS_DATATYPE}</span></span><br></pre></td></tr></tbody></table></div></figure>
<p>如果需要手动编辑 CPU 核数可以修改 <code>config.pbtxt</code> 配置：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">instance_group [</span><br><span class="line">  {</span><br><span class="line">    count: 8</span><br><span class="line">    kind : KIND_CPU</span><br><span class="line">  }</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></div></figure>
<p>需要注意的是这个参数不宜设置过大，否则可能导致服务拉不起来报 OOM 错误：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[TensorRT-LLM][ERROR] [engine.cpp::readEngineFromArchive::1093] Error Code 2: OutOfMemory (Requested size was 996311552 bytes.)</span><br></pre></td></tr></tbody></table></div></figure>
<p>启动 Triton Inference Server：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/workspace/nvidia/tensorrtllm_backend</span><br><span class="line"></span><br><span class="line">python scripts/launch_triton_server.py \</span><br><span class="line">  --model_repo /opt/workspace/models/llama/triton \</span><br><span class="line">  --world_size 1</span><br></pre></td></tr></tbody></table></div></figure>
<p>启动成功将会在最后打印：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I0314 12:05:57.080618 17817 grpc_server.cc:2558] "Started GRPCInferenceService at 0.0.0.0:8001"</span><br><span class="line">I0314 12:05:57.080826 17817 http_server.cc:4725] "Started HTTPService at 0.0.0.0:8000"</span><br><span class="line">I0314 12:05:57.122329 17817 http_server.cc:358] "Started Metrics Service at 0.0.0.0:8002"</span><br></pre></td></tr></tbody></table></div></figure>
<p>此时可以通过 POST 请求验证：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">curl --location <span class="string">'http://127.0.0.1:8000/v2/models/ensemble/generate'</span> \</span><br><span class="line">--header <span class="string">'Content-Type: application/json'</span> \</span><br><span class="line">--data <span class="string">'{</span></span><br><span class="line"><span class="string">  "text_input": "介绍一下 Triton Inference Server",</span></span><br><span class="line"><span class="string">  "parameters": {</span></span><br><span class="line"><span class="string">    "max_tokens": 256,</span></span><br><span class="line"><span class="string">    "bad_words": [</span></span><br><span class="line"><span class="string">      ""</span></span><br><span class="line"><span class="string">    ],</span></span><br><span class="line"><span class="string">    "stop_words": [</span></span><br><span class="line"><span class="string">      ""</span></span><br><span class="line"><span class="string">    ]</span></span><br><span class="line"><span class="string">  }</span></span><br><span class="line"><span class="string">}'</span></span><br></pre></td></tr></tbody></table></div></figure>

        <h2 id="引入-Ray-Serve-构建分布式在线推理服务">
          <a href="#引入-Ray-Serve-构建分布式在线推理服务" class="heading-link"><i class="fas fa-link"></i></a>引入 Ray Serve 构建分布式在线推理服务</h2>
      <p>Ray Serve 是依托于 Ray 分布式计算框架之上构建的模型在线服务库，提供基于给定模型构建高性能、低延迟在线推理服务的能力。基于 Ray Serve 构建在线推理服务具有如下优势：</p>
<ul>
<li><p>__异构计算__：支持根据模型对资源的需求，灵活地将模型部署并运行于 CPU 或 GPU 类型的节点上。</p>
</li>
<li><p>__弹性伸缩__：支持依据请求负载，在预设范围内自动调整集群规模，以平衡推理性能与资源开销。火山 EMR Ray 在社区 Autoscale 能力的基础上进一步优化，支持自定义扩缩容指标和策略。</p>
</li>
<li><p>__失败容错__：支持多副本部署以抵御单副本故障，同时提供针对系统级、应用级的故障恢复能力，当副本或节点出现故障时，可通过重试、重调度等方式确保服务持续运行。</p>
</li>
<li><p>__多模型融合__：支持在单个 Ray Serve 应用中编排部署多个模型，这些模型可以独立或组合对外提供服务，同时针对各个模型可以独立配置资源调度、弹性伸缩等。</p>
</li>
<li><p>__动态批处理__：支持按照请求数量和时间跨度对多个请求进行合并，然后批量发送给模型进行处理，通过借助硬件（如 GPU）的并行计算能力，能够在显著提升吞吐量的同时，尽量维持低延迟。</p>
</li>
<li><p>__模型多路复用__：针对需加载大量模型的应用场景（例如个性化推荐），允许单个应用加载多个模型并向外部提供服务，然而出于资源考量，也会对应用内单副本可加载的模型在数量上加以限制。同时，Ray Serve 会尽可能将请求路由至已完成模型加载的副本进行处理，以平衡推理性能与资源消耗。</p>
</li>
<li><p>__支持原生集成 vLLM 实现推理加速__：提供与 vLLM 引擎的原生集成能力，通过简单配置即可复用 vLLM 在推理场景中引入的 PagedAttention、动态批处理等多项优化技术，以提升模型推理性能和资源利用率。</p>
</li>
<li><p>__支持集成 Nvidia Triton Inference Server 实现优势互补__：Triton 针对单机推理场景进行了大量的优化以提升性能，但生产化部署还需要具备负载均衡、弹性伸缩、失败容错，以及模型编排等多方面的能力，通过集成 Triton，能够实现双方优势的有效互补。在集成形态下，Triton 作为单机版的模型推理服务，而 Ray Serve 则承担上层路由与管理职责，提供整体的请求路由、负载均衡、多模型编排，以及弹性伸缩等能力。</p>
</li>
<li><p>简洁的编程模型便于进行本地开发与调试，仅需进行简单修改便可进行线上部署。</p>
</li>
<li><p>Python Native。</p>
</li>
</ul>
<p>在分布式在线推理场景下，通过将 Ray Serve 与 Triton 进行集成，能够在发挥 Triton 在单机场景固有优势的同时，为其补齐在分布式场景下的不足。Triton 提供了 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/client_guide/python.html">In-Process Python API</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 用于支持 Python 生态与 Triton 进行集成，相较于发送 HTTP 请求的方式更为便捷。下面的示例通过 Triton In-Process Python API 实现了 Ray Serve 与 Triton 的集成：</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ctypes</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Iterable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tritonserver</span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"><span class="keyword">from</span> ray <span class="keyword">import</span> serve</span><br><span class="line"></span><br><span class="line">api = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@serve.deployment(<span class="params"></span></span></span><br><span class="line"><span class="meta"><span class="params">    ray_actor_options=<span class="built_in">dict</span>(<span class="params"></span></span></span></span><br><span class="line"><span class="meta"><span class="params"><span class="params">        num_gpus=<span class="number">1</span></span></span></span></span><br><span class="line"><span class="meta"><span class="params"><span class="params">    </span>)</span></span></span><br><span class="line"><span class="meta"><span class="params"></span>)</span></span><br><span class="line"><span class="meta">@serve.ingress(<span class="params">api</span>)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, model_path: <span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="keyword">import</span> torch</span><br><span class="line">        <span class="keyword">assert</span> torch.cuda.is_available(), RuntimeError(<span class="string">"CUDA is not available"</span>)</span><br><span class="line"></span><br><span class="line">        self._server = tritonserver.Server(</span><br><span class="line">            tritonserver.Options(</span><br><span class="line">                model_repository=model_path,</span><br><span class="line">                log_info=<span class="literal">True</span>,</span><br><span class="line">                log_warn=<span class="literal">True</span>,</span><br><span class="line">                log_error=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        self._server.start(wait_until_ready=<span class="literal">True</span>)</span><br><span class="line">        self._model = self._server.model(<span class="string">"ensemble"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @api.get(<span class="params"><span class="string">"/generate"</span></span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, query: <span class="built_in">str</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self._model.ready():</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">"Model is not ready, Please try again later."</span>)</span><br><span class="line"></span><br><span class="line">        resp = <span class="built_in">list</span>(self._model.infer(inputs={</span><br><span class="line">            <span class="string">"text_input"</span>: [[query]],</span><br><span class="line">            <span class="string">"max_tokens"</span>: np.array([[<span class="number">1024</span>]], dtype=np.int32)</span><br><span class="line">        }))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._to_string(resp.outputs[<span class="string">"text_output"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_to_string</span>(<span class="params">self, tensor: tritonserver.Tensor</span>) -&gt; str:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        This method is copied from https://github.com/triton-inference-server/server</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">app = Generator.bind(<span class="string">"/opt/workspace/models/llama/triton"</span>)</span><br></pre></td></tr></tbody></table></div></figure>
<p>注意：</p>
<ol>
<li><p>由于 ensemble 模块是整个推理模型流程对外接口，因此加载的模型名称应该是 ensemble，而不是 tensorrt_llm。</p>
</li>
<li><p>对于需要 GPU 资源的模型，必须配置 <code>num_gpus</code> 参数，否则 Ray Serve 并不会将 Deployment 调度部署到 GPU 节点上，从而导致 Triton 加载失败。</p>
</li>
</ol>
<p>通过执行 <code>serve deploy</code> 命令，可将上述 Ray Serve 应用部署至目标 Ray Cluster 集群。应用成功启动后，可通过以下 POST 请求向模型发送推理请求：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">curl --location <span class="string">'http://127.0.0.1:8000/v1/chat/completions'</span> \</span><br><span class="line">--header <span class="string">'Content-Type: application/json'</span> \</span><br><span class="line">--data <span class="string">'{</span></span><br><span class="line"><span class="string">  "model": "meta-llama/Llama-3.2-1B-Instruct",</span></span><br><span class="line"><span class="string">  "messages": [</span></span><br><span class="line"><span class="string">    {</span></span><br><span class="line"><span class="string">      "role": "user",</span></span><br><span class="line"><span class="string">      "content": "介绍一下 Ray Serve 集成 Triton Inference Server 的优势"</span></span><br><span class="line"><span class="string">    }</span></span><br><span class="line"><span class="string">  ]</span></span><br><span class="line"><span class="string">}'</span></span><br></pre></td></tr></tbody></table></div></figure>

        <h2 id="参考">
          <a href="#参考" class="heading-link"><i class="fas fa-link"></i></a>参考</h2>
      <ul>
<li><span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/introduction/index.html">NVIDIA Triton Inference Server</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/turbocharging-meta-llama-3-performance-with-nvidia-tensorrt-llm-and-nvidia-triton-inference-server/">Turbocharging Meta Llama 3 Performance with NVIDIA TensorRT-LLM and NVIDIA Triton Inference Server</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2020/06/29/sofa/sofa-jraft-linearizable-read/">SOFA-JRaft 源码解析：线性一致性读</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-06-29</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">4.3k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">18分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>关于线性一致性读的定义，简单而言就是在 T 时刻执行写入操作，那么在 T 时刻之后一定能够读取到之前写入的值。Raft 算法能够至少保证集群节点数据的最终一致性，也就说就某一特定时刻而言各个节点之间的数据状态允许存在滞后。在分布式场景下如果允许各个节点随机响应用户的读请求，则可能会读取到脏数据。如果希望基于 Raft 算法实现线性一致性读语义，最简单的方式就是将读操作作为一个指令提交给 Raft 集群，由于 Raft 算法能够保证指令执行的顺序性，所以该读操作指令一定能够读取到在此之前写入的值（本文将此类线性一致性读策略命名为 RaftLog Read）。然而，RaftLog Read 的缺点也是显而易见的，每次读操作都需要走一遍完整的 Raft 算法流程势必效率低下，并且大部分的应用场景都具备读多写少的特征，所以该策略势必会让 Raft 集群产生大量的日志文件，增加磁盘和网络的开销。</p>
<p>换一个角度思考，在 Raft 算法中更新操作都由 Leader 节点负责响应，那么极端一点每次都从 Leader 节点读数据是不是就万事大吉了呢？先不说这种方式将一个分布式系统退化成了单机系统，我们还需要考虑下面两个问题：</p>
<ul>
<li>日志数据从提交到被业务状态机所应用这中间存在一定的时间滞后性，所以直接执行读操作不一定能够读取到最新的数据。</li>
<li>当前 Leader 节点不一定是有效的，因为 Leader 节点的变更通常有一个时间差，而这中间存在导致脏读的可能性。</li>
</ul></div><div class="post-readmore"><a class="post-readmore__link" href="/2020/06/29/sofa/sofa-jraft-linearizable-read/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2020/06/22/sofa/sofa-jraft-snapshot/">SOFA-JRaft 源码解析：快照机制</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-06-22</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">6.4k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">33分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>上一篇我们介绍了 JRaft 关于日志复制机制的设计与实现，其中提到了快照机制本质上也是一种对日志数据复制的优化手段，本文我们就对 JRaft 关于快照机制的设计与实现展开分析。在开始之前我们先聊聊为什么需要引入快照机制，思考以下两个问题：</p>
<ol>
<li>因为日志数据需要落盘存储，当日志数据量大到磁盘空间无法容纳时，除了扩容是否还有其它的优化手段？</li>
<li>当一个新的节点加入 Raft 集群时需要重放集群之前接收到的所有指令以追赶上集群的数据状态，这一过程往往比较耗时和消费带宽，如何进行优化？</li>
</ol>
<p>对于一个生产级别的 Raft 算法库而言必须能够解决好上述问题，而 Raft 算法也为解决上述问题提供了思路，即快照机制。该机制通过定期为本地的数据状态生成对应的快照文件，并删除对应的日志文件，从而降低对于磁盘空间的容量消耗。当一个新的节点加入集群时，不同于从 Leader 节点复制集群在此之前的所有日志文件，基于快照机制该节点只需要从 Leader 节点下载安装最新的快照文件即可。由于快照文件是对某一时刻数据状态的备份，相对于原生日志数据而言在容量上要小很多，所以既降低了本地磁盘空间占用，也降低了新加入节点从 Leader 节点同步历史数据的时间和网络开销，很好的解决了上面抛出的两个问题。</p></div><div class="post-readmore"><a class="post-readmore__link" href="/2020/06/22/sofa/sofa-jraft-snapshot/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2020/06/15/sofa/sofa-jraft-log-replication/">SOFA-JRaft 源码解析：日志复制机制</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-06-15</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">15.8k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">78分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>与上一篇介绍的主节点选举一样，日志复制（Log Replication）同样是 Raft 算法的核心组成部分，是支撑 Raft 节点达成共识的基础。Raft 中的日志主要可以分为两类：一类是协议自身运行所生成的日志，例如集群节点配置变更信息；另外一类就是用户向集群提交的指令所生成的日志。为了让集群中的各个节点达成共识，Leader 节点需要将日志数据复制给集群中的各个节点，并采用投票机制让这些节点决定是否许可日志对应的操作。对于被许可的操作日志，各个节点会严格按照相同的顺序在本地进行存储，并重放日志对应的操作，以此实现节点之间的共识。</p>
<p>JRaft 在设计和实现层面为每个 Follower 和 Learner 节点都绑定了一个复制器 Replicator 实例，由 Replicator 负责向目标节点复制日志数据，Replicator 实例之间彼此相互隔离，互不影响，并由 ReplicatorGroup 进行统一管理。日志复制需要涉及到集群中节点之间的频繁通信和数据传输，所以需要保证复制操作的高性能，并且不允许出现乱序和断层。为此，JRaft 引入了多种优化策略，包括：Follower 节点之间并发复制、批量发送，以及 Pipeline 机制等。</p></div><div class="post-readmore"><a class="post-readmore__link" href="/2020/06/15/sofa/sofa-jraft-log-replication/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2020/06/08/sofa/sofa-jraft-leader-election/">SOFA-JRaft 源码解析：主节点选举机制</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-06-08</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">9.8k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">46分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>主节点选举（Leader Election）是 Raft 算法的核心组成部分，也是 Raft 算法库的主要应用场景之一。Raft 算法设计了 term 和 logIndex 两个属性，分别用于表示 Leader 节点的任期，以及集群运行期间接收到的指令对应的日志条目的 ID，这两个属性都是单调递增的。一个 Leader 节点在任期内会始终向其管理的所有 Follower 节点宣示主权，以避免这些 Follower 节点发动革命，推翻自己的政权，成为新的 Leader 节点。然而，世事无常，如果 Leader 节点因为某些原因不能或未能即时向某些 Follower 节点宣示自己的主权，则这些 Follower 节点在等待一段随机的时间之后就会尝试竞选成为新的 Leader 节点。</p>
<p>之所以这里采用随机化的等待时间，是为了避免两个或多个 Follower 节点同时发起选举进程，进而出现这些节点都没有赢得过半数的选票。于是，这些节点又在同一时间发起下一轮选举进程，延长了集群无 Leader 节点的时间，而通过随机化各个 Follower 节点等待的时间则能够很好的解决此类问题。</p></div><div class="post-readmore"><a class="post-readmore__link" href="/2020/06/08/sofa/sofa-jraft-leader-election/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2020/06/01/sofa/sofa-jraft-node-startup/">SOFA-JRaft 源码解析：节点的启动过程</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-06-01</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">10.3k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">52分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>在《<a href="/2020/01/01/protocol/raft/">理解 Raft 分布式共识算法</a>》一文中，我们对于 Raft 算法的理论进行了介绍。过去几年，围绕 Raft 算法涌现出了一系列各类语言的实现（参考 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://raft.github.io/#implementations">Raft 算法官网</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>），这也充分证明了该算法相对于 Paxos 算法在理解和实现层面的友好性。从本文开始，我就以 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.sofastack.tech/projects/sofa-jraft/overview/">SOFA-JRaft</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 为例，用多篇文章来分析一个生产级别的 Raft 算法应该如何实现。</p>
<p>SOFA-JRaft 是一个基于 Raft 算法的 java 语言实现算法库，提供生产级别的稳定性、容错性，以及高性能，支持 MULTI-RAFT-GROUP，适用于高负载低延迟的场景。</p></div><div class="post-readmore"><a class="post-readmore__link" href="/2020/06/01/sofa/sofa-jraft-node-startup/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2020/01/01/protocol/raft/">理解 Raft 分布式共识算法</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2020-01-01</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">7.3k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">26分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>Raft 算法是一类基于日志复制的分布式共识算法，旨在提供与 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Multi-Paxos</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 共识算法相同的容错性和性能的前提下，追求更好的可理解性和工程可实现性。Paxos 算法为分布式系统面临的共识问题提供了解决思路，但其难以理解的特性一直被大家所诟病，更不用说工程实现，这也是 Raft 算法诞生的主要动因。</p>
<p>Raft 算法的作者认为可理解性和工程可实现性也是一个优秀分布式共识算法所应该具备的特性，并通过以下两点保证算法的可理解性：</p>
<ul>
<li><strong>问题分解</strong> ：将分布式共识问题拆分成主节点选举、日志复制、安全点，以及成员变更 4 个独立子问题逐一进行解决。</li>
<li><strong>简化状态</strong> ：通过增强某些阶段的一致性程度（例如约束能够成为下一任 Leader 的参选节点条件），以减少算法需要考虑的系统状态数量。</li>
</ul></div><div class="post-readmore"><a class="post-readmore__link" href="/2020/01/01/protocol/raft/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2019/06/26/kafka/kafka-controller/">Kafka 源码解析：集群协同运行控制器</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2019-06-26</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">20.7k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">102分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>Kafka 集群由一系列的 broker 节点构成，在这些 broker 节点中会选举一个节点成为所有 broker 节点的 leader（称之为 kafka controller），其余的 broker 节点均为 follower 角色。Kafka Controller 负责管理集群中所有 topic 分区和副本的状态，协调集群中所有 broker 节点的运行，同时也负责 Kafka 与 ZK 之间的交互，下文中如果不特殊说明，Kafka Controller 均指代 leader 角色。</p></div><div class="post-readmore"><a class="post-readmore__link" href="/2019/06/26/kafka/kafka-controller/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2019/06/25/kafka/kafka-group-coordinator/">Kafka 源码解析：Group 协调管理机制</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2019-06-25</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">13.6k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">64分</span></span></div></header><div class="post-body"><div class="post-excerpt"><p>在 Kafka 的设计中，消费者一般都有一个 group 的概念（当然，也存在不属于任何 group 的消费者），将多个消费者组织成一个 group 可以提升消息的消费处理能力，同时又能保证消息消费的顺序性，不重复或遗漏消费。一个 group 名下的消费者包含一个 leader 角色和多个 follower 角色，虽然在消费消息方面这两类角色是等价的，但是 leader 角色相对于 follower 角色还担负着管理整个 group 的职责。当 group 中有新的消费者加入，或者某个消费者因为一些原因退出当前 group 时，亦或是订阅的 topic 分区发生变化时，都需要为 group 名下的消费者重新分配分区，在服务端确定好分区分配策略之后，具体执行分区分配的工作则交由 leader 消费者负责，并在完成分区分配之后将分配结果反馈给服务端。</p></div><div class="post-readmore"><a class="post-readmore__link" href="/2019/06/25/kafka/kafka-group-coordinator/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article></section><nav class="paginator"><div class="paginator-inner"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/author.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">追求技术深度，注重文章质量</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/plotor" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://weibo.com/" target="_blank" rel="noopener" data-popover="微博" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weibo"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="微信" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weixin"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="QQ" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-qq"></i></span></a><a class="sidebar-ov-social-item" href="https://twitter.com/" target="_blank" rel="noopener" data-popover="Twitter" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-twitter"></i></span></a></div><div class="sidebar-ov-feed"><span class="sidebar-ov-feed-rss"><a class="sidebar-ov-feed-rss__link" href="/atom.xml" target="_blank" rel="noopener"><span class="sidebar-ov-feed-rss__icon"><i class="fas fa-rss"></i></span><span>RSS 订阅</span></a></span></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">96</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">29</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2015~2025</span><span class="footer__devider"></span><span>Zhenchao All Rights Reserved</span><span class="footer__devider">|</span><span>浙ICP备 16010916 号</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.3.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script>function initSearch() {
  var isXML = true;
  var search_path = 'search.json';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;
  $.ajax({
    url: path,
    dataType: isXML ? 'xml' : 'json',
    async: true,
    success: function (res) {
      var datas = isXML ? $('entry', res).map(function () {
        // 将 XML 转为 JSON
        return {
          title: $('title', this).text(),
          content: $('content', this).text(),
          url: $('url', this).text()
        };
      }).get() : res;
      var $input = $('.search-input input');
      var $result = $('.search-results');
      // 搜索对象（标题、内容）的权重，影响显示顺序
      var WEIGHT = { title: 100, content: 1 };
      var searchPost = function () {
        var searchText = $input.val().toLowerCase().trim();
        // 根据空白字符分隔关键字
        var keywords = searchText.split(/[\s]+/);
        // 搜索结果
        var matchPosts = [];

        // 有多个关键字时，将原文字整个保存下来
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        // 防止未输入字符时搜索
        if (searchText.length > 0) {
          datas.forEach(function (data) {
            var isMatch  = false;
            // 没有标题的文章使用预设的 i18n 变量代替
            var title = (data.title && data.title.trim()) || '[ 文章无标题 ]';
            var titleLower = title && title.toLowerCase();
            // 删除 HTML 标签 和 所有空白字符
            var content = data.content && data.content.replace(/<[^>]+>/g, '');
            var contentLower = content && content.toLowerCase();
            // 删除重复的 /
            var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');
            // 标题中匹配到的关键词
            var titleHitSlice = [];
            // 内容中匹配到的关键词
            var contentHitSlice = [];

            keywords.forEach(function (keyword) {
              /**
              * 获取匹配的关键词的索引
              * @param {String} keyword 要匹配的关键字
              * @param {String} text 原文字
              * @param {Boolean} caseSensitive 是否区分大小写
              * @param {Number} weight 匹配对象的权重。权重大的优先显示
              * @return {Array}
              */
              function getIndexByword (word, text, caseSensitive, weight) {
                if (!word || !text) {
                  return [];
                };

                var startIndex = 0; // 每次匹配的开始索引
                var index = -1;     // 匹配到的索引值
                var result = [];    // 匹配结果

                if (!caseSensitive) {
                  word = word.toLowerCase();
                  text = text.toLowerCase();
                }

                while((index = text.indexOf(word, startIndex)) !== -1) {
                  var hasMatch = false;
                  // 索引位置相同的关键词，保留长度较长的
                  titleHitSlice.forEach(function (hit) {
                    if (hit.index === index && hit.word.length < word.length) {
                      hit.word = word;
                      hasMatch = true;
                    }
                  });
                  startIndex = index + word.length;
                  !hasMatch && result.push({ index: index, word: word, weight: weight });
                }
                return result;
              }
              titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
              contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
            });

            var hitTitle = titleHitSlice.length;
            var hitContent = contentHitSlice.length;

            if (hitTitle > 0 || hitContent > 0) {
              isMatch = true;
            }
            if (isMatch) {
              ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                // 按照匹配文字的索引的递增顺序排序
                hit.sort(function (left, right) {
                  return left.index - right.index;
                });
              });
              /**
              * 给文本中匹配到的关键词添加标记，从而进行高亮显示
              * @param {String} text 原文本
              * @param {Array} hitSlice 匹配项的索引信息
              * @param {Number} start 开始索引
              * @param {Number} end 结束索引
              * @return {String}
              */
              function highlightKeyword (text, hitSlice, start, end) {
                if (!text || !hitSlice || !hitSlice.length) {
                  return;
                }

                var result = '';
                var startIndex = start;
                var endIndex = end;
                hitSlice.forEach(function (hit) {
                  if (hit.index < startIndex) {
                    return;
                  }

                  var hitWordEnd = hit.index + hit.word.length;
                  result += text.slice(startIndex, hit.index);
                  result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                  startIndex = hitWordEnd;
                });
                result += text.slice(startIndex, endIndex);
                return result;
              }

              var postData = {};
              // 文章总的搜索权重
              var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
              // 标记匹配关键词后的标题
              var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
              // 标记匹配关键词后的内容
              var postContent;
              // 显示内容的长度
              var SHOW_WORD_LENGTH = 200;
              // 命中关键词前的字符显示长度
              var SHOW_WORD_FRONT_LENGTH = 20;
              var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

              // 截取匹配的第一个字符，前后共 200 个字符来显示
              if (contentHitSlice.length > 0) {
                var firstIndex = contentHitSlice[0].index;
                var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                var end = firstIndex + SHOW_WORD_END_LENGTH;
                postContent = highlightKeyword(content, contentHitSlice, start, end);
              } else { // 未匹配到内容，直接截取前 200 个字符来显示
                postContent = content.slice(0, SHOW_WORD_LENGTH);
              }
              postData.title = postTitle;
              postData.content = postContent;
              postData.url = postURL;
              postData.weight = postWeight;
              matchPosts.push(postData);
            }
          });
        }

        var resultInnerHtml = '';
        if (matchPosts.length) {
          // 按权重递增的顺序排序，使权重大的优先显示
          matchPosts.sort(function (left, right) {
            return right.weight - left.weight;
          });
          resultInnerHtml += '<ul>';
          matchPosts.forEach(function (post) {
            resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
            resultInnerHtml += post.title;
            resultInnerHtml += '</a><div class="search-results-content">';
            resultInnerHtml += post.content;
            resultInnerHtml += '</div></li>';
          });
          resultInnerHtml += '</ul>';
        } else {
          resultInnerHtml += '<div class="search-results-none"><i class="far fa-meh"></i></div>';
        }
        $result.html(resultInnerHtml);
      };
      $input.on('input', searchPost);
      $input.on('keyup', function (e) {
        if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
          searchPost();
        }
      });
    }
  });
}

function closeSearch () {
  $('body').css({ overflow: 'auto' });
  $('.search-popup').css({ display: 'none' });
  $('.search-mask').css({ display: 'none' });
}

window.addEventListener('DOMContentLoaded', function () {
  Stun.utils.pjaxReloadLocalSearch = function () {
    $('.header-nav-search').on('click', function (e) {
      e.stopPropagation();
      $('body').css('overflow', 'hidden');
      $('.search-popup')
        .velocity('stop')
        .velocity('transition.expandIn', {
          duration: 300,
          complete: function () {
            $('.search-popup input').focus();
          }
        });
      $('.search-mask')
        .velocity('stop')
        .velocity('transition.fadeIn', {
          duration: 300
        });

      initSearch();
    });
    $('.search-mask, .search-close').on('click', function () {
      closeSearch();
    });
    $(document).on('keydown', function (e) {
      // Escape <=> 27
      if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
        closeSearch();
      }
    });
  };

  Stun.utils.pjaxReloadLocalSearch();
}, false);

function safeOpenUrl(url) {
  var newTab = window.open();
  newTab.opener = null;
  newTab.location = url;
}

function extSearch(engine) {
  var engines = {
    google: 'https://www.google.com/search?q=',
    bing: 'https://cn.bing.com/search?q=',
    baidu: 'https://www.baidu.com/s?ie=UTF-8&wd=',
  };
  var host = window.location.host;
  var query = $('.search-input input').val().toLowerCase().trim();
  var uri = engines[engine] + query + ' site:' + host;

  if (query) {
    safeOpenUrl(uri);
  } else {
    Stun.utils.popAlert('warning', '请输入字符');
  }
}

var assistSearchList = window.CONFIG.assistSearch;

if (Array.isArray(assistSearchList)) {
  assistSearchList.forEach(function (name) {
    document.querySelector('.search-btns-item--' + name).addEventListener('click', function () {
      extSearch(name);
    }, false);
  });
}</script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload",".header-inner"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (true) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"></div><script data-pjax="">function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'plotor/hexo-comments');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (true) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (true) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.6.1"></script><script src="/js/stun-boot.js?v=2.6.1"></script><script src="/js/scroll.js?v=2.6.1"></script><script src="/js/header.js?v=2.6.1"></script><script src="/js/sidebar.js?v=2.6.1"></script><script type="application/json" src="/search.json"></script></body></html>