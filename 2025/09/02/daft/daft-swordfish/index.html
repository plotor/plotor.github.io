<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/favicon_16.png?v=2.6.1" type="image/png" sizes="16x16"><link rel="icon" href="/images/favicon_32.png?v=2.6.1" type="image/png" sizes="32x32"><meta name="google-site-verification" content="O5CNgi37yYXs3qQp7Xz61oL_AmGiwM28d7hRt5yh2to"><meta name="baidu-site-verification" content="pnKVynCWMP"><meta name="description" content="Daft 是一款面向 DATA + AI 多模态数据处理与分析场景的计算引擎，支持单机和分布式两种运行模式，内核采用 Rust 语言编写，并提供 SQL 和 Python DataFrame 两种交互方式。 在文章《Processing 300K Images Without OOM: A Streaming Solution》中，作者介绍了基于 Daft 能够轻松实现对大规模图片数据集进行流式处">
<meta property="og:type" content="article">
<meta property="og:title" content="通过图片分类任务探寻 Daft 运行机制之 Swordfish 引擎篇">
<meta property="og:url" content="https://plotor.github.io/2025/09/02/daft/daft-swordfish/index.html">
<meta property="og:site_name" content="指  间">
<meta property="og:description" content="Daft 是一款面向 DATA + AI 多模态数据处理与分析场景的计算引擎，支持单机和分布式两种运行模式，内核采用 Rust 语言编写，并提供 SQL 和 Python DataFrame 两种交互方式。 在文章《Processing 300K Images Without OOM: A Streaming Solution》中，作者介绍了基于 Daft 能够轻松实现对大规模图片数据集进行流式处">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://plotor.github.io/images/2025/daft-architecture.jpg">
<meta property="og:image" content="https://plotor.github.io/images/2025/daft-project-optimization.jpg">
<meta property="og:image" content="https://plotor.github.io/images/2025/push-vs-pull-based.png">
<meta property="og:image" content="https://plotor.github.io/images/2025/daft-swordfish-pipeline.jpg">
<meta property="og:image" content="https://plotor.github.io/images/2025/daft-pipeline-exec2.gif">
<meta property="article:published_time" content="2025-09-02T14:16:00.000Z">
<meta property="article:modified_time" content="2025-12-01T12:29:42.764Z">
<meta property="article:author" content="zhenchao">
<meta property="article:tag" content="Daft">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://plotor.github.io/images/2025/daft-architecture.jpg"><title>通过图片分类任务探寻 Daft 运行机制之 Swordfish 引擎篇 | 指  间</title><link ref="canonical" href="https://plotor.github.io/2025/09/02/daft/daft-swordfish/"><link rel="alternate" href="/atom.xml" type="application/atom+xml"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"carbon","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: {"avoidBanner":false},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user-circle"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-search"><span class="header-nav-search__icon"><i class="fas fa-search"></i></span><span class="header-nav-search__text">搜索</span></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">通过图片分类任务探寻 Daft 运行机制之 Swordfish 引擎篇</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2025-09-02</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">5.9k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">25分</span></span></div></header><div class="post-body"><p>Daft 是一款面向 DATA + AI 多模态数据处理与分析场景的计算引擎，支持单机和分布式两种运行模式，内核采用 Rust 语言编写，并提供 SQL 和 Python DataFrame 两种交互方式。</p>
<p>在文章《<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.daft.ai/blog/processing-300k-images-without-oom">Processing 300K Images Without OOM: A Streaming Solution</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>》中，作者介绍了基于 Daft 能够轻松实现对大规模图片数据集进行流式处理。那么，Daft 在幕后是如何执行用户输入的 SQL 或 DataFrame 的呢？在本文中，我们将继续以图片处理场景为切入点，通过一个典型的图片分类任务 DataFrame 示例，引领你深入探寻 Daft 单机执行引擎的运行机制。</p>
<blockquote>
<p>本文英文版本已发布至 Daft 官网：“<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.daft.ai/blog/exploring-daft-swordfish-execution-mechanism">Exploring Daft’s Local Execution: The Swordfish Engine</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>”<a id="more"></a></p>
</blockquote>

        <h2 id="图片分类任务-DataFrame-示例">
          <a href="#图片分类任务-DataFrame-示例" class="heading-link"><i class="fas fa-link"></i></a>图片分类任务 DataFrame 示例</h2>
      <p>我们以 ImageNet 图片数据集为例构造一个包含图片 name、height、width，以及 url 四个属性的 Parquet 格式数据集，然后基于 Daft DataFrame 实现如下处理逻辑：</p>
<ol>
<li>读取 Parquet 格式数据集，筛选出长宽为 256 像素的图片；</li>
<li>基于图片 URL 下载图片二进制数据，并将其解码成 Image 类型；</li>
<li>对图片执行简单的预处理操作，包括裁剪、归一化等；</li>
<li>调用 ResNet50 模型对图片执行离线分类打标，并返回相似性前 5 的分类标签；</li>
<li>将处理结果以 Lance 格式存储为新的图片数据集。</li>
</ol>
<p>上述步骤对应的核心实现逻辑如下：</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">df = (daft</span><br><span class="line">      <span class="comment"># 读取 Parquet 格式图片数据集</span></span><br><span class="line">      .read_parquet(</span><br><span class="line">        path=<span class="string">"s3://ai/dataset/url_ILSVRC2012/*.parquet"</span>,</span><br><span class="line">        io_config=IO_CONFIG)</span><br><span class="line">      <span class="comment"># 筛选目标尺寸的图片</span></span><br><span class="line">      .<span class="built_in">filter</span>((col(<span class="string">'height'</span>) == <span class="number">256</span>) &amp; (col(<span class="string">'width'</span>) == <span class="number">256</span>))</span><br><span class="line">      <span class="comment"># 通过 url 下载对应的图片二进制数据</span></span><br><span class="line">      .with_column(<span class="string">'bytes'</span>, col(<span class="string">'url'</span>).url.download(io_config=IO_CONFIG))</span><br><span class="line">      <span class="comment"># 将二进制图片解码成为图片类型</span></span><br><span class="line">      .with_column(<span class="string">'image'</span>, col(<span class="string">'bytes'</span>).image.decode(mode=ImageMode.RGB)).exclude(<span class="string">'bytes'</span>)</span><br><span class="line">      <span class="comment"># 对图片执行基本的预处理，包括裁剪、归一化等</span></span><br><span class="line">      .with_column(<span class="string">'tensor'</span>, col(<span class="string">'image'</span>).apply(</span><br><span class="line">        func=<span class="keyword">lambda</span> img: transform(img),</span><br><span class="line">        return_dtype=daft.DataType.tensor(dtype=daft.DataType.float32()))).exclude(<span class="string">"image"</span>)</span><br><span class="line">      <span class="comment"># 调用 ResNet50 模型对图片进行离线推理打标</span></span><br><span class="line">      .with_column(<span class="string">'labels'</span>, ResNetModel(col(<span class="string">'tensor'</span>))).exclude(<span class="string">'tensor'</span>)</span><br><span class="line">      .limit(<span class="number">100</span>)</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">df.write_lance(</span><br><span class="line">    uri=<span class="string">"/tmp/output"</span></span><br><span class="line">)</span><br></pre></td></tr></tbody></table></div></figure>
<p>其中 <code>transform</code> 和 <code>ResNetModel</code> 为 UDF 实现，前者实现对图片数据的预处理操作，后者实现调用 ResNet50 模型对图片进行分类打标。</p>

        <h2 id="Daft-分层架构设计">
          <a href="#Daft-分层架构设计" class="heading-link"><i class="fas fa-link"></i></a>Daft 分层架构设计</h2>
      <p>在正式开始分析上述 DataFrame 示例的执行过程之前，我们首先从整体层面介绍一下 Daft 的分层架构设计。如下图所示，除了 Daft 执行所依托的本地或分布式 Runtime 环境之外，就 Daft 本身的整体架构设计而言，可划分为 API 层、Plan 层，以及执行层 3 个层次：</p>
<ul>
<li><p><strong>API 层</strong> ：提供 SQL 和 Python DataFrame 两种接入 API。Daft 通过对分布式多模态数据进行表格化抽象，并按照行维度切分成多个分片分散到集群的各个节点进行分布式处理，以实现对数据的并行计算。</p>
</li>
<li><p><strong>Plan 层</strong> ：基于用户输入的 SQL 或 DataFrame 构建逻辑执行计划（LogicalPlan）和物理执行计划（PhysicalPlan）。虽然用户通过 SQL 或 DataFrame API 编写业务逻辑，但在 Daft 内部会将其转换成逻辑执行计划进行统一表示，并应用优化器对逻辑执行计划进行优化以获得更好的执行性能。经过优化处理后的逻辑执行计划将被进一步转换为物理执行计划，并应用物理执行计划层面的优化器做进一步优化。</p>
</li>
<li><p><strong>执行层</strong> ：经过 Plan 层构造并优化得到的物理执行计划需要根据单机或分布式运行模式进一步拆分成一系列可被执行的 Task，并由调度器调度执行，同时管理 Task 的执行状态。</p>
</li>
</ul>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2025/daft-architecture.jpg" alt="Daft Architecture">
      </p>
<p>目前在 DATA + AI 领域主流的数据计算引擎（例如 Pandas、Polars）都默认仅提供单机执行引擎，其中 Polars 的分布式执行引擎仅在商业版提供，而 Dask 虽然可以看作是 Pandas 的分布式实现，但执行效率相对较低。Daft 在开源版本中同时提供了单机和分布式两套执行引擎，并允许在这两套引擎之间任意切换。</p>
<p>Daft 的单机执行引擎命名为 Swordfish，而分布式执行引擎命名为 Flotilla。相对 Flotilla 而言，Swordfish 的运行机制要简单许多，Flotilla 可以看做是分布式的 Swordfish 实现，理解 Swordfish 是进一步理解  Flotilla 的基础。 因此本文主要基于 Swordfish 引擎分析 Daft 如何执行用户输入的 DataFrame，后续我们将会通过专门的文章介绍 Flotilla 引擎的执行机制。</p>

        <h2 id="构造并优化逻辑执行计划">
          <a href="#构造并优化逻辑执行计划" class="heading-link"><i class="fas fa-link"></i></a>构造并优化逻辑执行计划</h2>
      <p>在对 Daft 整体架构有了初步感知后，我们正式切入正题。在图片分类任务示例中，用户定义的 DataFrame 应用程序会被 Daft 在内部表示成逻辑执行计划树。Daft 内部通过 LogicalPlanBuilder 基于用户构建的 DataFrame 构造逻辑执行计划树。例如，上述示例中的 DataFrame 程序对应的逻辑执行计划如下：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">* Limit: 100</span><br><span class="line">|</span><br><span class="line">* Project: col(name), col(height), col(width), col(url), col(labels)</span><br><span class="line">|</span><br><span class="line">* Project: col(name), col(height), col(width), col(url), col(tensor), py_udf(col(tensor)) as labels</span><br><span class="line">|</span><br><span class="line">* Project: col(name), col(height), col(width), col(url), col(tensor)</span><br><span class="line">|</span><br><span class="line">* Project: col(name), col(height), col(width), col(url), col(image), py_udf(col(image)) as tensor</span><br><span class="line">|</span><br><span class="line">* Project: col(name), col(height), col(width), col(url), col(image)</span><br><span class="line">|</span><br><span class="line">* Project: col(name), col(height), col(width), col(url), col(bytes), image_decode(col(bytes), ..) as image</span><br><span class="line">|</span><br><span class="line">* Project: col(name), col(height), col(width), col(url), url_download(col(url), ..) as bytes</span><br><span class="line">|</span><br><span class="line">* Filter: [col(height) == lit(256)] &amp; [col(width) == lit(256)]</span><br><span class="line">|</span><br><span class="line">* GlobScanOperator</span><br><span class="line">|   Glob paths = [s3://ai/dataset/url_ILSVRC2012/*.parquet]</span><br><span class="line">|   Coerce int96 timestamp unit = Nanoseconds</span><br><span class="line">|   IO config = ..</span><br><span class="line">|   Use multithreading = true</span><br><span class="line">|   File schema = name#Utf8, height#Int64, width#Int64, url#Utf8</span><br><span class="line">|   Partitioning keys = []</span><br><span class="line">|   Output schema = name#Utf8, height#Int64, width#Int64, url#Utf8</span><br></pre></td></tr></tbody></table></div></figure>
<blockquote>
<p>说明：为保证可读性，本文中展示的执行计划会省略部分非关键内容。</p>
</blockquote>
<p>虽然图片分类任务示例程序中没有使用 <code>select</code> 算子进行列筛选，但从上述逻辑执行计划可以看出包含多个 Project 节点，这是因为示例 DataFrame 中使用了多个 <code>with_column</code> 算子通过执行内置算子或 UDF 实现增加列。Daft 在内部使用 Project 节点承载 <code>with_column</code> 算子语义，对应的实现逻辑如下：</p>
<figure class="highlight rust"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">pub</span> <span class="function"><span class="keyword">fn</span> <span class="title">with_columns</span></span>(&amp;<span class="keyword">self</span>, columns: <span class="built_in">Vec</span>&lt;ExprRef&gt;) -&gt; DaftResult&lt;<span class="keyword">Self</span>&gt; {</span><br><span class="line">    <span class="comment">// ... 省略输入表达式的解析逻辑</span></span><br><span class="line">    <span class="keyword">let</span> logical_plan: LogicalPlan = ops::Project::try_new(<span class="keyword">self</span>.plan.clone(), exprs)?.into();</span><br><span class="line">    <span class="literal">Ok</span>(<span class="keyword">self</span>.with_new_plan(logical_plan))</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>我们在编写 DataFrame 时往往更加注重业务逻辑的实现和代码可读性，而 LogicalPlanBuilder 基于 DataFrame 构造逻辑执行计划的过程可以理解是一个“直译”的过程，所以上述逻辑执行计划在构成上与示例程序 DataFrame 在算子个数和构成上几乎一一对应。因此，直接将 LogicalPlanBuilder 构造的逻辑执行计划提交给执行引擎执行，通常执行效率非常低下。</p>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2025/daft-project-optimization.jpg" alt="Daft Project Optimization">
      </p>
<p>为了解决这一问题，Daft 与传统计算引擎一样，在内部实现了面向逻辑执行计划的优化器框架，并内置数十条优化规则实现对逻辑执行计划树进行优化改写。以图片分类任务 DataFrame 程序对应的逻辑执行计划为例，Daft 主要会应用如下优化规则对其进行优化：</p>
<ul>
<li><p><strong>Filter/Limit 表达式下推</strong> ：示例程序通过 <code>filter</code> 算子筛选出长宽均为 256 的图片，并通过 <code>limit</code> 算子限制仅返回 100 条计算结果，优化器层面会将 Filter 和 Limit 表达式下推给 Scan 节点，实现过滤并减少需要扫描和处理的数据量。</p>
</li>
<li><p><strong>连续 Project 节点合并</strong> ：示例程序对应的逻辑执行计划树包含多个连续的 Project 节点，因此可以尝试将其合并以实现将多次 Project 投影操作合并为一次执行，这样在缩短执行链路的同时，也便于将 Project 表达式下推给 Scan 节点。如上图（1）所示，多个 Project 节点经过合并后可以压缩成为图（2）所示的 1 个 Project 节点。</p>
</li>
<li><p><strong>UDF Project 分离</strong> ：经过合并后的 Project 可能会将 UDF 列与普通列合并在一起，这通常不便于框架实现 UDF 功能，同时在执行 UDF 时也不够灵活和高效，因此通常的做法是将 UDF Project 列从 Project 中分离出来进行单独处理。例如上述图（2）经过合并后的 Project 包含两个 UDF Project 列（即 <code>py_udf</code> 列），其中内侧的 UDF 以解码得到的 image 列作为输入，其输出的 tensor 列继续作为外侧的 UDF 的输入，并输出 labels 列。因此我们需要将这两个 UDF Project 列从 Project 中分离，得到如图（3）所示的逻辑执行计划。</p>
</li>
<li><p><strong>细粒度 Project 节点拆分</strong> ：该优化规则目前主要面向包含 <code>url_download</code> 表达式的 Project 节点。设想对于一个包含 1 万个 URL 的 CSV 文件，那么按照 Daft 的分区策略，这 1 万个 URL 将会集中到 1 个 Task 中进行处理，因此无法发挥分布式多节点并行下载的优势，同时也容易造成单节点 OOM 和连接数超限等问题。因此，我们可以将包含 <code>url_download</code> 表达式的 Project 节点分离出来单独处理，从而更加细粒度的控制批次大小和并发度。图（3）中包含 <code>url_download</code> 表达式的 Project 节点经过拆分后得到的逻辑执行计划如图（4）所示。</p>
</li>
<li><p><strong>物化 Scan 算子</strong> ：依据用户输入的数据源路径，Daft 会在优化逻辑执行计划期间访问数据源的元信息，并构造一系列 ScanTask 实现对数据的并发加载。同时，针对一些下推优化规则探测到的可以下推到 Scan 节点的表达式（例如 Filter、Limit、Project ），也会一并物化到 ScanTask 中。</p>
</li>
</ul>
<p>示例 DataFrame 对应的逻辑执行计划在经过 Daft 优化器中的一系列优化规则优化改写后，进一步得到优化后的逻辑执行计划如下：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">* UDFProject:</span><br><span class="line">|   UDF __main__.ResNetModel = py_udf(col(__TruncateRootUDF_0-4-0__)) as labels</span><br><span class="line">|   Passthrough Columns = col(name), col(height), col(width), col(url)</span><br><span class="line">|   Concurrency = Some(4)</span><br><span class="line">|   Resource request = { num_cpus = 0, num_gpus = 0.25 }</span><br><span class="line">|</span><br><span class="line">* Project: col(__TruncateRootUDF_0-4-0__), col(name), col(height), col(width), col(url)</span><br><span class="line">|</span><br><span class="line">* UDFProject:</span><br><span class="line">|   UDF __main__.&lt;lambda&gt; = py_udf(col(__TruncateRootUDF_1-4-0__)) as tensor as __TruncateRootUDF_0-4-0__</span><br><span class="line">|   Passthrough Columns = col(name), col(height), col(width), col(url)</span><br><span class="line">|   Concurrency = None</span><br><span class="line">|</span><br><span class="line">* Project: col(__TruncateRootUDF_1-4-0__) as __TruncateRootUDF_1-4-0__, col(name) as name, col(height) as height, col(width) as width, col(url) as url</span><br><span class="line">|</span><br><span class="line">* Project: image_decode(col(id-5752374d-e36f-498f-bd85-29a3633c949a) as bytes, ..) as image as __TruncateRootUDF_1-4-0__, col(name), col(height), col(width), col(url)</span><br><span class="line">|</span><br><span class="line">* Project: url_download(col(url), ..) as id-5752374d-e36f-498f-bd85-29a3633c949a, col(name), col(height), col(width), col(url)</span><br><span class="line">|</span><br><span class="line">* Limit: 100</span><br><span class="line">|   Stats = { Approx num rows = 100, Approx size bytes = 5.47 KiB, Accumulated selectivity = 0.00 }</span><br><span class="line">|</span><br><span class="line">* Num Scan Tasks = 1424</span><br><span class="line">|   File schema = name#Utf8, height#Int64, width#Int64, url#Utf8</span><br><span class="line">|   Partitioning keys = []</span><br><span class="line">|   Filter pushdown = [col(height) == lit(256)] &amp; [col(width) == lit(256)]</span><br><span class="line">|   Limit pushdown = 100</span><br><span class="line">|   Output schema = name#Utf8, height#Int64, width#Int64, url#Utf8</span><br><span class="line">|   Stats = { Approx num rows = 300,469, Approx size bytes = 16.23 MiB, Accumulated selectivity = 0.20 }</span><br></pre></td></tr></tbody></table></div></figure>
<p>本小节仅对示例 DataFrame 命中的主要优化规则展开了介绍，实际上除上述几个优化规则外，Daft 优化器框架内置了大量的优化规则，典型包括消除 CrossJoin/Subquery/Repartition、谓词下推、列裁剪、Join Reorder 等，并且还在不断地演进和完善，旨在追求极致的计算性能。</p>

        <h2 id="构造并优化物理执行计划">
          <a href="#构造并优化物理执行计划" class="heading-link"><i class="fas fa-link"></i></a>构造并优化物理执行计划</h2>
      <p>经过优化器优化得到的逻辑执行计划本质上还是对用户构建的 DataFrame 在逻辑语义层面的表示，并不能直接被执行引擎执行。 <strong>逻辑执行计划层面更多关注的是用户提交的 SQL 或 DataFrame 需要“做什么”的问题，而无需关注需要在什么平台上执行，以及是单机执行还是分布式执行等“怎么做”的问题</strong> 。因此上述优化后的逻辑执行计划还需要进一步被转换成如下所示的物理执行计划：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">* UDF Executor:</span><br><span class="line">|   UDF __main__.ResNetModel = py_udf(col(0: __TruncateRootUDF_0-4-0__)) as labels</span><br><span class="line">|   Passthrough Columns = [col(1: name), col(2: height), col(3: width), col(4: url)]</span><br><span class="line">|   Concurrency = 4</span><br><span class="line">|   Resource request = { num_cpus = 0, num_gpus = 0.25 }</span><br><span class="line">|</span><br><span class="line">* Project: col(4: __TruncateRootUDF_0-4-0__), col(0: name), col(1: height), col(2: width), col(3: url)</span><br><span class="line">|</span><br><span class="line">* UDF Executor:</span><br><span class="line">|   UDF __main__.&lt;lambda&gt; = py_udf(col(0: __TruncateRootUDF_1-4-0__)) as tensor as __TruncateRootUDF_0-4-0__</span><br><span class="line">|   Passthrough Columns = [col(1: name), col(2: height), col(3: width), col(4: url)]</span><br><span class="line">|   Concurrency = 8</span><br><span class="line">|   Resource request = None</span><br><span class="line">|</span><br><span class="line">* Project: col(0: __TruncateRootUDF_1-4-0__) as __TruncateRootUDF_1-4-0__, col(1: name) as name, col(2: height) as height, col(3: width) as width, col(4: url) as url</span><br><span class="line">|</span><br><span class="line">* Project: image_decode(col(0: id-5752374d-e36f-498f-bd85-29a3633c949a) as bytes, ...) as image as __TruncateRootUDF_1-4-0__, col(1: name), col(2: height), col(3: width), col(4:</span><br><span class="line">|     url)</span><br><span class="line">|</span><br><span class="line">* Project: url_download(col(3: url), ...) as id-5752374d-e36f-498f-bd85-29a3633c949a, col(0: name), col(1: height), col(2: width), col(3: url)</span><br><span class="line">|</span><br><span class="line">* Limit: 100</span><br><span class="line">|   Stats = { Approx num rows = 100, Approx size bytes = 5.47 KiB, Accumulated selectivity = 0.00 }</span><br><span class="line">|</span><br><span class="line">* ScanTaskSource:</span><br><span class="line">|   Num Scan Tasks = 1424</span><br><span class="line">|   Estimated Scan Bytes = 28364955</span><br><span class="line">|   Pushdowns: {filter: [col(height) == lit(256)] &amp; [col(width) == lit(256)], limit: 100}</span><br><span class="line">|   Schema: {name#Utf8, height#Int64, width#Int64, url#Utf8}</span><br><span class="line">|   Scan Tasks: [ ... ]</span><br><span class="line">|   Stats = { Approx num rows = 300,469, Approx size bytes = 16.23 MiB, Accumulated selectivity = 0.20 }</span><br></pre></td></tr></tbody></table></div></figure>
<p>考虑 Daft 支持单机和分布式两套执行引擎，因此将逻辑执行计划转换成物理执行计划的过程需要区分是单机执行还是分布式执行。</p>
<p>本文主要介绍 Swordfish 单机执行引擎的运行机制。对于单机执行而言，转化成物理执行计划的过程可以简单理解为是对逻辑执行计划树进行深度优先遍历，并一一映射算子的过程，即将逻辑算子映射成为物理算子。当然，并不是所有的逻辑算子都有对应的物理算子实现，比如 Offset 这类逻辑算子会在优化器层面被改写成 Limit 算子，所以对于 <code>.offset(x).limit(y)</code> 这类操作最终在物理执行计划层面只需要一个 <code>limit(y, x)</code> 算子即可表达。</p>

        <h2 id="提交-Swordfish-单机执行">
          <a href="#提交-Swordfish-单机执行" class="heading-link"><i class="fas fa-link"></i></a>提交 Swordfish 单机执行</h2>
      <p>Daft 默认以单机模式执行用户输入的 SQL 或 DataFrame。Swordfish 作为 Daft 的单机执行引擎，采用 Pipeline 流式架构设计，并基于 Rust 语言实现，同时借助异步非阻塞 I/O 任务执行框架实现对数据的并行计算，具备优秀的执行性能和资源开销。</p>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2025/push-vs-pull-based.png" alt="Push vs Pull-Based">
      </p>
<blockquote>
<p>图片引用自论文 《<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.cambridge.org/core/journals/journal-of-functional-programming/article/push-versus-pullbased-loop-fusion-in-query-engines/D67AE4899E87F4B5102F859B0FC02045">Push versus pull-based loop fusion in query engines</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>》</p>
</blockquote>
<p>在实现层面，Swordfish 没有采用经典 Volcano 模型基于迭代器的 Pull 模式，而是采用基于数据块（Morsel）驱动的 Push 模式，本文我们不去谈论这两种模式孰优孰劣。在执行层面，当构成 Pipeline 管道的各个节点启动后，数据源节点即开始运行 Scan 任务加载数据，数据会被切分成一个个小的数据块，在节点之间采用“发布-订阅”模式通过 Channel 实现流式数据传递和处理。因此，整个 Pipeline 管道的运行可以看作是由数据块驱动，以一种 Push 的方式将数据块从 Source 节点推送给中间节点处理，并最终推送给 Sink 节点进行展示或写回文件系统。</p>

        <h3 id="构建-Pipeline-管道">
          <a href="#构建-Pipeline-管道" class="heading-link"><i class="fas fa-link"></i></a>构建 Pipeline 管道</h3>
      <p>对于示例 DataFrame 对应的物理执行计划，在提交给 Swordfish 执行引擎后，首先会将其转换成 Pipeline 管道。Swordfish 在内部定义了如下 4 类 Pipeline 节点：</p>
<div class="table-container"><table>
<thead>
<tr>
<th>节点类型</th>
<th>角色</th>
<th>并发数</th>
<th>Operator 示例</th>
</tr>
</thead>
<tbody><tr>
<td>SourceNode</td>
<td>数据源节点</td>
<td>依赖为数据分配的 Task 数，并依据经验值控制默认最大并发上限为 8</td>
<td>PhysicalScan</td>
</tr>
<tr>
<td>IntermediateNode</td>
<td>中间处理节点</td>
<td>取决于具体的 Operator 类型，默认为所在节点的 CPU 核数</td>
<td>Project、Filter、UDF</td>
</tr>
<tr>
<td>BlockingSinkNode</td>
<td>阻塞数据接收节点</td>
<td>取决于具体的 Operator 类型，默认为所在节点的 CPU 核数</td>
<td>Aggregate、Repartition、WriteSink</td>
</tr>
<tr>
<td>StreamingSinkNode</td>
<td>流式数据接收节点</td>
<td>取决于具体的 Operator 类型，默认为所在节点的 CPU 核数</td>
<td>Limit、Concat、MonotonicallyIncreasingId</td>
</tr>
</tbody></table></div>
<p>Swordfish 执行引擎将物理执行计划转换成 Pipeline 管道的过程本质上也是一个对物理执行计划树进行深度优先遍历，并一一映射算子的过程，即将物理算子映射成为上述 4 类节点。示例 DataFrame 对应的物理执行计划经过转换后得到的 Pipeline 管道如下图所示：</p>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2025/daft-swordfish-pipeline.jpg" alt="Daft Swordfish Pipeline">
      </p>
<p>构成 Pipeline 管道的各个节点通过类似消息队列的 Channel 进行连接，以“发布-订阅”模式实现数据的传递。</p>

        <h3 id="执行-Pipeline-管道">
          <a href="#执行-Pipeline-管道" class="heading-link"><i class="fas fa-link"></i></a>执行 Pipeline 管道</h3>
      <p>在执行阶段，Swordfish 首先会对 Pipeline 管道树进行深度优先遍历，并逐一调用各个节点的 <code>start</code> 方法启动运行各个节点。为了发挥多核 CPU 优势，Swordfish 引擎支持在节点内部、节点之间并发执行 Pipeline 任务，并允许用户控制节点并发度。Daft 在内部采用基于事件驱动的高性能异步 I/O 执行框架 Tokio 实现对于 Pipeline 各节点任务的并发调度执行。</p>
<p>例如对于上述图中的 Pipeline 管道，Swordfish 会按照从右至左的顺序逐一启动各 Pipeline 节点。当所有节点都启动后，整个 Pipeline 管道即处于运行状态。此时，Source 节点即开始运行 ScanTask 扫描数据，并将数据投递给与之绑定的 Channel，下游订阅该 Channel 的中间节点即可获得数据进行流式处理，并最终由 Sink 节点展示到终端或写回文件系统。整体执行流程如下图所示：</p>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2025/daft-pipeline-exec2.gif" alt="Daft Streaming Execution">
      </p>
<blockquote>
<p>图片引用自《<span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.daft.ai/blog/processing-300k-images-without-oom">Processing 300K Images Without OOM: A Streaming Solution</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>》，仅具象说明 Pipeline 中数据的流动方式，不代表示例 DataFrame 对于数据的处理过程。</p>
</blockquote>
<p>下面我们继续以示例 DataFrame 对应的 Pipeline 管道为例，围绕以下 3 个问题进一步探究构成 Pipeline 管道的节点的执行机制：</p>
<ol>
<li>数据是如何被加载的？</li>
<li>数据是如何被处理的？</li>
<li>数据是如何被写回的？</li>
</ol>
<p><strong>首先，我们来探究数据是如何被加载进 Pipeline 管道的</strong> 。前面已经介绍过，在构造和优化执行计划期间，Daft 会基于数据源路径构造和物化一系列的 ScanTask，用于实现对数据的并行加载。同时，Daft 在构建 Pipeline 管道期间会将数据源 Scan 算子封装到 SourceNode 节点中。在执行层面，Daft 在内部通过多生产者单消费者模式（MPSC: Multi Producer Single Consumer）实现对于这些 ScanTask 的调度执行。</p>
<p>具体来说，Daft 会在内部创建一个 MPSC 的 Channel ，所有的 ScanTask 在运行期间会依据数据源的具体格式调用对应的 SDK 流式加载数据，并将数据按照 Morsel 分片粒度投递给该 MPSC Channel，而下游的处理节点会订阅该 Channel，并从中以 Morsel 分片粒度消费数据并应用处理逻辑。当然，对于较大的数据源而言，Daft 可能会为其创建成千上万的 ScanTask，如果将这些 ScanTask 一并提交执行，势必会造成资源争抢，导致运行效率不高。因此，Daft 在内部会维护一个 Task 池子，以限制同一时刻能够执行的 ScanTask 数量上限。</p>
<p><strong>其次，我们以执行 UDF 为切入点来探究数据在 Pipeline 管道中是如何被处理的</strong> 。不同于传统大数据处理场景，在多模态数据处理场景中，UDF 扮演着至关重要的作用，也是一个被高频使用的功能。如下所示，我们在示例 DataFrame 中定义了两个 UDF：</p>
<figure class="highlight python"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">.with_column(<span class="string">'tensor'</span>, col(<span class="string">'image'</span>).apply(</span><br><span class="line">    func=<span class="keyword">lambda</span> img: transform(img),</span><br><span class="line">    return_dtype=daft.DataType.tensor(dtype=daft.DataType.float32()))).exclude(<span class="string">"image"</span>)</span><br><span class="line">.with_column(<span class="string">'labels'</span>, ResNetModel(col(<span class="string">'tensor'</span>))).exclude(<span class="string">'tensor'</span>)</span><br></pre></td></tr></tbody></table></div></figure>
<p>其中 <code>transform</code> 是一个无状态的 UDF，用于按行实现对于图片数据的基础预处理操作；<code>ResNetModel</code> 是一个有状态的 UDF，该 UDF 会在初始化时加载 ResNet50 模型，并按批次调用模型对图片进行离线分类打标。</p>
<p>在实现层面，Daft 使用 IntermediateNode 节点将 UDF 封装到 Pipeline 管道中。作为 Pipeline 管道的中间节点，IntermediateNode 在启动后会执行：</p>
<ol>
<li>按照 UDF 的 <code>concurrency</code> 参数计算并设置能够同时执行 UDF 算子的 Worker 并发度，如果未设置则会沿用当前所在机器的 CPU 核心数；</li>
<li>考虑 UDF 可能会设置 <code>batch_size</code> 参数，因此需要将上游算子处理后的 Morsel 分片数据重新切分或合并，然后投递到与各 Worker 关联的 Channel 中；</li>
<li>提交运行 UDF 的 Worker 集合，各 Worker 按照 MPSC 模式消费上游处理完成的数据，并执行 UDF 逻辑，同时将结果输出投递到输出 Channel 中，供下游节点继续消费处理。</li>
</ol>
<p>对于 Swordfish 执行引擎而言，Daft 内部定义了 UdfOperator 用于执行 UDF 的内在业务逻辑。如果我们在定义 UDF 时配置了内存资源参数 <code>memory_bytes</code>，则引擎在开始执行 UDF 之前，会尝试从逻辑层面申请对应数量的内存资源。本文我们侧重于从整体流程层面了解 Swordfish 引擎的执行机制，关于 UdfOperator 如何执行 UDF 中定义的内在业务逻辑，将不在本文中继续展开。我们计划后续通过专门的文章介绍 Daft 关于 UDF 的执行机制。</p>
<p><strong>最后，我们以写 Lance 格式数据为例来探究 Pipeline 管道中被处理完成的数据，是如何最终写回文件系统的</strong> 。Daft 内部定义了 LanceWriter 用于实现写 Lance 格式数据，而 LanceWriter 本质上是对 Lance Python SDK 的包装，内部定义了 <code>write</code> 方法实现按 Partition 分片将数据写成 Lance 格式。</p>
<p>针对 <code>write_lance</code> 这类 Sink 操作，Daft 使用 BlockingSinkNode 节点实现了 LanceWriter 与 Pipeline 管道的集成。在整体执行流程上，BlockingSinkNode 在启动后与 IntermediateNode 的执行流程类似，主要的区别在于：</p>
<ol>
<li>在计算 Worker 并发度时，主要判断是否指定了 <code>partition_by</code> 分区列，如果指定则默认采用当前所在机器的 CPU 核心数，否则就简单的设置为 1；</li>
<li>每个 Worker 执行的不再是常规的数据处理逻辑，而是会首先创建 Writer 实例，然后基于该实例执行 <code>Writer#write</code> 方法，通过调用对应格式的 SDK 将管道中的数据按照 Partition 分片粒度，以目标格式写回文件系统。</li>
</ol>

        <h2 id="结语">
          <a href="#结语" class="heading-link"><i class="fas fa-link"></i></a>结语</h2>
      <p>在本文中，我们首先从宏观视角介绍了 Daft 的三层架构设计；然后借助一个图片分类任务 DataFrame 示例，从 API 层切入剖析了 Daft 在底层如何依据 DataFrame 构建并优化逻辑执行计划，以及如何将逻辑执行计划进一步优化改写成物理执行计划的过程；最后，我们以 Swordfish 引擎为依托，剖析了在单机运行模式下，Daft 依据物理执行计划构建 Pipeline 管道的具体方式，以及以 Pipeline 模式实现流式数据处理的执行过程。</p>
<p>限于篇幅，我们很难仅通过一篇文章将 Daft 内在原理分析的面面俱到，更多的还是引领你从整体执行层面感知 Daft 的内在实现，以此作为你深入学习 Daft 的引导。</p>

        <h2 id="参考">
          <a href="#参考" class="heading-link"><i class="fas fa-link"></i></a>参考</h2>
      <ul>
<li><span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/2588555.2610507">Morsel-driven parallelism: a NUMA-aware query evaluation framework for the many-core age</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://www.cambridge.org/core/journals/journal-of-functional-programming/article/push-versus-pullbased-loop-fusion-in-query-engines/D67AE4899E87F4B5102F859B0FC02045">Push versus pull-based loop fusion in query engines</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li><span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.14778/2002938.2002940">Efficiently compiling efficient query plans for modern hardware</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://plotor.github.io">zhenchao</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://plotor.github.io/2025/09/02/daft/daft-swordfish/">https://plotor.github.io/2025/09/02/daft/daft-swordfish/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://plotor.github.io/tags/Daft/">Daft</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://plotor.github.io/tags/AI/">AI</a></span></div><nav class="post-paginator paginator"><div class="paginator-next"><a class="paginator-next__link" href="/2025/03/31/ray/triton-inference-server/"><span class="paginator-prev__text">Ray Serve 集成 Triton 构建模型在线推理服务</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="utterances-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1-DataFrame-%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.</span> <span class="toc-text">
          图片分类任务 DataFrame 示例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Daft-%E5%88%86%E5%B1%82%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.</span> <span class="toc-text">
          Daft 分层架构设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E9%80%A0%E5%B9%B6%E4%BC%98%E5%8C%96%E9%80%BB%E8%BE%91%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92"><span class="toc-number">3.</span> <span class="toc-text">
          构造并优化逻辑执行计划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E9%80%A0%E5%B9%B6%E4%BC%98%E5%8C%96%E7%89%A9%E7%90%86%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92"><span class="toc-number">4.</span> <span class="toc-text">
          构造并优化物理执行计划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4-Swordfish-%E5%8D%95%E6%9C%BA%E6%89%A7%E8%A1%8C"><span class="toc-number">5.</span> <span class="toc-text">
          提交 Swordfish 单机执行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA-Pipeline-%E7%AE%A1%E9%81%93"><span class="toc-number">5.1.</span> <span class="toc-text">
          构建 Pipeline 管道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C-Pipeline-%E7%AE%A1%E9%81%93"><span class="toc-number">5.2.</span> <span class="toc-text">
          执行 Pipeline 管道</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AF%AD"><span class="toc-number">6.</span> <span class="toc-text">
          结语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">7.</span> <span class="toc-text">
          参考</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/author.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">追求技术深度，注重文章质量</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/plotor" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://weibo.com/" target="_blank" rel="noopener" data-popover="微博" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weibo"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="微信" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weixin"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="QQ" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-qq"></i></span></a><a class="sidebar-ov-social-item" href="https://twitter.com/" target="_blank" rel="noopener" data-popover="Twitter" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-twitter"></i></span></a></div><div class="sidebar-ov-feed"><span class="sidebar-ov-feed-rss"><a class="sidebar-ov-feed-rss__link" href="/atom.xml" target="_blank" rel="noopener"><span class="sidebar-ov-feed-rss__icon"><i class="fas fa-rss"></i></span><span>RSS 订阅</span></a></span></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">97</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">15</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">31</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2015~2025</span><span class="footer__devider"></span><span>Zhenchao All Rights Reserved</span><span class="footer__devider">|</span><span>浙ICP备 16010916 号</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.3.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script>function initSearch() {
  var isXML = true;
  var search_path = 'search.json';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;
  $.ajax({
    url: path,
    dataType: isXML ? 'xml' : 'json',
    async: true,
    success: function (res) {
      var datas = isXML ? $('entry', res).map(function () {
        // 将 XML 转为 JSON
        return {
          title: $('title', this).text(),
          content: $('content', this).text(),
          url: $('url', this).text()
        };
      }).get() : res;
      var $input = $('.search-input input');
      var $result = $('.search-results');
      // 搜索对象（标题、内容）的权重，影响显示顺序
      var WEIGHT = { title: 100, content: 1 };
      var searchPost = function () {
        var searchText = $input.val().toLowerCase().trim();
        // 根据空白字符分隔关键字
        var keywords = searchText.split(/[\s]+/);
        // 搜索结果
        var matchPosts = [];

        // 有多个关键字时，将原文字整个保存下来
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        // 防止未输入字符时搜索
        if (searchText.length > 0) {
          datas.forEach(function (data) {
            var isMatch  = false;
            // 没有标题的文章使用预设的 i18n 变量代替
            var title = (data.title && data.title.trim()) || '[ 文章无标题 ]';
            var titleLower = title && title.toLowerCase();
            // 删除 HTML 标签 和 所有空白字符
            var content = data.content && data.content.replace(/<[^>]+>/g, '');
            var contentLower = content && content.toLowerCase();
            // 删除重复的 /
            var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');
            // 标题中匹配到的关键词
            var titleHitSlice = [];
            // 内容中匹配到的关键词
            var contentHitSlice = [];

            keywords.forEach(function (keyword) {
              /**
              * 获取匹配的关键词的索引
              * @param {String} keyword 要匹配的关键字
              * @param {String} text 原文字
              * @param {Boolean} caseSensitive 是否区分大小写
              * @param {Number} weight 匹配对象的权重。权重大的优先显示
              * @return {Array}
              */
              function getIndexByword (word, text, caseSensitive, weight) {
                if (!word || !text) {
                  return [];
                };

                var startIndex = 0; // 每次匹配的开始索引
                var index = -1;     // 匹配到的索引值
                var result = [];    // 匹配结果

                if (!caseSensitive) {
                  word = word.toLowerCase();
                  text = text.toLowerCase();
                }

                while((index = text.indexOf(word, startIndex)) !== -1) {
                  var hasMatch = false;
                  // 索引位置相同的关键词，保留长度较长的
                  titleHitSlice.forEach(function (hit) {
                    if (hit.index === index && hit.word.length < word.length) {
                      hit.word = word;
                      hasMatch = true;
                    }
                  });
                  startIndex = index + word.length;
                  !hasMatch && result.push({ index: index, word: word, weight: weight });
                }
                return result;
              }
              titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
              contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
            });

            var hitTitle = titleHitSlice.length;
            var hitContent = contentHitSlice.length;

            if (hitTitle > 0 || hitContent > 0) {
              isMatch = true;
            }
            if (isMatch) {
              ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                // 按照匹配文字的索引的递增顺序排序
                hit.sort(function (left, right) {
                  return left.index - right.index;
                });
              });
              /**
              * 给文本中匹配到的关键词添加标记，从而进行高亮显示
              * @param {String} text 原文本
              * @param {Array} hitSlice 匹配项的索引信息
              * @param {Number} start 开始索引
              * @param {Number} end 结束索引
              * @return {String}
              */
              function highlightKeyword (text, hitSlice, start, end) {
                if (!text || !hitSlice || !hitSlice.length) {
                  return;
                }

                var result = '';
                var startIndex = start;
                var endIndex = end;
                hitSlice.forEach(function (hit) {
                  if (hit.index < startIndex) {
                    return;
                  }

                  var hitWordEnd = hit.index + hit.word.length;
                  result += text.slice(startIndex, hit.index);
                  result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                  startIndex = hitWordEnd;
                });
                result += text.slice(startIndex, endIndex);
                return result;
              }

              var postData = {};
              // 文章总的搜索权重
              var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
              // 标记匹配关键词后的标题
              var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
              // 标记匹配关键词后的内容
              var postContent;
              // 显示内容的长度
              var SHOW_WORD_LENGTH = 200;
              // 命中关键词前的字符显示长度
              var SHOW_WORD_FRONT_LENGTH = 20;
              var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

              // 截取匹配的第一个字符，前后共 200 个字符来显示
              if (contentHitSlice.length > 0) {
                var firstIndex = contentHitSlice[0].index;
                var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                var end = firstIndex + SHOW_WORD_END_LENGTH;
                postContent = highlightKeyword(content, contentHitSlice, start, end);
              } else { // 未匹配到内容，直接截取前 200 个字符来显示
                postContent = content.slice(0, SHOW_WORD_LENGTH);
              }
              postData.title = postTitle;
              postData.content = postContent;
              postData.url = postURL;
              postData.weight = postWeight;
              matchPosts.push(postData);
            }
          });
        }

        var resultInnerHtml = '';
        if (matchPosts.length) {
          // 按权重递增的顺序排序，使权重大的优先显示
          matchPosts.sort(function (left, right) {
            return right.weight - left.weight;
          });
          resultInnerHtml += '<ul>';
          matchPosts.forEach(function (post) {
            resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
            resultInnerHtml += post.title;
            resultInnerHtml += '</a><div class="search-results-content">';
            resultInnerHtml += post.content;
            resultInnerHtml += '</div></li>';
          });
          resultInnerHtml += '</ul>';
        } else {
          resultInnerHtml += '<div class="search-results-none"><i class="far fa-meh"></i></div>';
        }
        $result.html(resultInnerHtml);
      };
      $input.on('input', searchPost);
      $input.on('keyup', function (e) {
        if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
          searchPost();
        }
      });
    }
  });
}

function closeSearch () {
  $('body').css({ overflow: 'auto' });
  $('.search-popup').css({ display: 'none' });
  $('.search-mask').css({ display: 'none' });
}

window.addEventListener('DOMContentLoaded', function () {
  Stun.utils.pjaxReloadLocalSearch = function () {
    $('.header-nav-search').on('click', function (e) {
      e.stopPropagation();
      $('body').css('overflow', 'hidden');
      $('.search-popup')
        .velocity('stop')
        .velocity('transition.expandIn', {
          duration: 300,
          complete: function () {
            $('.search-popup input').focus();
          }
        });
      $('.search-mask')
        .velocity('stop')
        .velocity('transition.fadeIn', {
          duration: 300
        });

      initSearch();
    });
    $('.search-mask, .search-close').on('click', function () {
      closeSearch();
    });
    $(document).on('keydown', function (e) {
      // Escape <=> 27
      if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
        closeSearch();
      }
    });
  };

  Stun.utils.pjaxReloadLocalSearch();
}, false);

function safeOpenUrl(url) {
  var newTab = window.open();
  newTab.opener = null;
  newTab.location = url;
}

function extSearch(engine) {
  var engines = {
    google: 'https://www.google.com/search?q=',
    bing: 'https://cn.bing.com/search?q=',
    baidu: 'https://www.baidu.com/s?ie=UTF-8&wd=',
  };
  var host = window.location.host;
  var query = $('.search-input input').val().toLowerCase().trim();
  var uri = engines[engine] + query + ' site:' + host;

  if (query) {
    safeOpenUrl(uri);
  } else {
    Stun.utils.popAlert('warning', '请输入字符');
  }
}

var assistSearchList = window.CONFIG.assistSearch;

if (Array.isArray(assistSearchList)) {
  assistSearchList.forEach(function (name) {
    document.querySelector('.search-btns-item--' + name).addEventListener('click', function () {
      extSearch(name);
    }, false);
  });
}</script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload",".header-inner"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (true) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"></div><script data-pjax="">function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'plotor/hexo-comments');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (true) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (true) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.6.1"></script><script src="/js/stun-boot.js?v=2.6.1"></script><script src="/js/scroll.js?v=2.6.1"></script><script src="/js/header.js?v=2.6.1"></script><script src="/js/sidebar.js?v=2.6.1"></script><script type="application/json" src="/search.json"></script></body></html>