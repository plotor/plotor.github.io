<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/favicon_16.png?v=2.6.1" type="image/png" sizes="16x16"><link rel="icon" href="/images/favicon_32.png?v=2.6.1" type="image/png" sizes="32x32"><meta name="google-site-verification" content="O5CNgi37yYXs3qQp7Xz61oL_AmGiwM28d7hRt5yh2to"><meta name="baidu-site-verification" content="pnKVynCWMP"><meta name="description" content="在分布式应用中，通常会引入冗余策略来保证集群中节点在宕机时的服务可用性，Kafka 在设计上也是如此。Kafka 会为每个 topic 分区创建多个副本，并将这些副本分散在多台 broker 节点上，以避免单点问题。一个分区的副本集合包含一个 leader 角色和多个 follower 角色，其中 leader 副本主要负责响应客户端对于指定 topic 分区消息的读写，并管理集合中的其它 fol">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka 源码解析：分区多副本容错机制">
<meta property="og:url" content="https://plotor.github.io/2019/06/24/kafka/kafka-replica/index.html">
<meta property="og:site_name" content="指  间">
<meta property="og:description" content="在分布式应用中，通常会引入冗余策略来保证集群中节点在宕机时的服务可用性，Kafka 在设计上也是如此。Kafka 会为每个 topic 分区创建多个副本，并将这些副本分散在多台 broker 节点上，以避免单点问题。一个分区的副本集合包含一个 leader 角色和多个 follower 角色，其中 leader 副本主要负责响应客户端对于指定 topic 分区消息的读写，并管理集合中的其它 fol">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2019-06-24T07:45:56.000Z">
<meta property="article:modified_time" content="2025-04-19T09:13:58.225Z">
<meta property="article:author" content="zhenchao">
<meta property="article:tag" content="Kafka">
<meta name="twitter:card" content="summary"><title>Kafka 源码解析：分区多副本容错机制 | 指  间</title><link ref="canonical" href="https://plotor.github.io/2019/06/24/kafka/kafka-replica/"><link rel="alternate" href="/atom.xml" type="application/atom+xml"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"carbon","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: {"avoidBanner":false},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user-circle"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-search"><span class="header-nav-search__icon"><i class="fas fa-search"></i></span><span class="header-nav-search__text">搜索</span></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Kafka 源码解析：分区多副本容错机制</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2019-06-24</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">18.5k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">88分</span></span></div></header><div class="post-body"><p>在分布式应用中，通常会引入冗余策略来保证集群中节点在宕机时的服务可用性，Kafka 在设计上也是如此。Kafka 会为每个 topic 分区创建多个副本，并将这些副本分散在多台 broker 节点上，以避免单点问题。一个分区的副本集合包含一个 leader 角色和多个 follower 角色，其中 leader 副本主要负责响应客户端对于指定 topic 分区消息的读写，并管理集合中的其它 follower 副本，而 follower 副本则主要负责与 leader 副本间保持数据同步，保证在 leader 副本失效时能够有新的 follower 选举成为新的 leader，以维持 Kafka 服务的正常运行。<a id="more"></a></p>

        <h3 id="Replica-组件">
          <a href="#Replica-组件" class="heading-link"><i class="fas fa-link"></i></a>Replica 组件</h3>
      <p>Replica 类用于定义 Kafka 中的副本，副本除了有前面介绍的 leader 和 follower 角色之分外，也区分 <strong>本地副本</strong> 和 <strong>远程副本</strong> ，其中本地副本是指与其关联的 Log 对象位于相同 broker 节点上，而远程副本的 Log 对象则位于其它 broker 节点上。对于远程副本而言，当前 broker 节点仅维护其 LEO 位置信息。 <strong>远程副本的主要作用在于协助 leader 副本维护分区的 HW 位置值</strong> ，具体过程将在后面分析 HW 位置管理时进行说明。</p>
<p>在前面介绍 Kafka 的日志存储机制时我们知道一个 topic 分区对应一个 Log 对象，而在设计上为了避免单点问题，一个 topic 分区又会包含多个副本，这些副本分布在多个不相同的 broker 节点上，如果某个副本正好位于其所属的 Log 对象所在的 broker 节点上，我们称之为本地副本，否则即为远程副本。</p>
<p>下面来看一下 Replica 类的字段定义：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Replica</span>(<span class="params">val brokerId: <span class="type">Int</span>, // 当前副本所在的 broker 的 <span class="type">ID</span></span></span></span><br><span class="line"><span class="class"><span class="params">              val partition: <span class="type">Partition</span>, // 当前副本所属的 topic 分区对象</span></span></span><br><span class="line"><span class="class"><span class="params">              time: <span class="type">Time</span> = <span class="type">Time</span>.<span class="type">SYSTEM</span>, // 时间戳工具</span></span></span><br><span class="line"><span class="class"><span class="params">              initialHighWatermarkValue: <span class="type">Long</span> = 0L, // 初始 <span class="type">HW</span> 值</span></span></span><br><span class="line"><span class="class"><span class="params">              val log: <span class="type">Option</span>[<span class="type">Log</span>] = <span class="type">None</span> // 当前副本所属的 <span class="type">Log</span> 对象，如果是远程副本，该字段为空，通过该字段可以区分是本地副本还是远程副本</span></span></span><br><span class="line"><span class="class"><span class="params">             </span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 记录副本的 HW 值，消费者只能读取 HW 之前的消息，之后的消息对消费者不可见，</span></span><br><span class="line"><span class="comment">     * 由 leader 副本维护，当消息被 ISR 集合中所有副本成功同步时更新该字段。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> highWatermarkMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(initialHighWatermarkValue)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 记录副本所属 Log 对象最后一条消息的 offset 值：</span></span><br><span class="line"><span class="comment">     * - 如果是本地副本，可以直接从 Log#nextOffsetMetadata 字段中获取；</span></span><br><span class="line"><span class="comment">     * - 如果是远程副本，则由其它 broker 发送请求来更新该值。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> logEndOffsetMetadata = <span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span></span><br><span class="line">    <span class="comment">/** 缓存上次从 leader 拉取消息时 leader 副本的 LEO 值 */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> lastFetchLeaderLogEndOffset = <span class="number">0</span>L</span><br><span class="line">    <span class="comment">/** 记录上次从 leader 拉取消息的时间戳 */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> lastFetchTimeMs = <span class="number">0</span>L</span><br><span class="line">    <span class="comment">/** 记录当前 follower 从 leader 拉取消息的最近一次时间戳，用于标识当前 follower 滞后 leader 的程度 */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> _lastCaughtUpTimeMs = <span class="number">0</span>L</span><br><span class="line">    <span class="comment">/** 副本所属 topic 分区对象 */</span></span><br><span class="line">    <span class="keyword">val</span> topicPartition: <span class="type">TopicPartition</span> = partition.topicPartition</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>对于本地副本来说会持有所属 Log 对象的引用，可以基于这一点来判定当前副本是本地副本还是远程副本。此外，Replica 对象还记录了当前副本的 LEO 和 HW 值，以及最近一次从 leader 副本拉取消息的时间戳，同时还定义了相关方法用于维护这些信息，下面分别来看一下维护 LEO 和 HW 值的方法。</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateLogReadResult</span></span>(logReadResult: <span class="type">LogReadResult</span>) {</span><br><span class="line">    <span class="comment">// 更新 _lastCaughtUpTimeMs 值，记录了 follower 从 leader 拉取消息的最新时间</span></span><br><span class="line">    <span class="keyword">if</span> (logReadResult.info.fetchOffsetMetadata.messageOffset &gt;= logReadResult.leaderLogEndOffset)</span><br><span class="line">        _lastCaughtUpTimeMs = math.max(_lastCaughtUpTimeMs, logReadResult.fetchTimeMs)</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (logReadResult.info.fetchOffsetMetadata.messageOffset &gt;= lastFetchLeaderLogEndOffset)</span><br><span class="line">             _lastCaughtUpTimeMs = math.max(_lastCaughtUpTimeMs, lastFetchTimeMs)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果当前副本是远程副本，则更新当前副本的 LEO 值</span></span><br><span class="line">    logEndOffset = logReadResult.info.fetchOffsetMetadata</span><br><span class="line">    <span class="comment">// 更新本地记录的从 leader 拉取消息时 leader 副本的 LEO 值</span></span><br><span class="line">    lastFetchLeaderLogEndOffset = logReadResult.leaderLogEndOffset</span><br><span class="line">    <span class="comment">// 更新本地记录的从 leader 拉取消息的时间戳</span></span><br><span class="line">    lastFetchTimeMs = logReadResult.fetchTimeMs</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">logEndOffset_=</span></span>(newLogEndOffset: <span class="type">LogOffsetMetadata</span>) {</span><br><span class="line">    <span class="keyword">if</span> (isLocal) {</span><br><span class="line">        <span class="comment">// 如果是本地副本无需更新 LEO 值，而是由对应 Log 对象的 Log#logEndOffsetMetadata 字段决定</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">s"Should not set log end offset on partition <span class="subst">$topicPartition</span>'s local replica <span class="subst">$brokerId</span>"</span>)</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 如果当前副本是远程副本，则更新副本的 LEO 值</span></span><br><span class="line">        logEndOffsetMetadata = newLogEndOffset</span><br><span class="line">        trace(<span class="string">s"Setting log end offset for replica <span class="subst">$brokerId</span> for partition <span class="subst">$topicPartition</span> to [<span class="subst">$logEndOffsetMetadata</span>]"</span>)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>方法 <code>Replica#updateLogReadResult</code> 用于更新当前 Replica 对象的 LEO 值。对于 follower 来说，当从 leader 完成一次消息同步操作后，follower 会更新本地记录的 LEO 值，并更新相应的时间戳信息，其中 <code>_lastCaughtUpTimeMs</code> 字段用于记录 follower 最近一次成功从 leader 拉取消息的时间戳，可以标识当前 follower 相对于 leader 的滞后程度。</p>
<p>由上面的实现可以看出，只有远程副本需要更新 LEO 值，因为远程副本未持有所属 Log 对象的引用，需要通过本地字段缓存当前副本的 LEO 值。Replica 类定义了 <code>Replica#logEndOffset</code> 方法用于获取当前副本的 LEO 值：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logEndOffset</span></span>: <span class="type">LogOffsetMetadata</span> = <span class="keyword">if</span> (isLocal) log.get.logEndOffsetMetadata <span class="keyword">else</span> logEndOffsetMetadata</span><br></pre></td></tr></tbody></table></div></figure>
<p>对于本地副本来说，可以调用其持有的 Log 对象的 <code>Log#logEndOffsetMetadata</code> 方法直接获取对应的 LEO 值，而对于远程副本来说则返回本地缓存的 LEO 值。</p>
<p>对于 HW 值而言，Replica 同样提供了更新的方法（如下），需要注意的一点是这里仅更新本地副本的 HW 值，因为远程副本所在的 broker 节点仅维护副本的 LEO 位置信息 ：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">highWatermark_=</span></span>(newHighWatermark: <span class="type">LogOffsetMetadata</span>) {</span><br><span class="line">    <span class="keyword">if</span> (isLocal) {</span><br><span class="line">        <span class="comment">// 如果是本地副本，则更新对应的 HW 值</span></span><br><span class="line">        highWatermarkMetadata = newHighWatermark</span><br><span class="line">        trace(<span class="string">s"Setting high watermark for replica <span class="subst">$brokerId</span> partition <span class="subst">$topicPartition</span> to [<span class="subst">$newHighWatermark</span>]"</span>)</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">s"Should not set high watermark on partition <span class="subst">$topicPartition</span>'s non-local replica <span class="subst">$brokerId</span>"</span>)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>同时，Replica 也提供了获取当前副本 HW 值的方法，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">highWatermark</span></span>: <span class="type">LogOffsetMetadata</span> = highWatermarkMetadata</span><br></pre></td></tr></tbody></table></div></figure>

        <h3 id="Partition-组件">
          <a href="#Partition-组件" class="heading-link"><i class="fas fa-link"></i></a>Partition 组件</h3>
      <p>Partition 类用于定义 Kafka 中的分区，一个 topic 可以设置多个分区。前面在介绍 Kafka 架构与核心概念时曾提及过，Kafka 之所以需要引入分区的概念，主要是希望利用分布式系统中的多节点来提升 Kafka 集群的性能和可扩展性。因为一个 topic 的各个分区可以分布在不同的 broker 节点上，进而就能将 topic 的消息数据分散在这些 broker 节点上存储，对于消息的读写压力就可以由这些节点进行分摊。当我们感知到一个 topic 的消息读写量较大时，我们可以适当增加分区的数目来实现扩容的目的。设想如果我们不引入分区策略，而是由一个 broker 节点完整负责一个 topic，考虑每个 topic 之间的消息数据量和读写量可能存在较大差别，那么各个 broker 节点在负载均衡性上也会有较大的差异，最终影响的是集群整体的可用性。</p>
<p>此外，为了保证高可用性，Kafka 会为每个分区设置多个副本，Partition 提供了管理这些副本的方法，包括执行副本角色切换、维护 ISR 集合、管理 HW 值和 LEO 值，以及调用日志存储系统写入日志数据等。</p>
<p>下面来看一下 Partition 类的字段定义：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Partition</span>(<span class="params">val topic: <span class="type">String</span>, // 分区所属的 topic</span></span></span><br><span class="line"><span class="class"><span class="params">                val partitionId: <span class="type">Int</span>, // 分区编号</span></span></span><br><span class="line"><span class="class"><span class="params">                time: <span class="type">Time</span>, // 时间戳工具</span></span></span><br><span class="line"><span class="class"><span class="params">                replicaManager: <span class="type">ReplicaManager</span> // 副本管理</span></span></span><br><span class="line"><span class="class"><span class="params">               </span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** topic 分区对象 */</span></span><br><span class="line">    <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partitionId)</span><br><span class="line">    <span class="comment">/** 当前 broker 的 ID */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> localBrokerId = replicaManager.config.brokerId</span><br><span class="line">    <span class="comment">/** 管理分区日志数据 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> logManager = replicaManager.logManager</span><br><span class="line">    <span class="comment">/** ZK 工具类 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> zkUtils = replicaManager.zkUtils</span><br><span class="line">    <span class="comment">/** AR 集合，维护当前分区全部副本的集合，key 是副本 ID */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> assignedReplicaMap = <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">Int</span>, <span class="type">Replica</span>]</span><br><span class="line">    <span class="comment">/** leader 副本的年代信息 */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> leaderEpoch: <span class="type">Int</span> = <span class="type">LeaderAndIsr</span>.initialLeaderEpoch - <span class="number">1</span></span><br><span class="line">    <span class="comment">/** leader 副本的 ID */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">var</span> leaderReplicaIdOpt: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">None</span></span><br><span class="line">    <span class="comment">/** 当前分区的 ISR 集合 */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">var</span> inSyncReplicas: <span class="type">Set</span>[<span class="type">Replica</span>] = <span class="type">Set</span>.empty[<span class="type">Replica</span>]</span><br><span class="line">    <span class="comment">/** 当前集群控制器的年代信息，会在切换副本角色时进行更新 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> controllerEpoch: <span class="type">Int</span> = <span class="type">KafkaController</span>.<span class="type">InitialControllerEpoch</span> - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>Partition 中提供了多种方法实现，按照功能划分可以将其中的核心方法划分为以下 5 类：</p>
<ol>
<li>副本对象操作：getOrCreateReplica / getReplica / removeReplica</li>
<li>副本角色切换：makeLeader / makeFollower</li>
<li>日志数据操作：delete / appendRecordsToLeader</li>
<li>ISR 集合管理：maybeExpandIsr / maybeShrinkIsr</li>
<li>HW 和 LEO 位置管理：checkEnoughReplicasReachOffset / maybeIncrementLeaderHW / updateReplicaLogReadResult</li>
</ol>
<p>下面按照分类对这些方法逐一进行分析。</p>

        <h4 id="副本对象操作">
          <a href="#副本对象操作" class="heading-link"><i class="fas fa-link"></i></a>副本对象操作</h4>
      <p>Partition 对象定义了 <code>Partition#assignedReplicaMap</code> 字段用于记录了隶属于当前分区的所有副本 Replica 对象，即 AR 集合，并提供了相关方法用于管理该字段。其中 <code>Partition#getReplica</code> 方法和 <code>Partition#removeReplica</code> 方法分别用于从字段中获取和移除指定副本 ID 对应的副本 Replica 对象，实现比较简单。</p>
<p>本小节我们主要对 <code>Partition#getOrCreateReplica</code> 方法进行分析，该方法相对于 <code>Partition#getReplica</code> 方法的区别在于当给定的副本 ID 在本地找不到对应的副本 Replica 对象时，会创建一个新的 Replica 对象。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOrCreateReplica</span></span>(replicaId: <span class="type">Int</span> = localBrokerId): <span class="type">Replica</span> = {</span><br><span class="line">    <span class="comment">// 尝试从 AR 集合中获取 replicaId 对应的 Replica 对象，如果不存在则创建一个</span></span><br><span class="line">    assignedReplicaMap.getAndMaybePut(replicaId, {</span><br><span class="line">        <span class="comment">// 如果是本地副本</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.isReplicaLocal(replicaId)) {</span><br><span class="line">            <span class="comment">// 获取 log 相关配置信息，ZK 中的配置会覆盖默认配置</span></span><br><span class="line">            <span class="keyword">val</span> config = <span class="type">LogConfig</span>.fromProps(logManager.defaultConfig.originals, <span class="type">AdminUtils</span>.fetchEntityConfig(zkUtils, <span class="type">ConfigType</span>.<span class="type">Topic</span>, topic))</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 创建 topic 分区对应的 Log 对象，如果已经存在则直接返回</span></span><br><span class="line">            <span class="keyword">val</span> log = logManager.createLog(topicPartition, config)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 加载对应 log 目录下的 replication-offset-checkpoint 文件，其中记录了每个 topic 分区的 HW 值</span></span><br><span class="line">            <span class="keyword">val</span> checkpoint = replicaManager.highWatermarkCheckpoints(log.dir.getParentFile.getAbsolutePath)</span><br><span class="line">            <span class="keyword">val</span> offsetMap = checkpoint.read()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取当前 topic 分区对应的 HW 值，并与 LEO 比较，选择较小的值作为此副本的 HW 位置</span></span><br><span class="line">            <span class="keyword">val</span> offset = math.min(offsetMap.getOrElse(topicPartition, <span class="number">0</span>L), log.logEndOffset)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 创建 Replica 对象</span></span><br><span class="line">            <span class="keyword">new</span> <span class="type">Replica</span>(replicaId, <span class="keyword">this</span>, time, offset, <span class="type">Some</span>(log))</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 如果是远程副本，无需加载本地对应的日志数据</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">new</span> <span class="type">Replica</span>(replicaId, <span class="keyword">this</span>, time)</span><br><span class="line">    })</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果参数指定的副本 ID 对应的副本 Replica 对象在本地 AR 集合中不存在，则方法会执行创建对应的 Replica 对象。这里区分本地副本和远程副本，对于远程副本来说创建的过程如上述代码所示，比较简单，而对于本地副本来说，因为本地副本持有副本所属分区对应的 Log 对象，所以需要加载相关数据信息，包括配置、初始 HW 值，以及分区对应的 Log 对象。其中构造 Log 对象的过程由 <code>LogManager#createLog</code> 方法实现：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createLog</span></span>(topicPartition: <span class="type">TopicPartition</span>, config: <span class="type">LogConfig</span>): <span class="type">Log</span> = {</span><br><span class="line">    logCreationOrDeletionLock synchronized {</span><br><span class="line">        <span class="comment">// 获取指定 topic 分区对应的 Log 对象</span></span><br><span class="line">        getLog(topicPartition).getOrElse {</span><br><span class="line">            <span class="comment">// 如果存在多个 log 目录，则选择 Log 数目最少的目录</span></span><br><span class="line">            <span class="keyword">val</span> dataDir = <span class="keyword">this</span>.nextLogDir()</span><br><span class="line">            <span class="comment">// 创建当前 topic 分区对应的日志目录</span></span><br><span class="line">            <span class="keyword">val</span> dir = <span class="keyword">new</span> <span class="type">File</span>(dataDir, topicPartition.topic + <span class="string">"-"</span> + topicPartition.partition)</span><br><span class="line">            dir.mkdirs()</span><br><span class="line">            <span class="comment">// 创建 Log 对象</span></span><br><span class="line">            <span class="keyword">val</span> log = <span class="keyword">new</span> <span class="type">Log</span>(dir, config, recoveryPoint = <span class="number">0</span>L, scheduler, time)</span><br><span class="line">            <span class="comment">// 缓存到本地 logs 字段中</span></span><br><span class="line">            logs.put(topicPartition, log)</span><br><span class="line">            info(<span class="string">"Created log for partition [%s,%d] in %s with properties {%s}."</span></span><br><span class="line">                    .format(topicPartition.topic, topicPartition.partition, dataDir.getAbsolutePath, config.originals.asScala.mkString(<span class="string">", "</span>)))</span><br><span class="line">            log</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>

        <h4 id="副本角色切换">
          <a href="#副本角色切换" class="heading-link"><i class="fas fa-link"></i></a>副本角色切换</h4>
      <p>副本有 leader 和 follower 角色之分，Partition 分别提供了 <code>Partition#makeLeader</code> 方法和 <code>Partition#makeFollower</code> 方法用于将本地副本切换成相应的 leader 和 follower 角色。</p>

        <h5 id="切换本地副本为-leader-角色">
          <a href="#切换本地副本为-leader-角色" class="heading-link"><i class="fas fa-link"></i></a>切换本地副本为 leader 角色</h5>
      <p>方法 <code>Partition#makeLeader</code> 用于将本地副本切换成 leader 角色，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeLeader</span></span>(controllerId: <span class="type">Int</span>, partitionStateInfo: <span class="type">PartitionState</span>, correlationId: <span class="type">Int</span>): <span class="type">Boolean</span> = {</span><br><span class="line">    <span class="keyword">val</span> (leaderHWIncremented, isNewLeader) = inWriteLock(leaderIsrUpdateLock) {</span><br><span class="line">        <span class="comment">// 1. 更新本地记录的 controller 的年代信息</span></span><br><span class="line">        controllerEpoch = partitionStateInfo.controllerEpoch</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 获取/创建请求信息中 AR 和 ISR 集合中所有副本对应的 Replica 对象</span></span><br><span class="line">        <span class="keyword">val</span> allReplicas = partitionStateInfo.replicas.asScala.map(_.toInt)</span><br><span class="line">        allReplicas.foreach(replica =&gt; getOrCreateReplica(replica))</span><br><span class="line">        <span class="keyword">val</span> newInSyncReplicas = partitionStateInfo.isr.asScala.map(r =&gt; getOrCreateReplica(r)).toSet</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 移除本地缓存的所有已过期的的副本对象</span></span><br><span class="line">        (assignedReplicas.map(_.brokerId) -- allReplicas).foreach(removeReplica)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 更新本地记录的分区 leader 副本相关信息</span></span><br><span class="line">        inSyncReplicas = newInSyncReplicas <span class="comment">// 更新 ISR 集合</span></span><br><span class="line">        leaderEpoch = partitionStateInfo.leaderEpoch <span class="comment">// 更新 leader 副本的年代信息</span></span><br><span class="line">        zkVersion = partitionStateInfo.zkVersion <span class="comment">// 更新 ZK 的版本信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 检测分区 leader 副本是否发生变化</span></span><br><span class="line">        <span class="keyword">val</span> isNewLeader =</span><br><span class="line">            <span class="keyword">if</span> (leaderReplicaIdOpt.isDefined &amp;&amp; leaderReplicaIdOpt.get == localBrokerId) {</span><br><span class="line">                <span class="literal">false</span> <span class="comment">// 未发生变化</span></span><br><span class="line">            } <span class="keyword">else</span> {</span><br><span class="line">                <span class="comment">// leader 发生变化，更新分区 leader 副本 ID</span></span><br><span class="line">                leaderReplicaIdOpt = <span class="type">Some</span>(localBrokerId)</span><br><span class="line">                <span class="literal">true</span></span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6. 遍历所有的 follower 副本，更新对应副本的相关时间戳信息</span></span><br><span class="line">        <span class="keyword">val</span> leaderReplica = getReplica().get <span class="comment">// 获取 leader 副本 Replica 对象</span></span><br><span class="line">        <span class="keyword">val</span> curLeaderLogEndOffset = leaderReplica.logEndOffset.messageOffset <span class="comment">// 获取 leader 副本的 LEO 值</span></span><br><span class="line">        <span class="keyword">val</span> curTimeMs = time.milliseconds</span><br><span class="line">        (assignedReplicas - leaderReplica).foreach { replica =&gt;</span><br><span class="line">            <span class="keyword">val</span> lastCaughtUpTimeMs = <span class="keyword">if</span> (inSyncReplicas.contains(replica)) curTimeMs <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">            replica.resetLastCaughtUpTime(curLeaderLogEndOffset, curTimeMs, lastCaughtUpTimeMs)</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 7. 如果当前 leader 是新选举出来的，则修正 leader 副本的 HW 值，并重置本地缓存的所有远程副本的相关信息</span></span><br><span class="line">        <span class="keyword">if</span> (isNewLeader) {</span><br><span class="line">            <span class="comment">// 尝试修正新 leader 副本的 HW 值</span></span><br><span class="line">            leaderReplica.convertHWToLocalOffsetMetadata()</span><br><span class="line">            <span class="comment">// 重置本地缓存的所有远程副本的相关信息</span></span><br><span class="line">            assignedReplicas.filter(_.brokerId != localBrokerId).foreach(_.updateLogReadResult(<span class="type">LogReadResult</span>.<span class="type">UnknownLogReadResult</span>))</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 8. 尝试后移 leader 副本的 HW 值</span></span><br><span class="line">        (maybeIncrementLeaderHW(leaderReplica), isNewLeader)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 9. 如果 leader 副本的 HW 值增加了，则尝试执行监听当前 topic 分区的 DelayedFetch 和 DelayedProduce 任务</span></span><br><span class="line">    <span class="keyword">if</span> (leaderHWIncremented) tryCompleteDelayedRequests()</span><br><span class="line"></span><br><span class="line">    isNewLeader</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>切换副本为 leader 角色的整体流程可以概括为：</p>
<ol>
<li>更新本地记录的 kafka controller 的年代信息；</li>
<li>获取分区新的 AR 集合和 ISR 集合中所有副本对应的 Replica 对象，如果不存在则创建；</li>
<li>移除本地缓存的对应分区已经过期的副本 Replica 对象；</li>
<li>更新本地记录的分区 leader 副本的相关信息，包括 ISR 集合、leader 副本的年代信息等；</li>
<li>检测分区 leader 副本是否发生变化，如果当前副本之前是 follower 角色，或者对应的 topic 分区的副本之前未分配给当前 broker 节点，则说明对应 topic 分区的 leader 副本发生了变化；</li>
<li>遍历所有的 follower 副本，更新对应副本的相关时间戳信息，包括最近一次从 leader 副本拉取消息的时间戳，以及 leader 副本的 LEO 值等；</li>
<li>如果当前 leader 副本是新选举出来的，则尝试修正对应副本的 HW 值，并重置本地缓存的所有远程副本的相关信息；</li>
<li>尝试后移 leader 副本的 HW 值；</li>
<li>如果上一步后移了 leader 副本的 HW 值，则尝试执行监听当前 topic 分区的 DelayedFetch 和 DelayedProduce 延时任务，因为等待的条件可能已经满足。</li>
</ol>
<p>其中，方法 <code>Partition#maybeIncrementLeaderHW</code> 用于尝试向后移动 leader 副本的 HW 值，相关实现我们将在本篇的后续部分进行分析。</p>

        <h5 id="切换本地副本为-follower-角色">
          <a href="#切换本地副本为-follower-角色" class="heading-link"><i class="fas fa-link"></i></a>切换本地副本为 follower 角色</h5>
      <p>方法 <code>Partition#makeFollower</code> 用于将本地副本切换成 follower 角色，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeFollower</span></span>(controllerId: <span class="type">Int</span>, partitionStateInfo: <span class="type">PartitionState</span>, correlationId: <span class="type">Int</span>): <span class="type">Boolean</span> = {</span><br><span class="line">    inWriteLock(leaderIsrUpdateLock) {</span><br><span class="line">        <span class="keyword">val</span> allReplicas = partitionStateInfo.replicas.asScala.map(_.toInt)</span><br><span class="line">        <span class="keyword">val</span> newLeaderBrokerId: <span class="type">Int</span> = partitionStateInfo.leader</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 更新本地记录的 controller 的年代信息</span></span><br><span class="line">        controllerEpoch = partitionStateInfo.controllerEpoch</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 获取/创建请求信息中所有副本对应的 Replica 对象</span></span><br><span class="line">        allReplicas.foreach(r =&gt; getOrCreateReplica(r))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 移除本地缓存的所有已过期的的副本对象</span></span><br><span class="line">        (assignedReplicas.map(_.brokerId) -- allReplicas).foreach(removeReplica)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 更新本地记录的分区 leader 副本相关信息，其中 ISR 集合由 leader 副本维护，将 follower 副本上的 ISR 集合置空</span></span><br><span class="line">        inSyncReplicas = <span class="type">Set</span>.empty[<span class="type">Replica</span>]</span><br><span class="line">        leaderEpoch = partitionStateInfo.leaderEpoch <span class="comment">// 更新 leader 副本的年代信息</span></span><br><span class="line">        zkVersion = partitionStateInfo.zkVersion <span class="comment">// 更新 zk 版本信息</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 检测分区 leader 副本是否发生变化，如果发生变化则更新本地记录的 ID 值</span></span><br><span class="line">        <span class="keyword">if</span> (leaderReplicaIdOpt.isDefined &amp;&amp; leaderReplicaIdOpt.get == newLeaderBrokerId) {</span><br><span class="line">            <span class="literal">false</span></span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 发生变化，更新本地记录的分区 leader 副本的 ID</span></span><br><span class="line">            leaderReplicaIdOpt = <span class="type">Some</span>(newLeaderBrokerId)</span><br><span class="line">            <span class="literal">true</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>切换副本为 follower 角色的整体流程可以概括为：</p>
<ol>
<li>更新本地记录的 kafka controller 的年代信息；</li>
<li>获取分区新的 AR 集合中所有副本对应的 Replica 对象，如果不存在则创建；</li>
<li>移除本地缓存的已经过期的副本 Replica 对象；</li>
<li>更新本地记录的分区 leader 副本的相关信息，因为 ISR 集合由 leader 副本管理，所以需要将 follower 副本记录的 ISR 集合置为空；</li>
<li>检测分区 leader 副本是否发生变化，如果发生变化则需要更新本地记录的 leader 副本的 ID。</li>
</ol>
<p>相对于切换成 leader 角色来说，将本地副本切换成 follower 的过程要简单许多。</p>

        <h4 id="日志数据操作">
          <a href="#日志数据操作" class="heading-link"><i class="fas fa-link"></i></a>日志数据操作</h4>
      <p>Partition 提供了 <code>Partition#delete</code> 方法和 <code>Partition#appendRecordsToLeader</code> 方法用于操作日志数据，其中前者用于清空当前分区记录的副本相关信息，包括 AR 集合、ISR 集合，以及 leader 副本的 ID 值等信息，并异步删除分区对应的日志文件和索引文件（由 <code>LogManager#asyncDelete</code> 方法实现，会将日志文件和索引文件添加 <code>.delete</code> 标记删除后缀，并交由定时任务执行删除操作），而后者用于往当前分区的 leader 副本追加消息。删除操作的实现比较简单，这里重点来看一下往 leader 副本追加消息的过程，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecordsToLeader</span></span>(records: <span class="type">MemoryRecords</span>, requiredAcks: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">LogAppendInfo</span> = {</span><br><span class="line">    <span class="keyword">val</span> (info, leaderHWIncremented) = inReadLock(leaderIsrUpdateLock) {</span><br><span class="line">        leaderReplicaIfLocal <span class="keyword">match</span> {</span><br><span class="line">            <span class="comment">// 只有 leader 副本支持追加消息操作</span></span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</span><br><span class="line">                <span class="comment">// 获取 leader 副本对应的 Log 对象</span></span><br><span class="line">                <span class="keyword">val</span> log = leaderReplica.log.get</span><br><span class="line">                <span class="comment">// 对应 min.insync.replicas 配置，表示 ISR 集合的最小值</span></span><br><span class="line">                <span class="keyword">val</span> minIsr = log.config.minInSyncReplicas</span><br><span class="line">                <span class="comment">// 获取当前分区 ISR 集合的大小</span></span><br><span class="line">                <span class="keyword">val</span> inSyncSize = inSyncReplicas.size</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 如果用户指定 acks = -1，但是当前 ISR 集合小于允许的最小值，则不允许追加消息，防止数据丢失</span></span><br><span class="line">                <span class="keyword">if</span> (inSyncSize &lt; minIsr &amp;&amp; requiredAcks == <span class="number">-1</span>) {</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotEnoughReplicasException</span>(</span><br><span class="line">                        <span class="string">"Number of insync replicas for partition %s is [%d], below required minimum [%s]"</span>.format(topicPartition, inSyncSize, minIsr))</span><br><span class="line">                }</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 往 leader 副本的 Log 对象中追加消息</span></span><br><span class="line">                <span class="keyword">val</span> info = log.append(records)</span><br><span class="line">                <span class="comment">// 有新的日志数据被追加，尝试执行监听当前 topic 分区的 DelayedFetch 延时任务</span></span><br><span class="line">                replicaManager.tryCompleteDelayedFetch(<span class="type">TopicPartitionOperationKey</span>(<span class="keyword">this</span>.topic, <span class="keyword">this</span>.partitionId))</span><br><span class="line">                <span class="comment">// 尝试后移 leader 副本的 HW 值</span></span><br><span class="line">                (info, maybeIncrementLeaderHW(leaderReplica))</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果不是 leader 副本，则抛出异常</span></span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NotLeaderForPartitionException</span>(<span class="string">"Leader not local for partition %s on broker %d"</span>.format(topicPartition, localBrokerId))</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果 leader 副本的 HW 值增加了，则尝试执行监听当前 topic 分区的 DelayedFetch 和 DelayedProduce 任务</span></span><br><span class="line">    <span class="keyword">if</span> (leaderHWIncremented) tryCompleteDelayedRequests()</span><br><span class="line"></span><br><span class="line">    info</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>首先我们多次提到的一点是，Kafka 只允许往目标 topic 分区的 leader 副本追加消息，而 follower 只能从 leader 副本同步消息，所以如果当前追加操作的是 follower 副本，则会抛出异常。</p>
<p>对于 leader 副本来说，在具体执行追加操作之前，如果用户指定了 acks 参数为 -1，即要求所有 ISR 副本在全部收到消息后才允许对客户端进行成功响应，那么会先检测当前分区的 ISR 集合中的副本数目是否大于等于配置的阈值（对应 <code>min.insync.replicas</code> 配置），如果数目不达标则会拒绝执行追加操作，防止数据丢失。具体追加消息数据的操作交由 <code>Log#append</code> 方法执行，该方法已经在前面的文章中分析过，这里不再重复撰述。完成了消息数据的追加操作后，Kafka 会立即尝试执行监听当前 topic 分区的 DelayedFetch 延时任务，避免让客户端和 follower 副本等待太久或超时，此外还会尝试后移 leader 副本的 HW 值。</p>

        <h4 id="ISR-集合管理">
          <a href="#ISR-集合管理" class="heading-link"><i class="fas fa-link"></i></a>ISR 集合管理</h4>
      <p>分区 leader 副本的一个重要的职责就是维护当前分区的 ISR 集合。在分布式应用中，考虑网络、机器性能等因素，follower 副本同步 leader 副本数据的状态是在动态变化的，如果一个 follower 副本与 leader 副本之间存在较大的同步延迟，则不应该被加入到 ISR 集合中，否则应该被纳入到 ISR 集合中的一员，从而能够在 leader 副本失效时，竞选成为新的 leader 副本，以保证 Kafka 服务的可用性。</p>
<p>Partition 类型分别定义了 <code>Partition#maybeExpandIsr</code> 方法和 <code>Partition#maybeShrinkIsr</code> 方法，用于将指定的副本在满足条件下加入到 ISR 集合中，以及依据给定的时间阈值将滞后于 leader 副本超过阈值时间的 follower 副本移出 ISR 集合。首先来看一下 <code>Partition#maybeExpandIsr</code> 方法的实现：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeExpandIsr</span></span>(replicaId: <span class="type">Int</span>, logReadResult: <span class="type">LogReadResult</span>) {</span><br><span class="line">    <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) {</span><br><span class="line">        leaderReplicaIfLocal <span class="keyword">match</span> {</span><br><span class="line">            <span class="comment">// 只有当本地副本是 leader 副本时，才执行扩张操作，因为 ISR 集合由 leader 副本维护</span></span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</span><br><span class="line">                <span class="comment">// 获取目标 follower 副本对应的 Replica 对象</span></span><br><span class="line">                <span class="keyword">val</span> replica = getReplica(replicaId).get</span><br><span class="line">                <span class="comment">// 获取 leader 副本对应的 HW 值</span></span><br><span class="line">                <span class="keyword">val</span> leaderHW = leaderReplica.highWatermark</span><br><span class="line">                <span class="comment">// 判断当前 follower 是否应该被加入到 ISR 集合，并在成功加入后更新相关信息</span></span><br><span class="line">                <span class="keyword">if</span> (!inSyncReplicas.contains(replica) <span class="comment">// follower 副本不在 ISR 集合中</span></span><br><span class="line">                        &amp;&amp; assignedReplicas.map(_.brokerId).contains(replicaId) <span class="comment">// AR 集合中包含该 follower 副本</span></span><br><span class="line">                        &amp;&amp; replica.logEndOffset.offsetDiff(leaderHW) &gt;= <span class="number">0</span>) { <span class="comment">// follower 副本的 LEO 已经追赶上 leader 副本的 HW 值</span></span><br><span class="line">                    <span class="comment">// 将 follower 副本添加到 ISR 集合中</span></span><br><span class="line">                    <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas + replica</span><br><span class="line">                    info(<span class="string">s"Expanding ISR for partition <span class="subst">$topicPartition</span> from <span class="subst">${inSyncReplicas.map(_.brokerId).mkString(",")}</span> to <span class="subst">${newInSyncReplicas.map(_.brokerId).mkString(",")}</span>"</span>)</span><br><span class="line">                    <span class="comment">// 更新 ZK 和本地记录的新的 ISR 集合信息</span></span><br><span class="line">                    <span class="keyword">this</span>.updateIsr(newInSyncReplicas)</span><br><span class="line">                    replicaManager.isrExpandRate.mark()</span><br><span class="line">                }</span><br><span class="line">                <span class="comment">// 尝试后移 leader 副本的 HW 值</span></span><br><span class="line">                <span class="keyword">this</span>.maybeIncrementLeaderHW(leaderReplica, logReadResult.fetchTimeMs)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果不是 leader 副本，啥也不干</span></span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果 leader 副本的 HW 值发生变化，尝试执行监听当前 topic 分区的 DelayedFetch 和 DelayedProduce 延时任务</span></span><br><span class="line">    <span class="keyword">if</span> (leaderHWIncremented) tryCompleteDelayedRequests()</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>ISR 集合的扩张和收缩操作均由 leader 副本负责，对于给定的 follower 副本如果同时满足以下条件，则将其添加到 ISR 集合中：</p>
<ol>
<li>目标 follower 副本不在当前分区的 ISR 集合中；</li>
<li>目标 follower 副本位于当前分区的 AR 集合中；</li>
<li>目标 follower 副本的 LEO 值已经追赶上对应 leader 副本的 HW 值。</li>
</ol>
<p>对于同时满足上述条件的 follower 副本，Kafka 会将其添加到对应 topic 分区的 ISR 集合中，并将新的 ISR 集合信息记录到 ZK，同时更新 leader 副本本地记录的 ISR 集合。</p>
<p>方法 <code>Partition#maybeShrinkIsr</code> 用于收缩当前分区的 ISR 集合，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeShrinkIsr</span></span>(replicaMaxLagTimeMs: <span class="type">Long</span>) {</span><br><span class="line">    <span class="keyword">val</span> leaderHWIncremented = inWriteLock(leaderIsrUpdateLock) {</span><br><span class="line">        leaderReplicaIfLocal <span class="keyword">match</span> {</span><br><span class="line">            <span class="comment">// 只有当本地副本是 leader 副本时，才执行缩减操作，因为 ISR 集合由 leader 副本维护</span></span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</span><br><span class="line">                <span class="comment">// 从 ISR 集合中获取滞后的 follower 副本集合</span></span><br><span class="line">                <span class="keyword">val</span> outOfSyncReplicas = <span class="keyword">this</span>.getOutOfSyncReplicas(leaderReplica, replicaMaxLagTimeMs)</span><br><span class="line">                <span class="keyword">if</span> (outOfSyncReplicas.nonEmpty) {</span><br><span class="line">                    <span class="comment">// 将滞后的 follower 副本从 ISR 集合中剔除</span></span><br><span class="line">                    <span class="keyword">val</span> newInSyncReplicas = inSyncReplicas -- outOfSyncReplicas</span><br><span class="line">                    assert(newInSyncReplicas.nonEmpty)</span><br><span class="line">                    info(<span class="string">"Shrinking ISR for partition [%s,%d] from %s to %s"</span>.format(topic, partitionId,</span><br><span class="line">                        inSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>), newInSyncReplicas.map(_.brokerId).mkString(<span class="string">","</span>)))</span><br><span class="line">                    <span class="comment">// 将新的 ISR 集合信息上报给 ZK，同时更新本地记录的 ISR 集合信息</span></span><br><span class="line">                    <span class="keyword">this</span>.updateIsr(newInSyncReplicas)</span><br><span class="line">                    replicaManager.isrShrinkRate.mark()</span><br><span class="line">                    <span class="comment">// 尝试后移 leader 副本的 HW 值</span></span><br><span class="line">                    <span class="keyword">this</span>.maybeIncrementLeaderHW(leaderReplica)</span><br><span class="line">                } <span class="keyword">else</span> {</span><br><span class="line">                    <span class="literal">false</span></span><br><span class="line">                }</span><br><span class="line">            <span class="comment">// 如果不是 leader 副本，则啥也不做</span></span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="literal">false</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果 leader 副本的 HW 值发生变化，尝试执行监听当前 topic 分区的 DelayedFetch 和 DelayedProduce 延时任务</span></span><br><span class="line">    <span class="keyword">if</span> (leaderHWIncremented) tryCompleteDelayedRequests()</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOutOfSyncReplicas</span></span>(leaderReplica: <span class="type">Replica</span>, maxLagMs: <span class="type">Long</span>): <span class="type">Set</span>[<span class="type">Replica</span>] = {</span><br><span class="line">    <span class="comment">// 获取 ISR 集合中所有的 follower 副本</span></span><br><span class="line">    <span class="keyword">val</span> candidateReplicas = inSyncReplicas - leaderReplica</span><br><span class="line">    <span class="comment">// 获取超过给定时间（对应 replica.lag.time.max.ms 配置）未向 leader 副本请求拉取消息的 follower 副本集合</span></span><br><span class="line">    <span class="keyword">val</span> laggingReplicas = candidateReplicas.filter(r =&gt; (time.milliseconds - r.lastCaughtUpTimeMs) &gt; maxLagMs)</span><br><span class="line">    laggingReplicas</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>对于 ISR 集合中的 follower 副本，如果其最近一次成功从 leader 副本拉取数据的时间戳相对于当前时间超过指定的阈值（对应 <code>replica.lag.time.max.ms</code> 配置，默认为 10 秒），则将其从 ISR 集合中移出，而不管当前 follower 副本与 leader 副本的数据延迟差异。一旦 follower 被从 ISR 踢出，Kafka 会将新的 ISR 集合信息上报给 ZK，同时更新 leader 副本本地记录的 ISR 集合。</p>

        <h4 id="HW-和-LEO-位置管理">
          <a href="#HW-和-LEO-位置管理" class="heading-link"><i class="fas fa-link"></i></a>HW 和 LEO 位置管理</h4>
      <p>Partition 定义了 <code>Partition#checkEnoughReplicasReachOffset</code> 方法和 <code>Partition#maybeIncrementLeaderHW</code> 方法，分别用于检测指定 offset 之前的消息是否已经被 ISR 集合中足够多的 follower 副本确认（ack），以及尝试向后移动 leader 副本的 HW 值。先来看一下 <code>Partition#checkEnoughReplicasReachOffset</code> 方法，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkEnoughReplicasReachOffset</span></span>(requiredOffset: <span class="type">Long</span>): (<span class="type">Boolean</span>, <span class="type">Errors</span>) = {</span><br><span class="line">    leaderReplicaIfLocal <span class="keyword">match</span> {</span><br><span class="line">        <span class="comment">// 如果当前副本是 leader 副本</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(leaderReplica) =&gt;</span><br><span class="line">            <span class="comment">// 获取 ISR 集合</span></span><br><span class="line">            <span class="keyword">val</span> curInSyncReplicas = inSyncReplicas</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 对应 min.insync.replicas 配置</span></span><br><span class="line">            <span class="keyword">val</span> minIsr = leaderReplica.log.get.config.minInSyncReplicas</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果当前请求的 offset 小于等于 HW 的 offset</span></span><br><span class="line">            <span class="keyword">if</span> (leaderReplica.highWatermark.messageOffset &gt;= requiredOffset) {</span><br><span class="line">                <span class="comment">// 如果当前分区的 ISR 集合大小大于等于允许的最小值</span></span><br><span class="line">                <span class="keyword">if</span> (minIsr &lt;= curInSyncReplicas.size) (<span class="literal">true</span>, <span class="type">Errors</span>.<span class="type">NONE</span>)</span><br><span class="line">                <span class="comment">// 否则返回 NOT_ENOUGH_REPLICAS_AFTER_APPEND 错误</span></span><br><span class="line">                <span class="keyword">else</span> (<span class="literal">true</span>, <span class="type">Errors</span>.<span class="type">NOT_ENOUGH_REPLICAS_AFTER_APPEND</span>)</span><br><span class="line">            } <span class="keyword">else</span> {</span><br><span class="line">                <span class="comment">// 如果当前请求的 offset 大于 HW，则直接返回 false，因为 HW 之后的消息对于客户端不可见</span></span><br><span class="line">                (<span class="literal">false</span>, <span class="type">Errors</span>.<span class="type">NONE</span>)</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果当前副本是 follower 副本，则返回 NOT_LEADER_FOR_PARTITION 错误</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; (<span class="literal">false</span>, <span class="type">Errors</span>.<span class="type">NOT_LEADER_FOR_PARTITION</span>)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>方法 <code>Partition#checkEnoughReplicasReachOffset</code> 接收一个 requiredOffset 参数，用于检测该 offset 之前的消息是否已经被确认，本质上就是将该 offset 与 leader 副本的 HW 值进行比较，如果 leader 副本的 HW 值大于等于该 offset 值，则认为之前的消息已经全部被确认。</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeIncrementLeaderHW</span></span>(leaderReplica: <span class="type">Replica</span>, curTime: <span class="type">Long</span> = time.milliseconds): <span class="type">Boolean</span> = {</span><br><span class="line">    <span class="comment">// 获取位于 ISR 集合中，或最近一次从 leader 拉取消息的时间戳位于指定时间范围（对应 replica.lag.time.max.ms 配置）内的所有副本的 LEO 值</span></span><br><span class="line">    <span class="keyword">val</span> allLogEndOffsets = assignedReplicas.filter { replica =&gt;</span><br><span class="line">        curTime - replica.lastCaughtUpTimeMs &lt;= replicaManager.config.replicaLagTimeMaxMs || inSyncReplicas.contains(replica)</span><br><span class="line">    }.map(_.logEndOffset)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 以这些副本中最小的 LEO 值作为 leader 副本新的 HW 值</span></span><br><span class="line">    <span class="keyword">val</span> newHighWatermark = allLogEndOffsets.min(<span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>.<span class="type">OffsetOrdering</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 比较新旧 HW 值，如果旧的 HW 小于新的 HW，或者旧的 HW 对应的 LogSegment 的 baseOffset 小于新的 HW 的 LogSegment 对象的 baseOffset，则更新</span></span><br><span class="line">    <span class="keyword">val</span> oldHighWatermark = leaderReplica.highWatermark</span><br><span class="line">    <span class="keyword">if</span> (oldHighWatermark.messageOffset &lt; newHighWatermark.messageOffset || oldHighWatermark.onOlderSegment(newHighWatermark)) {</span><br><span class="line">        leaderReplica.highWatermark = newHighWatermark</span><br><span class="line">        debug(<span class="string">"High watermark for partition [%s,%d] updated to %s"</span>.format(topic, partitionId, newHighWatermark))</span><br><span class="line">        <span class="literal">true</span></span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        debug(<span class="string">"Skipping update high watermark since Old hw %s is larger than new hw %s for partition [%s,%d]. All leo's are %s"</span></span><br><span class="line">                .format(oldHighWatermark, newHighWatermark, topic, partitionId, allLogEndOffsets.mkString(<span class="string">","</span>)))</span><br><span class="line">        <span class="literal">false</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法曾在前面的分析中多次出现，用于尝试后移 leader 副本的 HW 位置，其核心思想是选取 ISR 集合中副本最小的 LEO 值作为 leader 副本的新 HW 值，如果计算出来的 HW 值大于 leader 副本当前的 HW 值，则进行更新。考虑到一些位于 ISR 集合之外但是有机会加入 ISR 集合的副本加入 ISR 集合有一个延迟的过程，所以这里也考虑了这些滞后于 leader 副本时间较小的 follower 副本。</p>
<p>前面我们曾提及过远程副本的作用在于协助 leader 副本更新分区 HW 值，这里我们具体说明一下这一过程。分区 leader 副本所在的 broker 节点以远程副本的形式记录着所有 follower 副本的 LEO 值，当 follower 副本从 leader 副本同步数据时会告知 leader 副本从什么位置开始拉取数据，leader 副本会使用该 offset 值更新远程副本的 LEO 位置值。当 leader 副本需要更新分区 HW 值时会从所有远程副本中筛选出那些位于 ISR 集合中，或者与 leader 副本之间同步时间间隔位于 <code>replica.lag.time.max.ms</code> 内的副本，当这些副本中最小的 LEO 值大于当前 leader 副本的 HW 值时，则更新 leader 副本的 HW 值。</p>
<p>Partition 提供了 <code>Partition#updateReplicaLogReadResult</code> 方法用于更新指定 follower 副本的 LEO 值（具体通过调用 <code>Replica#updateLogReadResult</code> 方法实现），并在完成更新之后尝试调用 <code>Partition#maybeExpandIsr</code> 方法来扩张 ISR 集合，整体过程实现比较简单，不再展开。</p>
<p>Follower 副本在与 leader 副本进行数据同步时，会将从 leader 副本获取到的 HW 值与当前副本的 LEO 值进行比对，并选择较小者作为当前 follower 副本的 HW 值。这样就产生了一个问题，即 follower 副本的 HW 值与 leader 副本的 HW 值是有差距的，当选举某个 HW 滞后的 follower 副本作为新的 leader 时需要对数据进行截断，从而存在丢失消息的风险。为此，Kafka 0.11 版本引入了 Leader Epoch 机制以解决这一问题，关于 Leader Epoch 机制我们以后再补充说明。</p>

        <h3 id="ReplicaManager-组件">
          <a href="#ReplicaManager-组件" class="heading-link"><i class="fas fa-link"></i></a>ReplicaManager 组件</h3>
      <p>ReplicaManager 类用于管理分布在当前 broker 节点上的所有分区的副本信息，主要提供了创建并获取指定 topic 分区对象、副本管理、日志数据读写、副本角色转换，以及更新当前 broker 节点缓存的整个集群中全部分区的状态信息等功能。ReplicaManager 的字段定义如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplicaManager</span>(<span class="params">val config: <span class="type">KafkaConfig</span>, // 相关配置对象</span></span></span><br><span class="line"><span class="class"><span class="params">                     metrics: <span class="type">Metrics</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                     time: <span class="type">Time</span>, // 时间戳工具</span></span></span><br><span class="line"><span class="class"><span class="params">                     val zkUtils: <span class="type">ZkUtils</span>, // <span class="type">ZK</span> 工具类</span></span></span><br><span class="line"><span class="class"><span class="params">                     scheduler: <span class="type">Scheduler</span>, // 定时任务调度器</span></span></span><br><span class="line"><span class="class"><span class="params">                     val logManager: <span class="type">LogManager</span>, // 用于对分区日志数据执行读写操作</span></span></span><br><span class="line"><span class="class"><span class="params">                     val isShuttingDown: <span class="type">AtomicBoolean</span>, // 标记 kafka 服务是否正在执行关闭操作</span></span></span><br><span class="line"><span class="class"><span class="params">                     quotaManager: <span class="type">ReplicationQuotaManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                     threadNamePrefix: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 记录 kafka controller 的年代信息，当重新选择 controller leader 时会递增该字段，</span></span><br><span class="line"><span class="comment">     * 用于校验来自 controller 的请求的年代信息，防止处理来自老的 controller 的请求</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">var</span> controllerEpoch: <span class="type">Int</span> = <span class="type">KafkaController</span>.<span class="type">InitialControllerEpoch</span> - <span class="number">1</span></span><br><span class="line">    <span class="comment">/** 本地 broker 的 ID */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> localBrokerId = config.brokerId</span><br><span class="line">    <span class="comment">/** 记录当前 broker 管理的所有分区信息，如果不存在则创建 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> allPartitions = <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">TopicPartition</span>, <span class="type">Partition</span>](<span class="type">Some</span>(tp =&gt; <span class="keyword">new</span> <span class="type">Partition</span>(tp.topic, tp.partition, time, <span class="keyword">this</span>)))</span><br><span class="line">    <span class="comment">/** 管理向 leader 副本发送 FetchRequest 请求的 ReplicaFetcherThread 线程 */</span></span><br><span class="line">    <span class="keyword">val</span> replicaFetcherManager = <span class="keyword">new</span> <span class="type">ReplicaFetcherManager</span>(config, <span class="keyword">this</span>, metrics, time, threadNamePrefix, quotaManager)</span><br><span class="line">    <span class="comment">/** 标记 highwatermark-checkpoint 定时任务是否已经启动 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> highWatermarkCheckPointThreadStarted = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line">    <span class="comment">/** 记录每个 log 目录与对应 topic 分区 HW 值的映射关系 */</span></span><br><span class="line">    <span class="keyword">val</span> highWatermarkCheckpoints: <span class="type">Predef</span>.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">OffsetCheckpoint</span>] = config.logDirs.map(dir =&gt;</span><br><span class="line">        (<span class="keyword">new</span> <span class="type">File</span>(dir).getAbsolutePath, <span class="keyword">new</span> <span class="type">OffsetCheckpoint</span>(<span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">ReplicaManager</span>.<span class="type">HighWatermarkFilename</span>)))).toMap</span><br><span class="line">    <span class="comment">/** 标记 highwatermark-checkpoint 定时任务是否已经启动 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> hwThreadInitialized = <span class="literal">false</span></span><br><span class="line">    <span class="comment">/** 记录 ISR 集合发生变化的 topic 分区信息 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> isrChangeSet: mutable.<span class="type">Set</span>[<span class="type">TopicPartition</span>] = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">TopicPartition</span>]()</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> lastIsrChangeMs = <span class="keyword">new</span> <span class="type">AtomicLong</span>(<span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> lastIsrPropagationMs = <span class="keyword">new</span> <span class="type">AtomicLong</span>(<span class="type">System</span>.currentTimeMillis())</span><br><span class="line">    <span class="comment">/** 管理 DelayedProduce 延时任务的炼狱 */</span></span><br><span class="line">    <span class="keyword">val</span> delayedProducePurgatory: <span class="type">DelayedOperationPurgatory</span>[<span class="type">DelayedProduce</span>] = <span class="type">DelayedOperationPurgatory</span>[<span class="type">DelayedProduce</span>](</span><br><span class="line">        purgatoryName = <span class="string">"Produce"</span>, localBrokerId, config.producerPurgatoryPurgeIntervalRequests)</span><br><span class="line">    <span class="comment">/** 管理 DelayedFetch 延时任务的炼狱 */</span></span><br><span class="line">    <span class="keyword">val</span> delayedFetchPurgatory: <span class="type">DelayedOperationPurgatory</span>[<span class="type">DelayedFetch</span>] = <span class="type">DelayedOperationPurgatory</span>[<span class="type">DelayedFetch</span>](</span><br><span class="line">        purgatoryName = <span class="string">"Fetch"</span>, localBrokerId, config.fetchPurgatoryPurgeIntervalRequests)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ... 省略相关方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>Kafka 服务在启动时会创建 ReplicaManager 对象，并调用 <code>ReplicaManager#startup</code> 方法启动 ReplicaManager 管理的定时任务，即 isr-expiration 和 isr-change-propagation 定时任务。实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() {</span><br><span class="line">    <span class="comment">// 定时检测当前 broker 节点管理的每个分区是否需要缩减 ISR 集合，并执行缩减操作</span></span><br><span class="line">    scheduler.schedule(<span class="string">"isr-expiration"</span>, maybeShrinkIsr, period = config.replicaLagTimeMaxMs / <span class="number">2</span>, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    <span class="comment">// 定时将 ISR 集合发生变化的 topic 分区记录到 ZK</span></span><br><span class="line">    scheduler.schedule(<span class="string">"isr-change-propagation"</span>, maybePropagateIsrChanges, period = <span class="number">2500</span>L, unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>定时任务 isr-expiration 周期性执行 <code>ReplicaManager#maybeShrinkIsr</code> 方法，尝试缩减当前 broker 节点管理的分区对应的 ISR 集合，具体缩减操作由 <code>Partition#maybeShrinkIsr</code> 方法实现，前面已经分析过，不再重复撰述。</p>
<p>定时任务 isr-change-propagation 周期性将 ISR 集合发生变化的 topic 副本信息更新到 ZK 相应节点下，Kafka 集群控制器基于 ZK 的 Watcher 机制监听相应节点，并在节点内容发生变化时向所有可用的 broker 节点发送 UpdateMetadataRequest 请求，以更新相应 broker 节点本地管理的整个集群中所有分区的状态信息。定时任务的执行逻辑由 <code>ReplicaManager#maybePropagateIsrChanges</code> 方法实现：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybePropagateIsrChanges</span></span>() {</span><br><span class="line">    <span class="keyword">val</span> now = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    isrChangeSet synchronized {</span><br><span class="line">        <span class="comment">// 定期将 ISR 集合发生变化的分区记录到 ZK，kafka controller 对相应 ZK 路径添加了 Watcher，</span></span><br><span class="line">        <span class="comment">// 当 Watcher 被触发后会向所有可用的 broker 节点发送 UpdateMetadataRequest 请求，以更新 broker 节点缓存的所有分区状态信息</span></span><br><span class="line">        <span class="keyword">if</span> (isrChangeSet.nonEmpty &amp;&amp;</span><br><span class="line">                <span class="comment">// 最后一次有 ISR 集合发生变化的时间距离现在已经超过 5 秒</span></span><br><span class="line">                (lastIsrChangeMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationBlackOut</span> &lt; now</span><br><span class="line">                        <span class="comment">// 上次写入 ZK 的时间距离现在已经超过 1 分钟</span></span><br><span class="line">                        || lastIsrPropagationMs.get() + <span class="type">ReplicaManager</span>.<span class="type">IsrChangePropagationInterval</span> &lt; now)) {</span><br><span class="line">            <span class="comment">// 将 ISR 集合发生变更的 topic 分区信息记录到 ZK</span></span><br><span class="line">            <span class="type">ReplicationUtils</span>.propagateIsrChanges(zkUtils, isrChangeSet)</span><br><span class="line">            isrChangeSet.clear()</span><br><span class="line">            lastIsrPropagationMs.set(now)</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>为了避免频繁操作 ZK，上述方法在设计上添加了一定的过滤条件，只有当最近一次 ISR 集合变化的时间距离现在超过 5 秒，或者距离上一次操作 ZK 已经超过 1 分钟，才允许再次操作 ZK。Kafka Controller 在成为 leader 角色时会在相应 ZK 路径上注册 Watcher 监听器，当监听到有数据变化时，会构建 UpdateMetadataRequest 请求对象发送给所有可用的 broker 节点，以更新 broker 节点本地缓存的整个集群所有分区的状态信息。</p>
<p>ReplicaManager 提供了 <code>ReplicaManager#maybeUpdateMetadataCache</code> 方法来处理 UpdateMetadataRequest 请求，方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maybeUpdateMetadataCache</span></span>(correlationId: <span class="type">Int</span>,</span><br><span class="line">                             updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>,</span><br><span class="line">                             metadataCache: <span class="type">MetadataCache</span>): <span class="type">Seq</span>[<span class="type">TopicPartition</span>] = {</span><br><span class="line">    replicaStateChangeLock synchronized {</span><br><span class="line">        <span class="comment">// 校验 controller 的年代信息，避免处理来自已经过期的 controller 的请求</span></span><br><span class="line">        <span class="keyword">if</span> (updateMetadataRequest.controllerEpoch &lt; controllerEpoch) {</span><br><span class="line">            <span class="keyword">val</span> stateControllerEpochErrorMessage = (<span class="string">"Broker %d received update metadata request with correlation id %d from an "</span> +</span><br><span class="line">                    <span class="string">"old controller %d with epoch %d. Latest known controller epoch is %d"</span>).format(localBrokerId,</span><br><span class="line">                correlationId, updateMetadataRequest.controllerId, updateMetadataRequest.controllerEpoch, controllerEpoch)</span><br><span class="line">            stateChangeLogger.warn(stateControllerEpochErrorMessage)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">ControllerMovedException</span>(stateControllerEpochErrorMessage)</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 更新所有分区的状态信息，并返回需要被移除的分区集合</span></span><br><span class="line">            <span class="keyword">val</span> deletedPartitions = metadataCache.updateCache(correlationId, updateMetadataRequest)</span><br><span class="line">            <span class="comment">// 更新本地缓存的 controller 年代信息</span></span><br><span class="line">            controllerEpoch = updateMetadataRequest.controllerEpoch</span><br><span class="line">            deletedPartitions</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法首先会校验当前 UpdateMetadataRequest 请求的年代信息，避免处理那些来自老的 kafka controller 的请求。对于合法的 UpdateMetadataRequest 请求，则会调用 <code>MetadataCache#updateCache</code> 方法更新所有分区的状态信息，并返回需要被移除的分区集合，同时更新本地缓存的 kafka controller 的年代信息。关于 MetadataCache 类的实现，留到后面针对性分析，这里先不展开。</p>
<p>除了上面介绍的 2 个定时任务以外，ReplicaManager 还定义了另外一个定时任务 highwatermark-checkpoint，该任务周期性将当前 broker 节点管理的每个 topic 分区的 HW 值更新到对应 log 目录下的 replication-offset-checkpoint 文件中。相关逻辑由 <code>ReplicaManager#startHighWaterMarksCheckPointThread</code> 方法实现：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startHighWaterMarksCheckPointThread</span></span>(): <span class="type">Unit</span> = {</span><br><span class="line">    <span class="keyword">if</span> (highWatermarkCheckPointThreadStarted.compareAndSet(<span class="literal">false</span>, <span class="literal">true</span>))</span><br><span class="line">        scheduler.schedule(</span><br><span class="line">            <span class="string">"highwatermark-checkpoint"</span>,</span><br><span class="line">            checkpointHighWatermarks,</span><br><span class="line">            period = config.replicaHighWatermarkCheckpointIntervalMs,</span><br><span class="line">            unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpointHighWatermarks</span></span>() {</span><br><span class="line">    <span class="comment">// 获取所有分区全部的本地副本 Replica 对象</span></span><br><span class="line">    <span class="keyword">val</span> replicas = allPartitions.values.flatMap(_.getReplica(localBrokerId))</span><br><span class="line">    <span class="comment">// 按照副本所在的 log 目录进行分组</span></span><br><span class="line">    <span class="keyword">val</span> replicasByDir = replicas.filter(_.log.isDefined).groupBy(_.log.get.dir.getParentFile.getAbsolutePath)</span><br><span class="line">    <span class="comment">// 遍历将位于相同 log 目录下的分区 HW 值，写入到对应的 replication-offset-checkpoint 文件中</span></span><br><span class="line">    <span class="keyword">for</span> ((dir, reps) &lt;- replicasByDir) {</span><br><span class="line">        <span class="comment">// 获取每个 topic 分区对应的 HW 值</span></span><br><span class="line">        <span class="keyword">val</span> hwms: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = reps.map(r =&gt; r.partition.topicPartition -&gt; r.highWatermark.messageOffset).toMap</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// 更新对应 log 目录下的 replication-offset-checkpoint 文件</span></span><br><span class="line">            highWatermarkCheckpoints(dir).write(hwms)</span><br><span class="line">        } <span class="keyword">catch</span> {</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt;</span><br><span class="line">                fatal(<span class="string">"Error writing to highwatermark file: "</span>, e)</span><br><span class="line">                <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>具体执行逻辑如代码注释，比较简单。该定时任务会在当前 ReplicaManager 首次收到来自 kafka controller 的 LeaderAndIsrRequest 请求时被启动。</p>

        <h4 id="消息同步机制">
          <a href="#消息同步机制" class="heading-link"><i class="fas fa-link"></i></a>消息同步机制</h4>
      <p>为了支持在 topic 分区 leader 副本失效时，有新的副本可以继续对外提供服务，Kafka 为副本引入了 leader/follower 模型设计，follower 副本在平时并不负责与客户端进行交互，主要职责在于从 leader 副本同步消息数据，以备在 leader 副本失效时可以从所有符合条件的 follower 副本中选举一个新的 leader 副本，从而避免对应 topic 的长时间停车，本小节我们重点来分析一下 follower 副本从 leader 副本同步消息的操作。</p>
<p>ReplicaManager 使用 ReplicaFetcherManager 管理 follower 副本与 leader 副本的同步工作，ReplicaFetcherManager 继承自 AbstractFetcherManager 抽象类。ReplicaFetcherManager 将当前 broker 节点管理的分区对应的副本按照一定的条件进行分组，并为每个组创建一个 fetcher 线程，用于从对应 leader 副本所在的 broker 节点拉取指定 offset 的消息数据。</p>
<p>Fetcher 线程由 ReplicaFetcherThread 实现，ReplicaFetcherThread 继承自 AbstractFetcherThread 抽象类。每个 ReplicaFetcherManager 维护了一个 <code>HashMap[BrokerAndFetcherId, AbstractFetcherThread]</code> 类型的 <code>AbstractFetcherManager#fetcherThreadMap</code> 集合，用于记录每个分组对应的 fetcher 线程对象，其中 BrokerAndFetcherId 封装了目标 broker 节点的 id、host、port，以及对应 fetcher 线程 ID 等信息。</p>
<p>ReplicaFetcherManager 提供了多个方法用于管理 <code>AbstractFetcherManager#fetcherThreadMap</code> 集合，主要包括：</p>
<ol>
<li><code>AbstractFetcherManager#addFetcherForPartitions</code>：将指定的待同步 topic 分区分组，并为每个分组创建并启动一个 fetcher 线程，从指定的 offset 开始与 leader 副本进行同步。</li>
<li><code>AbstractFetcherManager#removeFetcherForPartitions</code>：停止对指定 topic 分区集合的副本同步任务。</li>
<li><code>AbstractFetcherManager#shutdownIdleFetcherThreads</code>：关闭空闲的 fetcher 线程，相应线程不再为任何 topic 分区执行同步工作。</li>
</ol>
<p>上述方法中 2 和 3 在实现上都比较简单，下面重点来看一下方法 1，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addFetcherForPartitions</span></span>(partitionAndOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">BrokerAndInitialOffset</span>]) {</span><br><span class="line">    mapLock synchronized {</span><br><span class="line">        <span class="keyword">val</span> partitionsPerFetcher = partitionAndOffsets.groupBy {</span><br><span class="line">            <span class="keyword">case</span> (topicPartition, brokerAndInitialOffset) =&gt;</span><br><span class="line">                <span class="comment">// 由分区所属的 topic 和分区编号计算得到对应的 fetcher 线程 ID，并与 broker 的网络位置信息组成 key，然后按 key 进行分组，</span></span><br><span class="line">                <span class="comment">// 后面会为每组分配一个 fetcher 线程，每个线程只连接一个 broker，可以同时为组内多个分区的 follower 副本执行同步操作。</span></span><br><span class="line">                <span class="type">BrokerAndFetcherId</span>(brokerAndInitialOffset.broker, <span class="keyword">this</span>.getFetcherId(topicPartition.topic, topicPartition.partition))</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动所有的的 fetcher 线程，如果对应线程不存在，则创建并启动</span></span><br><span class="line">        <span class="keyword">for</span> ((brokerAndFetcherId, partitionAndOffsets) &lt;- partitionsPerFetcher) {</span><br><span class="line">            <span class="keyword">var</span> fetcherThread: <span class="type">AbstractFetcherThread</span> = <span class="literal">null</span></span><br><span class="line">            fetcherThreadMap.get(brokerAndFetcherId) <span class="keyword">match</span> {</span><br><span class="line">                <span class="keyword">case</span> <span class="type">Some</span>(f) =&gt; fetcherThread = f</span><br><span class="line">                <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">                    <span class="comment">// 创建 ReplicaFetcherThread 线程对象，并记录到 fetcherThreadMap 集合中</span></span><br><span class="line">                    fetcherThread = <span class="keyword">this</span>.createFetcherThread(brokerAndFetcherId.fetcherId, brokerAndFetcherId.broker)</span><br><span class="line">                    fetcherThreadMap.put(brokerAndFetcherId, fetcherThread)</span><br><span class="line">                    fetcherThread.start() <span class="comment">// 启动线程</span></span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将 topic 分区和同步起始位置传递给 fetcher 线程，并唤醒 fetcher 线程开始同步</span></span><br><span class="line">            fetcherThreadMap(brokerAndFetcherId).addPartitions(partitionAndOffsets.map {</span><br><span class="line">                <span class="keyword">case</span> (tp, brokerAndInitOffset) =&gt; tp -&gt; brokerAndInitOffset.initOffset</span><br><span class="line">            })</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法首先会考虑目标 broker 节点的网络位置信息（brokerId、host 和 port）和 fetcher 线程的 ID 对待同步的 topic 分区进行分组，并以这些信息作为对应 fetcher 线程对象在 <code>AbstractFetcherManager#fetcherThreadMap</code> 集合中的 key，如果 key 对应的 fetcher 线程对象不存在则会创建并启动新的线程，同时将待同步 topic 分区的同步起始 offset 传递给对应线程，然后唤醒线程执行。创建 fetcher 线程的实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createFetcherThread</span></span>(fetcherId: <span class="type">Int</span>, sourceBroker: <span class="type">BrokerEndPoint</span>): <span class="type">AbstractFetcherThread</span> = {</span><br><span class="line">    <span class="keyword">val</span> threadName = threadNamePrefix <span class="keyword">match</span> {</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="string">"ReplicaFetcherThread-%d-%d"</span>.format(fetcherId, sourceBroker.id)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(p) =&gt; <span class="string">"%s:ReplicaFetcherThread-%d-%d"</span>.format(p, fetcherId, sourceBroker.id)</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ReplicaFetcherThread</span>(threadName, fetcherId, sourceBroker, brokerConfig, replicaMgr, metrics, time, quotaManager)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>ReplicaFetcherThread 继承自 ShutdownableThread 抽象方法，所以在线程被启动之后会循环调度执行 <code>AbstractFetcherThread#doWork</code> 方法，该方法会构造 FetchRequest 请求从 leader 副本拉取指定 offset 对应的消息数据，并处理 FetchResponse 响应。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doWork</span></span>() {</span><br><span class="line">    <span class="keyword">val</span> fetchRequest = inLock(partitionMapLock) {</span><br><span class="line">        <span class="comment">// 创建 FetchRequest 请求对象</span></span><br><span class="line">        <span class="keyword">val</span> fetchRequest = <span class="keyword">this</span>.buildFetchRequest(partitionStates.partitionStates.asScala.map { state =&gt;</span><br><span class="line">            state.topicPartition -&gt; state.value</span><br><span class="line">        })</span><br><span class="line">        <span class="comment">// 如果没有拉取消息的需求，则等待一会后重试</span></span><br><span class="line">        <span class="keyword">if</span> (fetchRequest.isEmpty) {</span><br><span class="line">            trace(<span class="string">"There are no active partitions. Back off for %d ms before sending a fetch request"</span>.format(fetchBackOffMs))</span><br><span class="line">            partitionMapCond.await(fetchBackOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">        }</span><br><span class="line">        fetchRequest</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 发送 FetchRequest 请求，并处理 FetchResponse 响应</span></span><br><span class="line">    <span class="keyword">if</span> (!fetchRequest.isEmpty) <span class="keyword">this</span>.processFetchRequest(fetchRequest)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法仅仅是构造了 FetchRequest 请求，而发送和处理响应的过程则由 <code>AbstractFetcherThread#processFetchRequest</code> 方法实现：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">processFetchRequest</span></span>(fetchRequest: <span class="type">REQ</span>) {</span><br><span class="line">    <span class="keyword">val</span> partitionsWithError = mutable.<span class="type">Set</span>[<span class="type">TopicPartition</span>]()</span><br><span class="line">    <span class="keyword">var</span> responseData: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PD</span>)] = <span class="type">Seq</span>.empty</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 发送 FetchRequest 请求，并阻塞等待响应</span></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        trace(<span class="string">"Issuing to broker %d of fetch request %s"</span>.format(sourceBroker.id, fetchRequest))</span><br><span class="line">        responseData = <span class="keyword">this</span>.fetch(fetchRequest) <span class="comment">// 模板方法</span></span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="comment">// ... 省略异常处理</span></span><br><span class="line">    }</span><br><span class="line">    fetcherStats.requestRate.mark()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 处理响应</span></span><br><span class="line">    <span class="keyword">if</span> (responseData.nonEmpty) {</span><br><span class="line">        inLock(partitionMapLock) {</span><br><span class="line">            <span class="comment">// 遍历处理每个 topic 分区对应的响应</span></span><br><span class="line">            responseData.foreach { <span class="keyword">case</span> (topicPartition, partitionData) =&gt;</span><br><span class="line">                <span class="keyword">val</span> topic = topicPartition.topic</span><br><span class="line">                <span class="keyword">val</span> partitionId = topicPartition.partition</span><br><span class="line">                <span class="type">Option</span>(partitionStates.stateValue(topicPartition)).foreach(currentPartitionFetchState =&gt;</span><br><span class="line">                    <span class="comment">// 如果从发送 FetchRequest 请求到收到响应期间，offset 没有发生变化，则追加收到的日志数据</span></span><br><span class="line">                    <span class="keyword">if</span> (fetchRequest.offset(topicPartition) == currentPartitionFetchState.offset) {</span><br><span class="line">                        <span class="type">Errors</span>.forCode(partitionData.errorCode) <span class="keyword">match</span> {</span><br><span class="line">                            <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">NONE</span> =&gt;</span><br><span class="line">                                <span class="keyword">try</span> {</span><br><span class="line">                                    <span class="comment">// 获取返回的消息集合</span></span><br><span class="line">                                    <span class="keyword">val</span> records = partitionData.toRecords</span><br><span class="line">                                    <span class="comment">// 获取返回的最后一条消息的 offset 值</span></span><br><span class="line">                                    <span class="keyword">val</span> newOffset = records.shallowEntries.asScala.lastOption.map(_.nextOffset).getOrElse(currentPartitionFetchState.offset)</span><br><span class="line"></span><br><span class="line">                                    fetcherLagStats.getAndMaybePut(topic, partitionId).lag = <span class="type">Math</span>.max(<span class="number">0</span>L, partitionData.highWatermark - newOffset)</span><br><span class="line">                                    <span class="comment">// 将从 leader 副本获取到的消息追加到当前 follower 副本对应的 Log 对象中</span></span><br><span class="line">                                    <span class="keyword">this</span>.processPartitionData(topicPartition, currentPartitionFetchState.offset, partitionData)</span><br><span class="line"></span><br><span class="line">                                    <span class="keyword">val</span> validBytes = records.validBytes</span><br><span class="line">                                    <span class="keyword">if</span> (validBytes &gt; <span class="number">0</span>) {</span><br><span class="line">                                        <span class="comment">// 更新本地缓存的 fetch 状态</span></span><br><span class="line">                                        partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</span><br><span class="line">                                        fetcherStats.byteRate.mark(validBytes)</span><br><span class="line">                                    }</span><br><span class="line">                                } <span class="keyword">catch</span> {</span><br><span class="line">                                    <span class="comment">// ... 省略异常处理</span></span><br><span class="line">                                }</span><br><span class="line">                            <span class="comment">// follower 请求的 offset 超出了 leader 的 LEO 值</span></span><br><span class="line">                            <span class="keyword">case</span> <span class="type">Errors</span>.<span class="type">OFFSET_OUT_OF_RANGE</span> =&gt;</span><br><span class="line">                                <span class="keyword">try</span> {</span><br><span class="line">                                    <span class="comment">// 计算有效的 offset，并更新本地缓存的 fetch 状态</span></span><br><span class="line">                                    <span class="keyword">val</span> newOffset = <span class="keyword">this</span>.handleOffsetOutOfRange(topicPartition)</span><br><span class="line">                                    partitionStates.updateAndMoveToEnd(topicPartition, <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(newOffset))</span><br><span class="line">                                    error(<span class="string">"Current offset %d for partition [%s,%d] out of range; reset offset to %d"</span>.format(currentPartitionFetchState.offset, topic, partitionId, newOffset))</span><br><span class="line">                                } <span class="keyword">catch</span> {</span><br><span class="line">                                    <span class="comment">// ... 省略异常处理</span></span><br><span class="line">                                }</span><br><span class="line">                            <span class="comment">// ... 其他异常</span></span><br><span class="line">                        }</span><br><span class="line">                    })</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对于操作存在异常的 topic 分区，暂停发送 FetchRequest 请求，休息一会儿</span></span><br><span class="line">    <span class="keyword">if</span> (partitionsWithError.nonEmpty) {</span><br><span class="line">        debug(<span class="string">"handling partitions with error for %s"</span>.format(partitionsWithError))</span><br><span class="line">        <span class="keyword">this</span>.handlePartitionsWithErrors(partitionsWithError)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>由上述实现可以看到方法 <code>AbstractFetcherThread#processFetchRequest</code> 主要做了两件事情：发送 FetchRequest 请求并阻塞等待响应，以及处理响应。其中发送 FetchRequest 请求的过程由 <code>ReplicaFetcherThread#fetch</code> 方法实现，该方法使用 NetworkClient 的阻塞版本 NetworkClientBlockingOps 向目标 broker 节点发送 FetchRequest 请求，并阻塞等待响应结果，然后将针对每个 topic 分区的响应结果封装成 PartitionData 对象交由后续处理。</p>
<p>在遍历处理对于每个 topic 分区的 FetchResponse 响应时，分为 3 种情况：</p>
<ol>
<li>正常响应，拉回指定 offset 对应的消息数据。</li>
<li>异常响应，请求的 offset 不在 leader 副本允许的范围内。</li>
<li>其它异常响应。</li>
</ol>
<p>对于 <strong>第 1 种情况</strong> 来说，会调用 <code>ReplicaFetcherThread#processPartitionData</code> 方法将从对应 leader 副本拉取回来的消息数据写入 follower 副本对应的 Log 对象中，并更新本地缓存的对应分区的消息同步状态信息。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processPartitionData</span></span>(topicPartition: <span class="type">TopicPartition</span>, fetchOffset: <span class="type">Long</span>, partitionData: <span class="type">PartitionData</span>) {</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">val</span> replica = replicaMgr.getReplica(topicPartition).get</span><br><span class="line">        <span class="keyword">val</span> records = partitionData.toRecords</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果拉取到的消息数据过大，则打印异常</span></span><br><span class="line">        <span class="keyword">this</span>.maybeWarnIfOversizedRecords(records, topicPartition)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (fetchOffset != replica.logEndOffset.messageOffset)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">"Offset mismatch for partition %s: fetched offset = %d, log end offset = %d."</span>.format(topicPartition, fetchOffset, replica.logEndOffset.messageOffset))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将消息追加到 Log 中，因为 leader 已经为消息分配了 offset，所以 follower 无需在对消息分配 offset 值</span></span><br><span class="line">        replica.log.get.append(records, assignOffsets = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更新对应 follower 副本的 HW 值</span></span><br><span class="line">        <span class="keyword">val</span> followerHighWatermark = replica.logEndOffset.messageOffset.min(partitionData.highWatermark)</span><br><span class="line">        replica.highWatermark = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(followerHighWatermark)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (quota.isThrottled(topicPartition)) quota.record(records.sizeInBytes)</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">KafkaStorageException</span> =&gt;</span><br><span class="line">            fatal(<span class="string">s"Disk error while replicating data for <span class="subst">$topicPartition</span>"</span>, e)</span><br><span class="line">            <span class="type">Runtime</span>.getRuntime.halt(<span class="number">1</span>)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>追加消息数据由对应副本持有的 Log 对象的 <code>Log#append</code> 方法完成，如果成功追加则会更新副本对应的 HW 值。</p>
<p>如果在执行同步操作时，某个 topic 分区出现异常，则需要依据对应的异常类型分别处理，如果是除 <code>OFFSET_OUT_OF_RANGE</code> 以外的错误（对应 <strong>第 3 种情况</strong> ），则会暂停到对应分区 leader 副本同步数据的请求，休整一段时间（对应 <code>replica.fetch.backoff.ms</code> 配置）之后再继续，对应 <code>ReplicaFetcherThread#handlePartitionsWithErrors</code> 方法实现，比较简单。</p>
<p>下面来看一下 <strong>第 2 种情况</strong> ，如果同步操作请求的 offset 不合法，即位于 leader 副本的 <code>[startOffset, LEO]</code> 之外，则需要修正本地缓存的对应副本的同步状态信息，修正 offset 的过程由 <code>ReplicaFetcherThread#handleOffsetOutOfRange</code> 方法实现，这里需要区分 2 种情况：</p>
<ol>
<li>请求同步的 offset 大于对应 leader 副本的 LEO 值。</li>
<li>请求同步的 offset 小于对应 leader 副本的 startOffset 值。</li>
</ol>
<p>一般 follower 副本的 LEO 值都是小于等于 leader 副本的 LEO 值，但是如果发生以下场景（unclean leader election），则可能出现 follower 副本的 LEO 值大于 leader 副本的 LEO 值，此时如果 follower 副本请求同步 leader 副本就有可能出现请求的 offset 大于目标 leader 副本的 LEO 值的情况。这类场景的发生过程为（令场景中 follower 副本为 F）：</p>
<ol>
<li>F 副本失效，期间 F 所属分区的 leader 副本继续追加消息数据；</li>
<li>F 副本失效后恢复，继续从 leader 副本同步数据，但是在追赶上 leader 副本之前，所有 ISR 集合中的副本全部失效；</li>
<li>为了保证 Kafka 服务的正常运行，选举 F 成为对应 topic 分区新的 leader 副本，并开始负责处理来自生产者的消息读写请求；</li>
<li>上一任 leader 从失效中恢复，并成为 follower 角色，此时其 LEO 值很有可能大于 F 的 LEO 值。</li>
</ol>
<p>针对这种情况简单的处理方式是将 follower 副本的消息进行截断，但是 Kafka 也提供了 <code>unclean.leader.election.enable</code> 配置，允许在发生这种情况时停服。相关实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleOffsetOutOfRange</span></span>(topicPartition: <span class="type">TopicPartition</span>): <span class="type">Long</span> = {</span><br><span class="line">    <span class="keyword">val</span> replica = replicaMgr.getReplica(topicPartition).get</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 发送 ListOffsetRequest 请求，获取 leader 副本的 LEO 值</span></span><br><span class="line">    <span class="keyword">val</span> leaderEndOffset: <span class="type">Long</span> = <span class="keyword">this</span>.earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">LATEST_TIMESTAMP</span>, brokerConfig.brokerId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果 leader 副本的 LEO 值落后于 follower 副本的 LEO 值</span></span><br><span class="line">    <span class="keyword">if</span> (leaderEndOffset &lt; replica.logEndOffset.messageOffset) {</span><br><span class="line">        <span class="comment">// 依据配置（unclean.leader.election.enable）决定是否需要停机</span></span><br><span class="line">        <span class="keyword">if</span> (!<span class="type">LogConfig</span>.fromProps(brokerConfig.originals,</span><br><span class="line">            <span class="type">AdminUtils</span>.fetchEntityConfig(replicaMgr.zkUtils, <span class="type">ConfigType</span>.<span class="type">Topic</span>, topicPartition.topic)).uncleanLeaderElectionEnable) {</span><br><span class="line">            <span class="comment">// Log a fatal error and shutdown the broker to ensure that data loss does not unexpectedly occur.</span></span><br><span class="line">            fatal(<span class="string">"Exiting because log truncation is not allowed for partition %s,"</span>.format(topicPartition) +</span><br><span class="line">                    <span class="string">" Current leader %d's latest offset %d is less than replica %d's latest offset %d"</span></span><br><span class="line">                            .format(sourceBroker.id, leaderEndOffset, brokerConfig.brokerId, replica.logEndOffset.messageOffset))</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's latest offset %d"</span></span><br><span class="line">                .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderEndOffset))</span><br><span class="line">        <span class="comment">// 将分区对应的 Log 截断到 leader 副本的 LEO 位置，从该位置开始重新与 leader 副本进行同步</span></span><br><span class="line">        replicaMgr.logManager.truncateTo(<span class="type">Map</span>(topicPartition -&gt; leaderEndOffset))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回下次获取消息的 offset 位置</span></span><br><span class="line">        leaderEndOffset</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// ... 第 2 种情况</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>有时候 leader 副本的 LEO 值也会明显领先于某个 follower 副本的 LEO 值，此时 follower 请求同步 leader 副本时可能出现请求同步的 offset 小于对应 leader 副本的 startOffset 值。出现这种情况的原因一般有以下 2 种：</p>
<ol>
<li>follower 副本长时间失效，期间 leader 副本不断在追加新的数据，等到 follower 再次上线时，leader 副本对应 offset 位置的日志数据已被定时任务清除。</li>
<li>出现前面介绍的 unclean leader election 场景，follower 在执行截断操作到 HW 位置后，offset 仍然大于新 leader 的 LEO 值，此时执行同步会导致 OffsetOutOfRangeException 异常，follower 在处理该异常的期间，leader 副本因为追加了大量的数据而导致 follower 再次请求同步时，offset 小于 leader 副本的 startOffset 值。</li>
</ol>
<p>出现以上这 2 种情况只需要将 follower 同步请求同步的 offset 置为 leader 副本的 startOffset 即可，此外还需要清空 follower 副本的 Log 对象，因为其中的数据已经全部失效，没有继续保留的意义。相关实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleOffsetOutOfRange</span></span>(topicPartition: <span class="type">TopicPartition</span>): <span class="type">Long</span> = {</span><br><span class="line">    <span class="keyword">val</span> replica = replicaMgr.getReplica(topicPartition).get</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 发送 ListOffsetRequest 请求，获取 leader 副本的 LEO 值</span></span><br><span class="line">    <span class="keyword">val</span> leaderEndOffset: <span class="type">Long</span> = <span class="keyword">this</span>.earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">LATEST_TIMESTAMP</span>, brokerConfig.brokerId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果 leader 副本的 LEO 值落后于 follower 副本的 LEO 值</span></span><br><span class="line">    <span class="keyword">if</span> (leaderEndOffset &lt; replica.logEndOffset.messageOffset) {</span><br><span class="line">        <span class="comment">// ... 第 1 种情况</span></span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 发送 ListOffsetRequest 请求，获取 leader 副本的 startOffset 值</span></span><br><span class="line">        <span class="keyword">val</span> leaderStartOffset: <span class="type">Long</span> = <span class="keyword">this</span>.earliestOrLatestOffset(topicPartition, <span class="type">ListOffsetRequest</span>.<span class="type">EARLIEST_TIMESTAMP</span>, brokerConfig.brokerId)</span><br><span class="line">        warn(<span class="string">"Replica %d for partition %s reset its fetch offset from %d to current leader %d's start offset %d"</span></span><br><span class="line">                .format(brokerConfig.brokerId, topicPartition, replica.logEndOffset.messageOffset, sourceBroker.id, leaderStartOffset))</span><br><span class="line">        <span class="comment">// 选择下次获取消息的起始 offset 值</span></span><br><span class="line">        <span class="keyword">val</span> offsetToFetch = <span class="type">Math</span>.max(leaderStartOffset, replica.logEndOffset.messageOffset)</span><br><span class="line">        <span class="comment">// 如果当前 leader 的 startOffset 大于对应副本的 LEO 值，则将该副本的 Log 全部截断，并创建新的 activeSegment 对象</span></span><br><span class="line">        <span class="keyword">if</span> (leaderStartOffset &gt; replica.logEndOffset.messageOffset)</span><br><span class="line">            replicaMgr.logManager.truncateFullyAndStartAt(topicPartition, leaderStartOffset)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回下次获取消息的 offset 位置</span></span><br><span class="line">        offsetToFetch</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法 <code>AbstractFetcherThread#handleOffsetOutOfRange</code> 还会在 <code>AbstractFetcherThread#addPartitions</code> 方法中被调用，该方法用于为每个 topic 分区构造合法的分区同步状态 PartitionFetchState 对象，并更新本地缓存，同时唤醒消息数据同步操作，前面分析过的 <code>AbstractFetcherManager#addFetcherForPartitions</code> 调用了该方法。实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addPartitions</span></span>(partitionAndOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]) {</span><br><span class="line">    partitionMapLock.lockInterruptibly()</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 基于指定的 offset 构造每个 topic 分区合法的 PartitionFetchState 对象，忽略已经存在的 topic 分区</span></span><br><span class="line">        <span class="keyword">val</span> newPartitionToState = partitionAndOffsets</span><br><span class="line">                .filter { <span class="keyword">case</span> (tp, _) =&gt; !partitionStates.contains(tp) }</span><br><span class="line">                .map { <span class="keyword">case</span> (tp, offset) =&gt;</span><br><span class="line">                    <span class="comment">// 基于指定的 offset 创建对应的 PartitionFetchState 对象，如果 offset 无效，则尝试解析得到合法的 offset 值</span></span><br><span class="line">                    <span class="keyword">val</span> fetchState =</span><br><span class="line">                        <span class="keyword">if</span> (<span class="type">PartitionTopicInfo</span>.isOffsetInvalid(offset)) <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(<span class="keyword">this</span>.handleOffsetOutOfRange(tp))</span><br><span class="line">                        <span class="keyword">else</span> <span class="keyword">new</span> <span class="type">PartitionFetchState</span>(offset)</span><br><span class="line">                    tp -&gt; fetchState</span><br><span class="line">                }</span><br><span class="line">        <span class="comment">// 获取并更新本地缓存的已有的 topic 分区与 PartitionFetchState 对象之间的映射关系</span></span><br><span class="line">        <span class="keyword">val</span> existingPartitionToState = partitionStates.partitionStates.asScala.map { state =&gt; state.topicPartition -&gt; state.value }.toMap</span><br><span class="line">        partitionStates.set((existingPartitionToState ++ newPartitionToState).asJava)</span><br><span class="line">        <span class="comment">// 唤醒当前 fetcher 线程，执行同步操作</span></span><br><span class="line">        partitionMapCond.signalAll()</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        partitionMapLock.unlock()</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>该方法中调用 <code>AbstractFetcherThread#handleOffsetOutOfRange</code> 方法的目的在于当参数未指定 offset 时，利用该方法获取合法的同步 offset 值。</p>

        <h4 id="副本角色切换-1">
          <a href="#副本角色切换-1" class="heading-link"><i class="fas fa-link"></i></a>副本角色切换</h4>
      <p>ReplicaManager 定义了 <code>ReplicaManager#becomeLeaderOrFollower</code> 方法，用于处理来自 kafka controller 的 LeaderAndIsrRequest 请求，指导位于当前 broker 节点上的相应分区副本的角色切换工作。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">becomeLeaderOrFollower</span></span>(correlationId: <span class="type">Int</span>,</span><br><span class="line">                           leaderAndISRRequest: <span class="type">LeaderAndIsrRequest</span>,</span><br><span class="line">                           metadataCache: <span class="type">MetadataCache</span>,</span><br><span class="line">                           onLeadershipChange: (<span class="type">Iterable</span>[<span class="type">Partition</span>], <span class="type">Iterable</span>[<span class="type">Partition</span>]) =&gt; <span class="type">Unit</span>): <span class="type">BecomeLeaderOrFollowerResult</span> = {</span><br><span class="line"></span><br><span class="line">    replicaStateChangeLock synchronized {</span><br><span class="line">        <span class="comment">// 用于记录每个分区角色切换操作的状态码</span></span><br><span class="line">        <span class="keyword">val</span> responseMap = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]</span><br><span class="line">        <span class="comment">// 校验 controller 的年代信息，避免处理来自已经过期的 controller 的请求</span></span><br><span class="line">        <span class="keyword">if</span> (leaderAndISRRequest.controllerEpoch &lt; controllerEpoch) {</span><br><span class="line">            <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="keyword">val</span> controllerId = leaderAndISRRequest.controllerId</span><br><span class="line">            <span class="comment">// 1. 更新本地缓存的 kafka controller 的年代信息</span></span><br><span class="line">            controllerEpoch = leaderAndISRRequest.controllerEpoch</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 2. 校验请求的 leader 副本的年代信息，以及是否由当前 broker 节点管理，将满足条件的分区信息记录到 partitionState 集合中</span></span><br><span class="line">            <span class="keyword">val</span> partitionState = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>]()</span><br><span class="line">            leaderAndISRRequest.partitionStates.asScala.foreach { <span class="keyword">case</span> (topicPartition, stateInfo) =&gt;</span><br><span class="line">                <span class="comment">// 获取/创建指定 topic 分区的 Partition 对象</span></span><br><span class="line">                <span class="keyword">val</span> partition = <span class="keyword">this</span>.getOrCreatePartition(topicPartition)</span><br><span class="line">                <span class="comment">// 获取 leader 副本的年代信息</span></span><br><span class="line">                <span class="keyword">val</span> partitionLeaderEpoch = partition.getLeaderEpoch</span><br><span class="line">                <span class="comment">// 校验 leader 副本的年代信息，需要保证请求中的 leader 副本的年代信息大于本地缓存的 topic 分区 leader 副本的年代信息</span></span><br><span class="line">                <span class="keyword">if</span> (partitionLeaderEpoch &lt; stateInfo.leaderEpoch) {</span><br><span class="line">                    <span class="comment">// 如果请求的分区副本位于当前 broker 节点上，记录到 partitionState 集合中</span></span><br><span class="line">                    <span class="keyword">if</span> (stateInfo.replicas.contains(localBrokerId))</span><br><span class="line">                        partitionState.put(partition, stateInfo)</span><br><span class="line">                    <span class="keyword">else</span> {</span><br><span class="line">                        <span class="comment">// 请求的分区副本不在当前 broker 节点上，响应 UNKNOWN_TOPIC_OR_PARTITION 错误</span></span><br><span class="line">                        responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">UNKNOWN_TOPIC_OR_PARTITION</span>.code)</span><br><span class="line">                    }</span><br><span class="line">                } <span class="keyword">else</span> {</span><br><span class="line">                    <span class="comment">// 请求中的 leader 副本的年代信息小于等于本地记录的对应 topic 分区 leader 副本的年代信息，响应 STALE_CONTROLLER_EPOCH 错误</span></span><br><span class="line">                    responseMap.put(topicPartition, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3. 将请求对象中的分区集合分割成 leader 和 follower 两类，并执行角色切换</span></span><br><span class="line">            <span class="keyword">val</span> partitionsTobeLeader = partitionState.filter { <span class="keyword">case</span> (_, stateInfo) =&gt; stateInfo.leader == localBrokerId }</span><br><span class="line">            <span class="keyword">val</span> partitionsToBeFollower = partitionState -- partitionsTobeLeader.keys</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3.1 将指定分区的副本切换成 leader 角色</span></span><br><span class="line">            <span class="keyword">val</span> partitionsBecomeLeader = <span class="keyword">if</span> (partitionsTobeLeader.nonEmpty)</span><br><span class="line">                                             <span class="keyword">this</span>.makeLeaders(controllerId, controllerEpoch, partitionsTobeLeader, correlationId, responseMap)</span><br><span class="line">                                         <span class="keyword">else</span></span><br><span class="line">                                             <span class="type">Set</span>.empty[<span class="type">Partition</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 3.2 将指定分区的副本切换成 follower 角色</span></span><br><span class="line">            <span class="keyword">val</span> partitionsBecomeFollower = <span class="keyword">if</span> (partitionsToBeFollower.nonEmpty)</span><br><span class="line">                                               <span class="keyword">this</span>.makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap, metadataCache)</span><br><span class="line">                                           <span class="keyword">else</span></span><br><span class="line">                                               <span class="type">Set</span>.empty[<span class="type">Partition</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 4. 如果 highwatermark-checkpoint 定时任务尚未启动，则执行启动</span></span><br><span class="line">            <span class="keyword">if</span> (!hwThreadInitialized) {</span><br><span class="line">                <span class="keyword">this</span>.startHighWaterMarksCheckPointThread()</span><br><span class="line">                hwThreadInitialized = <span class="literal">true</span></span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 5. 关闭空闲的 fetcher 线程</span></span><br><span class="line">            replicaFetcherManager.shutdownIdleFetcherThreads()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 6. 执行回调函数，完成 GroupCoordinator 的迁移操作</span></span><br><span class="line">            onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 7. 封装结果对象返回</span></span><br><span class="line">            <span class="type">BecomeLeaderOrFollowerResult</span>(responseMap, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>副本角色切换的整体执行流程可以概括为：</p>
<ol>
<li>更新本地缓存的 kafka controller 的年代信息；</li>
<li>校验请求的合法性，确保请求操作对应的分区 leader 副本年代信息合法，以及请求操作的分区副本位于当前 broker 节点上；</li>
<li>对请求的分区副本按照角色分类，并执行角色切换；</li>
<li>如果 highwatermark-checkpoint 定时任务尚未启动，则执行启动；</li>
<li>关闭空闲的副本数据同步 fetcher 线程；</li>
<li>因为副本角色发生变化，可能影响消费者的消费操作，尝试执行 GroupCoordinator 迁移操作；</li>
<li>封装响应结果返回。</li>
</ol>
<p>上面的步骤中我们重点来看一下步骤 3，关于 GroupCoordinator 将留到后面的篇章中针对性分析。步骤 3 首先会将待处理的副本集合按照角色分为 leader 和 follower 两组，然后针对 leader 分组调用 <code>ReplicaManager#makeLeaders</code> 方法将对应的分区切换成 leader 角色，调用 <code>ReplicaManager#makeFollowers</code> 方法将对应的分区切换成 follower 角色。</p>
<p>方法 <code>ReplicaManager#makeLeaders</code> 的实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeLeaders</span></span>(controllerId: <span class="type">Int</span>,</span><br><span class="line">                        epoch: <span class="type">Int</span>,</span><br><span class="line">                        partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>], <span class="comment">// 记录需要切换成 leader 角色的分区副本信息</span></span><br><span class="line">                        correlationId: <span class="type">Int</span>,</span><br><span class="line">                        responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]): <span class="type">Set</span>[<span class="type">Partition</span>] = {</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化每个 topic 分区的错误码为 NONE</span></span><br><span class="line">    <span class="keyword">for</span> (partition &lt;- partitionState.keys)</span><br><span class="line">        responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> partitionsToMakeLeaders: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 如果对应的副本当前是 follower 角色，需要要先停止这些副本的消息同步工作</span></span><br><span class="line">        replicaFetcherManager.removeFetcherForPartitions(partitionState.keySet.map(_.topicPartition))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历处理 partitionState 集合，将其中记录的分区转换成 leader 角色</span></span><br><span class="line">        partitionState.foreach {</span><br><span class="line">            <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</span><br><span class="line">                <span class="comment">// 调用 Partition#makeLeader 方法，将分区的本地副本切换成 leader 角色</span></span><br><span class="line">                <span class="keyword">if</span> (partition.makeLeader(controllerId, partitionStateInfo, correlationId))</span><br><span class="line">                    partitionsToMakeLeaders += partition <span class="comment">// 记录成功完成 leader 角色切换的副本对应的分区</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    <span class="comment">// ... 省略日志打点</span></span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="comment">// ... 省略异常处理</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    partitionsToMakeLeaders</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>切换副本角色为 leader 的过程比较简单，首先停止这些待切换 follower 副本的数据同步 fetcher 线程，然后调用 <code>Partition#makeLeader</code> 方法逐个将副本切换成 leader 角色，该方法已在前面分析过，不再重复撰述。</p>
<p>方法 <code>ReplicaManager#makeFollowers</code> 的实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeFollowers</span></span>(controllerId: <span class="type">Int</span>,</span><br><span class="line">                          epoch: <span class="type">Int</span>,</span><br><span class="line">                          partitionState: <span class="type">Map</span>[<span class="type">Partition</span>, <span class="type">PartitionState</span>],</span><br><span class="line">                          correlationId: <span class="type">Int</span>,</span><br><span class="line">                          responseMap: mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>],</span><br><span class="line">                          metadataCache: <span class="type">MetadataCache</span>): <span class="type">Set</span>[<span class="type">Partition</span>] = {</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化每个 topic 分区的错误码为 NONE</span></span><br><span class="line">    <span class="keyword">for</span> (partition &lt;- partitionState.keys)</span><br><span class="line">        responseMap.put(partition.topicPartition, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> partitionsToMakeFollower: mutable.<span class="type">Set</span>[<span class="type">Partition</span>] = mutable.<span class="type">Set</span>()</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        partitionState.foreach {</span><br><span class="line">            <span class="keyword">case</span> (partition, partitionStateInfo) =&gt;</span><br><span class="line">                <span class="comment">// 检测 leader 副本所在的 broker 是否可用</span></span><br><span class="line">                <span class="keyword">val</span> newLeaderBrokerId = partitionStateInfo.leader</span><br><span class="line">                metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) <span class="keyword">match</span> {</span><br><span class="line">                    <span class="comment">// 仅对 leader 副本所在 broker 节点可用的副本执行角色切换</span></span><br><span class="line">                    <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt;</span><br><span class="line">                        <span class="comment">// 调用 Partition#makeFollower 方法，将分区的本地副本切换成 follower 角色</span></span><br><span class="line">                        <span class="keyword">if</span> (partition.makeFollower(controllerId, partitionStateInfo, correlationId))</span><br><span class="line">                            partitionsToMakeFollower += partition <span class="comment">// 记录成功完成 follower 角色切换的副本对应的分区</span></span><br><span class="line">                        <span class="keyword">else</span></span><br><span class="line">                            <span class="comment">// ... 省略日志打点</span></span><br><span class="line">                    <span class="comment">// 对应 leader 副本所在的 broker 节点失效</span></span><br><span class="line">                    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">                        <span class="comment">// 即使 leader 副本所在的 broker 不可用，也要创建本地副本对象，主要是为了在 checkpoint 文件中记录此分区的 HW 值</span></span><br><span class="line">                        partition.getOrCreateReplica()</span><br><span class="line">                }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 停止与旧的 leader 副本同步的 fetcher 线程</span></span><br><span class="line">        replicaFetcherManager.removeFetcherForPartitions(partitionsToMakeFollower.map(_.topicPartition))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 由于 leader 副本发生变化，所以新旧 leader 在 [HW, LEO] 之间的消息可能不一致，</span></span><br><span class="line">        <span class="comment">// 但是 HW 之前的消息是一致的，所以将 Log 截断到 HW 位置，可能会出现 unclean leader election 的场景</span></span><br><span class="line">        logManager.truncateTo(partitionsToMakeFollower.map { partition =&gt;</span><br><span class="line">            (partition.topicPartition, partition.getOrCreateReplica().highWatermark.messageOffset)</span><br><span class="line">        }.toMap)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 尝试完成监听对应分区的 DelayedProduce 和 DelayedFetch 延时任务</span></span><br><span class="line">        partitionsToMakeFollower.foreach { partition =&gt;</span><br><span class="line">            <span class="keyword">val</span> topicPartitionOperationKey = <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(partition.topicPartition)</span><br><span class="line">            <span class="keyword">this</span>.tryCompleteDelayedProduce(topicPartitionOperationKey)</span><br><span class="line">            <span class="keyword">this</span>.tryCompleteDelayedFetch(topicPartitionOperationKey)</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 检测 ReplicaManager 的运行状态</span></span><br><span class="line">        <span class="keyword">if</span> (isShuttingDown.get()) {</span><br><span class="line">            <span class="comment">// ... 省略日志打点</span></span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 重新启用与新 leader 副本同步的 fetcher 线程</span></span><br><span class="line">        <span class="keyword">else</span> {</span><br><span class="line">            <span class="keyword">val</span> partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map(partition =&gt;</span><br><span class="line">                partition.topicPartition -&gt; <span class="type">BrokerAndInitialOffset</span>(</span><br><span class="line">                    metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get.getBrokerEndPoint(config.interBrokerListenerName),</span><br><span class="line">                    partition.getReplica().get.logEndOffset.messageOffset)).toMap</span><br><span class="line">            <span class="comment">// 为需要同步的分区创建并启动同步线程，从指定的 offset 开始与 leader 副本进行同步</span></span><br><span class="line">            replicaFetcherManager.addFetcherForPartitions(partitionsToMakeFollowerWithLeaderAndOffset)</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="comment">// ... 省略异常处理</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    partitionsToMakeFollower</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>切换副本为 follower 角色的过程相对要复杂一些，整体执行流程可以概括为：</p>
<ol>
<li>检测对应新的 leader 副本所在 broker 节点是否可用，如果不可用则无需执行切换操作，否则调用 <code>Partition#makeFollower</code> 方法执行副本角色切换；</li>
<li>停止待切换副本的数据同步 fetcher 线程；</li>
<li>由于 leader 副本发生变化，新旧 leader 在 <code>[HW, LEO]</code> 之间的数据可能不一致，所以需要将当前副本截断到 HW 位置，以保证数据一致性；</li>
<li>尝试完成监听对应分区的 DelayedProduce 和 DelayedFetch 延时任务；</li>
<li>为新的 follower 副本集合创建并启动对应的数据同步 fetcher 线程（如果已存在，则复用）。</li>
</ol>
<p>上述过程中涉及到的相关方法已经在前面分析过，不再重复撰述。</p>

        <h4 id="分区与副本管理">
          <a href="#分区与副本管理" class="heading-link"><i class="fas fa-link"></i></a>分区与副本管理</h4>
      <p>ReplicaManager 定义了 <code>ReplicaManager#getOrCreatePartition</code> 方法和 <code>ReplicaManager#getPartition</code> 方法用于获取本地缓存的指定 topic 分区的 Partition 对象，二者的区别在于前者会在本地检索不到目标 topic 分区时创建对应的 Partition 对象。同时，ReplicaManager 还提供了 <code>ReplicaManager#getReplicaOrException</code>、<code>ReplicaManager#getLeaderReplicaIfLocal</code>，以及 <code>ReplicaManager#getReplica</code> 方法用于获取指定 topic 分区的指定副本对象，实现上都比较简单，不展开分析。</p>
<p>下面来重点看一下关闭副本的 <code>ReplicaManager#stopReplicas</code> 方法实现，当 broker 节点收到来自 kafka controller 的 StopReplicaRequest 请求时，会关闭指定的副本，包括停止副本的数据同步 fetcher 线程，以及依据参数决定是否删除副本对应的 Log 对象和文件，并清空本地缓存的相关信息。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stopReplicas</span></span>(stopReplicaRequest: <span class="type">StopReplicaRequest</span>): (mutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>], <span class="type">Short</span>) = {</span><br><span class="line">    replicaStateChangeLock synchronized {</span><br><span class="line">        <span class="keyword">val</span> responseMap = <span class="keyword">new</span> collection.mutable.<span class="type">HashMap</span>[<span class="type">TopicPartition</span>, <span class="type">Short</span>]</span><br><span class="line">        <span class="comment">// 校验 controller 的年代信息，避免处理来自已经过期的 controller 的请求</span></span><br><span class="line">        <span class="keyword">if</span> (stopReplicaRequest.controllerEpoch() &lt; controllerEpoch) {</span><br><span class="line">            (responseMap, <span class="type">Errors</span>.<span class="type">STALE_CONTROLLER_EPOCH</span>.code)</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="keyword">val</span> partitions = stopReplicaRequest.partitions.asScala</span><br><span class="line">            <span class="comment">// 更新本地记录的 kafka controller 的年代信息</span></span><br><span class="line">            controllerEpoch = stopReplicaRequest.controllerEpoch</span><br><span class="line">            <span class="comment">// 停止对指定分区的数据同步 fetcher 线程</span></span><br><span class="line">            replicaFetcherManager.removeFetcherForPartitions(partitions)</span><br><span class="line">            <span class="keyword">for</span> (topicPartition &lt;- partitions) {</span><br><span class="line">                <span class="comment">// 关闭指定分区的副本</span></span><br><span class="line">                <span class="keyword">val</span> errorCode = <span class="keyword">this</span>.stopReplica(topicPartition, stopReplicaRequest.deletePartitions())</span><br><span class="line">                responseMap.put(topicPartition, errorCode)</span><br><span class="line">            }</span><br><span class="line">            (responseMap, <span class="type">Errors</span>.<span class="type">NONE</span>.code)</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stopReplica</span></span>(topicPartition: <span class="type">TopicPartition</span>, deletePartition: <span class="type">Boolean</span>): <span class="type">Short</span> = {</span><br><span class="line">    <span class="keyword">val</span> errorCode = <span class="type">Errors</span>.<span class="type">NONE</span>.code</span><br><span class="line">    getPartition(topicPartition) <span class="keyword">match</span> {</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt;</span><br><span class="line">            <span class="comment">// 如果 deletePartition = true，则删除分区对应的副本及其日志和索引文件</span></span><br><span class="line">            <span class="keyword">if</span> (deletePartition) {</span><br><span class="line">                <span class="comment">// 从本地移除指定的 topic 分区</span></span><br><span class="line">                <span class="keyword">val</span> removedPartition = allPartitions.remove(topicPartition)</span><br><span class="line">                <span class="keyword">if</span> (removedPartition != <span class="literal">null</span>) {</span><br><span class="line">                    <span class="comment">// 删除分区的日志和索引文件，并清空本地缓存的相关信息</span></span><br><span class="line">                    removedPartition.delete() <span class="comment">// this will delete the local log</span></span><br><span class="line">                    <span class="keyword">val</span> topicHasPartitions = allPartitions.keys.exists(tp =&gt; topicPartition.topic == tp.topic)</span><br><span class="line">                    <span class="keyword">if</span> (!topicHasPartitions) <span class="type">BrokerTopicStats</span>.removeMetrics(topicPartition.topic)</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        <span class="comment">// 本地未缓存对应的分区（一般发生在对应的 topic 已经被删除，但是期间 broker 宕机了），直接尝试对应的删除日志和索引文件</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            <span class="keyword">if</span> (deletePartition &amp;&amp; logManager.getLog(topicPartition).isDefined) logManager.asyncDelete(topicPartition)</span><br><span class="line">    }</span><br><span class="line">    errorCode</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果在 StopReplicaRequest 请求中指明了要删除对应 topic 分区的日志和索引文件，则方法会调用 <code>Partition#delete</code> 方法执行删除操作，并清空本地缓存的相关信息。如果某个 broker 节点在宕机中恢复后，之前管理的 topic 分区很可能已经被分配到新的 broker 节点上，此时该 broker 节点已经不再管理相应的 topic 分区对象，如果收到相应的 StopReplicaRequest 请求，则仍然会调用 <code>LogManager#asyncDelete</code> 方法尝试删除之前遗留的日志文件和索引文件。</p>

        <h4 id="日志数据读写">
          <a href="#日志数据读写" class="heading-link"><i class="fas fa-link"></i></a>日志数据读写</h4>
      <p>ReplicaManager 提供了 <code>ReplicaManager#appendRecords</code> 方法，用于处理 ProduceRequest 请求，将给定的日志数据追加到对应 topic 分区的 leader 副本中。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">appendRecords</span></span>(timeout: <span class="type">Long</span>,</span><br><span class="line">                  requiredAcks: <span class="type">Short</span>,</span><br><span class="line">                  internalTopicsAllowed: <span class="type">Boolean</span>,</span><br><span class="line">                  entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>],</span><br><span class="line">                  responseCallback: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">PartitionResponse</span>] =&gt; <span class="type">Unit</span>) {</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果 acks 参数合法</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>.isValidRequiredAcks(requiredAcks)) {</span><br><span class="line">        <span class="keyword">val</span> sTime = time.milliseconds</span><br><span class="line">        <span class="comment">// 将消息追加到 Log 对象中</span></span><br><span class="line">        <span class="keyword">val</span> localProduceResults = <span class="keyword">this</span>.appendToLocalLog(internalTopicsAllowed, entriesPerPartition, requiredAcks)</span><br><span class="line">        debug(<span class="string">"Produce to local log in %d ms"</span>.format(time.milliseconds - sTime))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 封装数据追加结果</span></span><br><span class="line">        <span class="keyword">val</span> produceStatus = localProduceResults.map { <span class="keyword">case</span> (topicPartition, result) =&gt;</span><br><span class="line">            topicPartition -&gt; <span class="type">ProducePartitionStatus</span>(</span><br><span class="line">                result.info.lastOffset + <span class="number">1</span>, <span class="comment">// 下一次请求日志的 offset 值</span></span><br><span class="line">                <span class="keyword">new</span> <span class="type">PartitionResponse</span>(result.error, result.info.firstOffset, result.info.logAppendTime)) <span class="comment">// response status</span></span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果需要生成 DelayedProduce 延时任务</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.delayedRequestRequired(requiredAcks, entriesPerPartition, localProduceResults)) {</span><br><span class="line">            <span class="comment">// 创建 DelayedProduce 延时任务对象，将回调响应函数封装到延时任务对象中</span></span><br><span class="line">            <span class="keyword">val</span> produceMetadata = <span class="type">ProduceMetadata</span>(requiredAcks, produceStatus)</span><br><span class="line">            <span class="keyword">val</span> delayedProduce = <span class="keyword">new</span> <span class="type">DelayedProduce</span>(timeout, produceMetadata, <span class="keyword">this</span>, responseCallback)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 创建当前延时任务监听的一系列 key 对象，监听本次追加操作的所有 topic 分区</span></span><br><span class="line">            <span class="keyword">val</span> producerRequestKeys = entriesPerPartition.keys.map(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(_)).toSeq</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 尝试执行延时任务，如果还未到期则将任务交由炼狱管理</span></span><br><span class="line">            delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 无需生成 DelayedProduce 延时任务，立即响应</span></span><br><span class="line">            <span class="keyword">val</span> produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus)</span><br><span class="line">            responseCallback(produceResponseStatus)</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 对应的 acks 参数错误，构造 INVALID_REQUIRED_ACKS 响应</span></span><br><span class="line">        <span class="keyword">val</span> responseStatus = entriesPerPartition.map { <span class="keyword">case</span> (topicPartition, _) =&gt;</span><br><span class="line">            topicPartition -&gt; <span class="keyword">new</span> <span class="type">PartitionResponse</span>(</span><br><span class="line">                <span class="type">Errors</span>.<span class="type">INVALID_REQUIRED_ACKS</span>, <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>.firstOffset, <span class="type">Record</span>.<span class="type">NO_TIMESTAMP</span>)</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 回调响应</span></span><br><span class="line">        responseCallback(responseStatus)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果请求的 acks 参数合法，则会调用 <code>ReplicaManager#appendToLocalLog</code> 方法往相应 leader 副本对应的 Log 对象中追加日志数据，并依据以下条件决定是否延时响应：</p>
<ol>
<li>acks 参数为 -1，表示需要 ISR 集合中全部的 follower 副本确认追加的消息数据；</li>
<li>请求添加的消息数据不为空；</li>
<li>至少有一个 topic 分区的消息追加成功。</li>
</ol>
<p>如果上面 3 个条件同时满足，则方法会创建对应的 DelayedProduce 延时任务对象，并交由相应的炼狱进行管理。DelayedProduce 对象封装了响应回调函数（即 <code>KafkaApis#handleProducerRequest</code> 方法中定义的 sendResponseCallback 方法），当 ISR 集合中所有的 follower 副本完成对本次追加的日志数据的同步操作之后会触发响应操作，这里延时任务监听的 key 是 topic 分区对象，当某个 topic 分区完成消息追加操作时可以提前触发延时任务执行。关于 DelayedProduce 延时任务我们已经在前面分析过，读者可以将上述逻辑与上一篇中对 DelayedProduce 的分析结合起来进一步加深理解。</p>
<p>方法 <code>ReplicaManager#appendToLocalLog</code> 的实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">appendToLocalLog</span></span>(internalTopicsAllowed: <span class="type">Boolean</span>, <span class="comment">// 是否允许往内部 topic 追加消息</span></span><br><span class="line">                             entriesPerPartition: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">MemoryRecords</span>], <span class="comment">// 对应分区需要追加的消息数据</span></span><br><span class="line">                             requiredAcks: <span class="type">Short</span> <span class="comment">// acks</span></span><br><span class="line">                            ): <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">LogAppendResult</span>] = {</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历处理每个 topic 分区及其待追加的消息数据</span></span><br><span class="line">    entriesPerPartition.map { <span class="keyword">case</span> (topicPartition, records) =&gt;</span><br><span class="line">        <span class="comment">// 如果追加的对象是内部 topic，依据参数 internalTopicsAllowed 决定是否追加</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="type">Topic</span>.isInternal(topicPartition.topic) &amp;&amp; !internalTopicsAllowed) {</span><br><span class="line">            (topicPartition, <span class="type">LogAppendResult</span>(</span><br><span class="line">                <span class="type">LogAppendInfo</span>.<span class="type">UnknownLogAppendInfo</span>,</span><br><span class="line">                <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">InvalidTopicException</span>(<span class="string">s"Cannot append to internal topic <span class="subst">${topicPartition.topic}</span>"</span>))))</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                <span class="comment">// 获取 topic 分区对应的 Partition 对象</span></span><br><span class="line">                <span class="keyword">val</span> partitionOpt = <span class="keyword">this</span>.getPartition(topicPartition)</span><br><span class="line">                <span class="keyword">val</span> info = partitionOpt <span class="keyword">match</span> {</span><br><span class="line">                    <span class="comment">// 往 leader 副本对应的 Log 对象中追加消息数据</span></span><br><span class="line">                    <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt; partition.appendRecordsToLeader(records, requiredAcks)</span><br><span class="line">                    <span class="comment">// 找不到 topic 分区对应的 Partition 对象</span></span><br><span class="line">                    <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnknownTopicOrPartitionException</span>(<span class="string">"Partition %s doesn't exist on %d"</span>.format(topicPartition, localBrokerId))</span><br><span class="line">                }</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 返回每个分区写入的消息结果</span></span><br><span class="line">                (topicPartition, <span class="type">LogAppendResult</span>(info))</span><br><span class="line">            } <span class="keyword">catch</span> {</span><br><span class="line">                <span class="comment">// ... 省略异常处理</span></span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法最终调用了 <code>Partition#appendRecordsToLeader</code> 方法将消息数据追加到指定 topic 分区的 leader 副本中。</p>
<p>ReplicaManager 定义了 <code>ReplicaManager#fetchMessages</code> 方法，用于处理来自消费者或 follower 副本读取消息数据的 FetchRequest 请求。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetchMessages</span></span>(timeout: <span class="type">Long</span>,</span><br><span class="line">                  replicaId: <span class="type">Int</span>,</span><br><span class="line">                  fetchMinBytes: <span class="type">Int</span>,</span><br><span class="line">                  fetchMaxBytes: <span class="type">Int</span>,</span><br><span class="line">                  hardMaxBytesLimit: <span class="type">Boolean</span>,</span><br><span class="line">                  fetchInfos: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)],</span><br><span class="line">                  quota: <span class="type">ReplicaQuota</span> = <span class="type">UnboundedQuota</span>,</span><br><span class="line">                  responseCallback: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">FetchPartitionData</span>)] =&gt; <span class="type">Unit</span>) {</span><br><span class="line">    <span class="comment">// 标记是否是来自 follower 的 fetch 请求</span></span><br><span class="line">    <span class="keyword">val</span> isFromFollower = replicaId &gt;= <span class="number">0</span></span><br><span class="line">    <span class="comment">// 是否只读 leader 副本的消息，一般 debug 模式下可以读 follower 副本的数据</span></span><br><span class="line">    <span class="keyword">val</span> fetchOnlyFromLeader: <span class="type">Boolean</span> = replicaId != <span class="type">Request</span>.<span class="type">DebuggingConsumerId</span></span><br><span class="line">    <span class="comment">// 是否只读已完成提交的消息（即 HW 之前的消息），如果是来自消费者的请求则该参数是 true，如果是 follower 则该参数是 false</span></span><br><span class="line">    <span class="keyword">val</span> fetchOnlyCommitted: <span class="type">Boolean</span> = !<span class="type">Request</span>.isValidBrokerId(replicaId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取指定位置和大小的消息数据</span></span><br><span class="line">    <span class="keyword">val</span> logReadResults = <span class="keyword">this</span>.readFromLocalLog(</span><br><span class="line">        replicaId = replicaId,</span><br><span class="line">        fetchOnlyFromLeader = fetchOnlyFromLeader,</span><br><span class="line">        readOnlyCommitted = fetchOnlyCommitted,</span><br><span class="line">        fetchMaxBytes = fetchMaxBytes,</span><br><span class="line">        hardMaxBytesLimit = hardMaxBytesLimit,</span><br><span class="line">        readPartitionInfo = fetchInfos,</span><br><span class="line">        quota = quota)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果当前是来自 follower 的同步消息数据请求，则更新 follower 副本的状态，</span></span><br><span class="line">    <span class="comment">// 并尝试扩张 ISR 集合，同时尝试触发监听对应 topic 分区的 DelayedProduce 延时任务</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="type">Request</span>.isValidBrokerId(replicaId))</span><br><span class="line">        <span class="keyword">this</span>.updateFollowerLogReadResults(replicaId, logReadResults)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> logReadResultValues = logReadResults.map { <span class="keyword">case</span> (_, v) =&gt; v }</span><br><span class="line">    <span class="keyword">val</span> bytesReadable = logReadResultValues.map(_.info.records.sizeInBytes).sum</span><br><span class="line">    <span class="keyword">val</span> errorReadingData = logReadResultValues.foldLeft(<span class="literal">false</span>)(</span><br><span class="line">        (errorIncurred, readResult) =&gt; errorIncurred || (readResult.error != <span class="type">Errors</span>.<span class="type">NONE</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (timeout &lt;= <span class="number">0</span> <span class="comment">// 请求希望立即响应</span></span><br><span class="line">            || fetchInfos.isEmpty <span class="comment">// 请求不期望有响应数据</span></span><br><span class="line">            || bytesReadable &gt;= fetchMinBytes <span class="comment">// 已经有足够的数据可以响应</span></span><br><span class="line">            || errorReadingData) { <span class="comment">// 读取数据出现错误</span></span><br><span class="line">        <span class="keyword">val</span> fetchPartitionData = logReadResults.map { <span class="keyword">case</span> (tp, result) =&gt;</span><br><span class="line">            tp -&gt; <span class="type">FetchPartitionData</span>(result.error, result.hw, result.info.records)</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 立即响应</span></span><br><span class="line">        responseCallback(fetchPartitionData)</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 构造 DelayedFetch 延时任务</span></span><br><span class="line">        <span class="keyword">val</span> fetchPartitionStatus = logReadResults.map { <span class="keyword">case</span> (topicPartition, result) =&gt;</span><br><span class="line">            <span class="keyword">val</span> fetchInfo = fetchInfos.collectFirst {</span><br><span class="line">                <span class="keyword">case</span> (tp, v) <span class="keyword">if</span> tp == topicPartition =&gt; v</span><br><span class="line">            }.getOrElse(sys.error(<span class="string">s"Partition <span class="subst">$topicPartition</span> not found in fetchInfos"</span>))</span><br><span class="line">            (topicPartition, <span class="type">FetchPartitionStatus</span>(result.info.fetchOffsetMetadata, fetchInfo))</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">val</span> fetchMetadata = <span class="type">FetchMetadata</span>(fetchMinBytes, fetchMaxBytes, hardMaxBytesLimit,</span><br><span class="line">            fetchOnlyFromLeader, fetchOnlyCommitted, isFromFollower, replicaId, fetchPartitionStatus)</span><br><span class="line">        <span class="keyword">val</span> delayedFetch = <span class="keyword">new</span> <span class="type">DelayedFetch</span>(timeout, fetchMetadata, <span class="keyword">this</span>, quota, responseCallback)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构造延时任务关注的 key，即相应的 topic 分区对象</span></span><br><span class="line">        <span class="keyword">val</span> delayedFetchKeys = fetchPartitionStatus.map { <span class="keyword">case</span> (tp, _) =&gt; <span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(tp) }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 交由炼狱管理</span></span><br><span class="line">        delayedFetchPurgatory.tryCompleteElseWatch(delayedFetch, delayedFetchKeys)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>从指定 topic 分区 leader 副本拉取消息的整体执行流程如下：</p>
<ol>
<li>从本地副本读取指定位置和大小的消息数据；</li>
<li>如果是来自 follower 副本的请求，则更新对应的 follower 副本的状态信息，并尝试扩张对应 topic 分区的 ISR 集合，同时尝试执行监听该分区的 DelayedProduce 延时任务；</li>
<li>判定是否需要对请求方进行立即响应，如果需要则立即触发响应回调函数；</li>
<li>否则，构造 DelayedFetch 延时任务，监听对应的 topic 分区对象，并交由炼狱管理。</li>
</ol>
<p>下面对上述各个步骤逐一进行分析，首先来看 <strong>步骤 1</strong> ，对应 <code>ReplicaManager#readFromLocalLog</code> 方法，实现了从本地读取指定 topic 分区相应位置和大小的消息数据的功能，具体的消息数据读操作由 <code>Log#read</code> 方法实现。方法 <code>ReplicaManager#readFromLocalLog</code> 实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readFromLocalLog</span></span>(replicaId: <span class="type">Int</span>, <span class="comment">// 请求的 follower 副本 ID</span></span><br><span class="line">                     fetchOnlyFromLeader: <span class="type">Boolean</span>, <span class="comment">// 是否只读 leader 副本的消息，一般 debug 模式下可以读 follower 副本的数据</span></span><br><span class="line">                     readOnlyCommitted: <span class="type">Boolean</span>, <span class="comment">// 是否只读已完成提交的消息（即 HW 之前的消息），如果是来自消费者的请求则该参数是 true，如果是 follower 则该参数是 false</span></span><br><span class="line">                     fetchMaxBytes: <span class="type">Int</span>, <span class="comment">// 最大 fetch 字节数</span></span><br><span class="line">                     hardMaxBytesLimit: <span class="type">Boolean</span>,</span><br><span class="line">                     readPartitionInfo: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">PartitionData</span>)], <span class="comment">// 每个分区读取的起始 offset 和最大字节数</span></span><br><span class="line">                     quota: <span class="type">ReplicaQuota</span>): <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)] = {</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(tp: <span class="type">TopicPartition</span>, fetchInfo: <span class="type">PartitionData</span>, limitBytes: <span class="type">Int</span>, minOneMessage: <span class="type">Boolean</span>): <span class="type">LogReadResult</span> = {</span><br><span class="line">        <span class="keyword">val</span> offset = fetchInfo.offset</span><br><span class="line">        <span class="keyword">val</span> partitionFetchSize = fetchInfo.maxBytes</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// 获取待读取消息的副本对象，一般都是从本地副本读取（debug 模式除外）</span></span><br><span class="line">            <span class="keyword">val</span> localReplica = <span class="keyword">if</span> (fetchOnlyFromLeader) getLeaderReplicaIfLocal(tp) <span class="keyword">else</span> getReplicaOrException(tp)</span><br><span class="line">            <span class="comment">// 计算读取消息的 offset 上界，如果是来自消费者的请求，则上界为 HW，如果是来自 follower 的请求，则上界为 LEO</span></span><br><span class="line">            <span class="keyword">val</span> maxOffsetOpt = <span class="keyword">if</span> (readOnlyCommitted) <span class="type">Some</span>(localReplica.highWatermark.messageOffset) <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">            <span class="keyword">val</span> initialLogEndOffset = localReplica.logEndOffset.messageOffset <span class="comment">// LEO</span></span><br><span class="line">            <span class="keyword">val</span> initialHighWatermark = localReplica.highWatermark.messageOffset <span class="comment">// HW</span></span><br><span class="line">            <span class="keyword">val</span> fetchTimeMs = time.milliseconds</span><br><span class="line">            <span class="keyword">val</span> logReadInfo = localReplica.log <span class="keyword">match</span> {</span><br><span class="line">                <span class="keyword">case</span> <span class="type">Some</span>(log) =&gt;</span><br><span class="line">                    <span class="keyword">val</span> adjustedFetchSize = math.min(partitionFetchSize, limitBytes)</span><br><span class="line">                    <span class="comment">// 从 Log 中读取消息数据</span></span><br><span class="line">                    <span class="keyword">val</span> fetch = log.read(offset, adjustedFetchSize, maxOffsetOpt, minOneMessage)</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 限流检测</span></span><br><span class="line">                    <span class="keyword">if</span> (shouldLeaderThrottle(quota, tp, replicaId)) <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">                    <span class="comment">// For FetchRequest version 3, we replace incomplete message sets with an empty one as consumers can make</span></span><br><span class="line">                    <span class="comment">// progress in such cases and don't need to report a `RecordTooLargeException`</span></span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> (!hardMaxBytesLimit &amp;&amp; fetch.firstEntryIncomplete) <span class="type">FetchDataInfo</span>(fetch.fetchOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">                    <span class="keyword">else</span> fetch</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 对应副本的 Log 对象不存在</span></span><br><span class="line">                <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">                    error(<span class="string">s"Leader for partition <span class="subst">$tp</span> does not have a local log"</span>)</span><br><span class="line">                    <span class="type">FetchDataInfo</span>(<span class="type">LogOffsetMetadata</span>.<span class="type">UnknownOffsetMetadata</span>, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 封装结果返回</span></span><br><span class="line">            <span class="type">LogReadResult</span>(info = logReadInfo,</span><br><span class="line">                hw = initialHighWatermark,</span><br><span class="line">                leaderLogEndOffset = initialLogEndOffset,</span><br><span class="line">                fetchTimeMs = fetchTimeMs,</span><br><span class="line">                readSize = partitionFetchSize,</span><br><span class="line">                exception = <span class="type">None</span>)</span><br><span class="line">        } <span class="keyword">catch</span> {</span><br><span class="line">            <span class="comment">// ... 省略异常处理</span></span><br><span class="line">        }</span><br><span class="line">    } <span class="comment">// ~ end of read</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> limitBytes = fetchMaxBytes</span><br><span class="line">    <span class="keyword">val</span> result = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]</span><br><span class="line">    <span class="keyword">var</span> minOneMessage = !hardMaxBytesLimit</span><br><span class="line">    <span class="comment">// 遍历读取每个 topic 分区的消息数据</span></span><br><span class="line">    readPartitionInfo.foreach {</span><br><span class="line">        <span class="keyword">case</span> (tp, fetchInfo) =&gt;</span><br><span class="line">            <span class="keyword">val</span> readResult = read(tp, fetchInfo, limitBytes, minOneMessage)</span><br><span class="line">            <span class="keyword">val</span> messageSetSize = readResult.info.records.sizeInBytes</span><br><span class="line">            <span class="keyword">if</span> (messageSetSize &gt; <span class="number">0</span>) minOneMessage = <span class="literal">false</span></span><br><span class="line">            limitBytes = math.max(<span class="number">0</span>, limitBytes - messageSetSize)</span><br><span class="line">            result += (tp -&gt; readResult)</span><br><span class="line">    }</span><br><span class="line">    result</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果本次请求是由 follower 副本发起，则会执行 <code>ReplicaManager#updateFollowerLogReadResults</code> 方法（ <strong>步骤 2</strong> ），该方法主要做了以下 4 件事情：</p>
<ol>
<li>更新指定 follower 副本的状态信息（包括 LEO 值、最近一次成功从 leader 拉取消息的时间戳等）；</li>
<li>尝试扩张副本所属分区的 ISR 集合，因为 follower 的 LEO 值递增，可能已经符合加入 ISR 集合的条件；</li>
<li>因为有新的消息被成功追加，尝试后移对应 leader 副本的 HW 值；</li>
<li>尝试执行监听对应 topic 分区的 DelayedProduce 延时任务。</li>
</ol>
<p>方法 <code>ReplicaManager#updateFollowerLogReadResults</code> 的实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateFollowerLogReadResults</span></span>(replicaId: <span class="type">Int</span>, readResults: <span class="type">Seq</span>[(<span class="type">TopicPartition</span>, <span class="type">LogReadResult</span>)]) {</span><br><span class="line">    debug(<span class="string">"Recording follower broker %d log read results: %s "</span>.format(replicaId, readResults))</span><br><span class="line">    <span class="comment">// 遍历处理对应 topic 分区的日志数据读取结果</span></span><br><span class="line">    readResults.foreach {</span><br><span class="line">        <span class="keyword">case</span> (topicPartition, readResult) =&gt;</span><br><span class="line">            getPartition(topicPartition) <span class="keyword">match</span> {</span><br><span class="line">                <span class="keyword">case</span> <span class="type">Some</span>(partition) =&gt;</span><br><span class="line">                    <span class="comment">// 更新指定 follower 副本的状态，并尝试扩张对应分区的 ISR 集合，以及后移 leader 副本的 HW 值</span></span><br><span class="line">                    partition.updateReplicaLogReadResult(replicaId, readResult)</span><br><span class="line">                    <span class="comment">// 尝试执行 DelayedProduce 延时任务，因为此时对应 topic 分区下已经有新的消息成功写入</span></span><br><span class="line">                    <span class="keyword">this</span>.tryCompleteDelayedProduce(<span class="keyword">new</span> <span class="type">TopicPartitionOperationKey</span>(topicPartition))</span><br><span class="line">                <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">                    warn(<span class="string">"While recording the replica LEO, the partition %s hasn't been created."</span>.format(topicPartition))</span><br><span class="line">            }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p><strong>步骤 3</strong> 会判定是否需要立即响应当前拉取消息的 FetchRequest 请求，如果满足以下条件之一则执行回调函数，立即响应请求：</p>
<ol>
<li>请求指定期望立即响应。</li>
<li>请求不期望有响应数据。</li>
<li>当前已经有足够的响应数据。</li>
<li>读取日志数据期间出错。</li>
</ol>
<p>如果满足以上条件之一，则会立即触发执行回调函数（即 <code>KafkaApis#handleFetchRequest</code> 方法中定义的 sendResponseCallback 方法）响应请求，该函数已经在前面分析过，不再重复撰述。否则会构造 DelayedFetch 延时任务，并交由相应的炼狱进行管理（ <strong>步骤 4</strong> ）。</p>

        <h4 id="集群分区状态管理">
          <a href="#集群分区状态管理" class="heading-link"><i class="fas fa-link"></i></a>集群分区状态管理</h4>
      <p>Kafka 的所有 broker 节点在本地均使用 MetadataCache 缓存整个集群上所有 topic 分区的状态信息，并由 kafka controller 通过 UpdateMetadataRequest 请求进行维护。MetadataCache 的字段定义如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[server] <span class="class"><span class="keyword">class</span> <span class="title">MetadataCache</span>(<span class="params">brokerId: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 缓存每个分区的状态信息 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> cache = mutable.<span class="type">Map</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">PartitionStateInfo</span>]]() <span class="comment">// [topic, [分区 ID, 分区状态信息]]</span></span><br><span class="line">    <span class="comment">/** kafka controller leader 的 ID */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> controllerId: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">None</span></span><br><span class="line">    <span class="comment">/** 记录当前可用的 broker 信息 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> aliveBrokers = mutable.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Broker</span>]()</span><br><span class="line">    <span class="comment">/** 记录当前可用的 broker 节点信息 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> aliveNodes = mutable.<span class="type">Map</span>[<span class="type">Int</span>, collection.<span class="type">Map</span>[<span class="type">ListenerName</span>, <span class="type">Node</span>]]()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>ReplicaManager 提供了 <code>ReplicaManager#maybeUpdateMetadataCache</code> 方法用于处理 UpdateMetadataRequest 请求，该方法首先会校验请求中 kafka controller 的年代信息，以避免处理来自已经过期的 kafka controller 的请求，对于合法的请求则会调用 <code>MetadataCache#updateCache</code> 方法更新本地缓存的整个集群的 topic 分区状态信息。前面我们已经分析了 <code>ReplicaManager#maybeUpdateMetadataCache</code> 方法，但对于其中调用的 <code>MetadataCache#updateCache</code> 方法未展开分析，这里我们继续分析一下该方法的实现：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateCache</span></span>(correlationId: <span class="type">Int</span>, updateMetadataRequest: <span class="type">UpdateMetadataRequest</span>): <span class="type">Seq</span>[<span class="type">TopicPartition</span>] = {</span><br><span class="line">    inWriteLock(partitionMetadataLock) {</span><br><span class="line">        <span class="comment">// 更新本地缓存的 kafka controller 的 ID</span></span><br><span class="line">        controllerId = updateMetadataRequest.controllerId <span class="keyword">match</span> {</span><br><span class="line">            <span class="keyword">case</span> id <span class="keyword">if</span> id &lt; <span class="number">0</span> =&gt; <span class="type">None</span></span><br><span class="line">            <span class="keyword">case</span> id =&gt; <span class="type">Some</span>(id)</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 清除本地缓存的集群可用的 broker 节点信息，并由 UpdateMetadataRequest 请求重新构建</span></span><br><span class="line">        aliveNodes.clear()</span><br><span class="line">        aliveBrokers.clear()</span><br><span class="line">        updateMetadataRequest.liveBrokers.asScala.foreach { broker =&gt;</span><br><span class="line">            <span class="comment">// aliveNodes 是一个请求热点，所以这里使用 java.util.HashMap 来提升性能，如果是 scala 2.10 之后可以使用 AnyRefMap 代替</span></span><br><span class="line">            <span class="keyword">val</span> nodes = <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">ListenerName</span>, <span class="type">Node</span>]</span><br><span class="line">            <span class="keyword">val</span> endPoints = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">EndPoint</span>]</span><br><span class="line">            broker.endPoints.asScala.foreach { ep =&gt;</span><br><span class="line">                endPoints += <span class="type">EndPoint</span>(ep.host, ep.port, ep.listenerName, ep.securityProtocol)</span><br><span class="line">                nodes.put(ep.listenerName, <span class="keyword">new</span> <span class="type">Node</span>(broker.id, ep.host, ep.port))</span><br><span class="line">            }</span><br><span class="line">            aliveBrokers(broker.id) = <span class="type">Broker</span>(broker.id, endPoints, <span class="type">Option</span>(broker.rack))</span><br><span class="line">            aliveNodes(broker.id) = nodes.asScala</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 基于 UpdateMetadataRequest 请求更新每个分区的状态信息，并返回需要被移除的分区集合</span></span><br><span class="line">        <span class="keyword">val</span> deletedPartitions = <span class="keyword">new</span> mutable.<span class="type">ArrayBuffer</span>[<span class="type">TopicPartition</span>]</span><br><span class="line">        updateMetadataRequest.partitionStates.asScala.foreach {</span><br><span class="line">            <span class="keyword">case</span> (tp, info) =&gt;</span><br><span class="line">                <span class="keyword">val</span> controllerId = updateMetadataRequest.controllerId</span><br><span class="line">                <span class="keyword">val</span> controllerEpoch = updateMetadataRequest.controllerEpoch</span><br><span class="line">                <span class="comment">// 如果请求标记对应的 topic 分区需要被删除</span></span><br><span class="line">                <span class="keyword">if</span> (info.leader == <span class="type">LeaderAndIsr</span>.<span class="type">LeaderDuringDelete</span>) {</span><br><span class="line">                    <span class="comment">// 删除本地缓存的对应 topic 分区的状态信息</span></span><br><span class="line">                    <span class="keyword">this</span>.removePartitionInfo(tp.topic, tp.partition)</span><br><span class="line">                    deletedPartitions += tp</span><br><span class="line">                } <span class="keyword">else</span> {</span><br><span class="line">                    <span class="comment">// PartitionState -&gt; PartitionStateInfo</span></span><br><span class="line">                    <span class="keyword">val</span> partitionInfo = <span class="keyword">this</span>.partitionStateToPartitionStateInfo(info)</span><br><span class="line">                    <span class="comment">// 更新本地缓存的对应 topic 分区的状态信息</span></span><br><span class="line">                    <span class="keyword">this</span>.addOrUpdatePartitionInfo(tp.topic, tp.partition, partitionInfo)</span><br><span class="line">                }</span><br><span class="line">        }</span><br><span class="line">        deletedPartitions</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>MetadataCache 使用 <code>MetadataCache#aliveBrokers</code> 和 <code>MetadataCache#aliveNodes</code> 字段记录整个集群中可用的 broker 节点信息，当收到来自 kafka controller 的 UpdateMetadataRequest 请求时，MetadataCache 会清空本地缓存，并由请求信息重新构建新的可用的 broker 节点信息。此外还会依据 UpdateMetadataRequest 请求更新本地缓存的整个集群 topic 分区的状态信息（对应 <code>MetadataCache#cache</code> 字段）。</p>
<p>MetadataCache 提供了 <code>MetadataCache#getTopicMetadata</code> 方法用于获取本地缓存的指定 topic 的元数据信息，包括是否是内部 topic，以及对应 topic 下所有分区的元数据信息。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTopicMetadata</span></span>(topics: <span class="type">Set</span>[<span class="type">String</span>],</span><br><span class="line">                     listenerName: <span class="type">ListenerName</span>,</span><br><span class="line">                     errorUnavailableEndpoints: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">Seq</span>[<span class="type">MetadataResponse</span>.<span class="type">TopicMetadata</span>] = {</span><br><span class="line">    inReadLock(partitionMetadataLock) {</span><br><span class="line">        topics.toSeq.flatMap { topic =&gt;</span><br><span class="line">            <span class="comment">// 获取指定 topic 下分区元数据信息，并与 topic 一起构造 topic 元数据对象返回</span></span><br><span class="line">            <span class="keyword">this</span>.getPartitionMetadata(topic, listenerName, errorUnavailableEndpoints).map { partitionMetadata =&gt;</span><br><span class="line">                <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">TopicMetadata</span>(<span class="type">Errors</span>.<span class="type">NONE</span>, topic, <span class="type">Topic</span>.isInternal(topic), partitionMetadata.toBuffer.asJava)</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitionMetadata</span></span>(</span><br><span class="line">                                 topic: <span class="type">String</span>,</span><br><span class="line">                                 listenerName: <span class="type">ListenerName</span>,</span><br><span class="line">                                 errorUnavailableEndpoints: <span class="type">Boolean</span>): <span class="type">Option</span>[<span class="type">Iterable</span>[<span class="type">MetadataResponse</span>.<span class="type">PartitionMetadata</span>]] = {</span><br><span class="line">    <span class="comment">// 遍历每个 topic 对应的分区集合</span></span><br><span class="line">    cache.get(topic).map { partitions =&gt;</span><br><span class="line">        partitions.map { <span class="keyword">case</span> (partitionId, partitionState) =&gt;</span><br><span class="line">            <span class="keyword">val</span> topicPartition = <span class="type">TopicAndPartition</span>(topic, partitionId)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取分区对应的 LeaderAndIsr 对象，其中封装了对应分区的 leader 副本 ID 和 ISR 集合等信息</span></span><br><span class="line">            <span class="keyword">val</span> leaderAndIsr = partitionState.leaderIsrAndControllerEpoch.leaderAndIsr</span><br><span class="line">            <span class="comment">// 获取 leader 副本所在的节点信息</span></span><br><span class="line">            <span class="keyword">val</span> maybeLeader = <span class="keyword">this</span>.getAliveEndpoint(leaderAndIsr.leader, listenerName)</span><br><span class="line">            <span class="comment">// 获取分区的 AR 集合</span></span><br><span class="line">            <span class="keyword">val</span> replicas = partitionState.allReplicas</span><br><span class="line">            <span class="comment">// 获取 AR 集合中可用的副本对应的节点信息</span></span><br><span class="line">            <span class="keyword">val</span> replicaInfo = <span class="keyword">this</span>.getEndpoints(replicas, listenerName, errorUnavailableEndpoints)</span><br><span class="line"></span><br><span class="line">            maybeLeader <span class="keyword">match</span> {</span><br><span class="line">                <span class="comment">// 分区 leader 副本不可用</span></span><br><span class="line">                <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">                    <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">PartitionMetadata</span>(<span class="type">Errors</span>.<span class="type">LEADER_NOT_AVAILABLE</span>,</span><br><span class="line">                        partitionId, <span class="type">Node</span>.noNode(), replicaInfo.asJava, java.util.<span class="type">Collections</span>.emptyList())</span><br><span class="line">                <span class="keyword">case</span> <span class="type">Some</span>(leader) =&gt;</span><br><span class="line">                    <span class="comment">// 获取分区的 ISR 集合</span></span><br><span class="line">                    <span class="keyword">val</span> isr = leaderAndIsr.isr</span><br><span class="line">                    <span class="comment">// 获取 ISR 集合中可用的副本对应的节点信息</span></span><br><span class="line">                    <span class="keyword">val</span> isrInfo = <span class="keyword">this</span>.getEndpoints(isr, listenerName, errorUnavailableEndpoints)</span><br><span class="line">                    <span class="keyword">if</span> (replicaInfo.size &lt; replicas.size) {</span><br><span class="line">                        <span class="comment">// 如果 AR 集合中存在不可用的副本，则返回 REPLICA_NOT_AVAILABLE 错误</span></span><br><span class="line">                        <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">PartitionMetadata</span>(<span class="type">Errors</span>.<span class="type">REPLICA_NOT_AVAILABLE</span>, partitionId, leader, replicaInfo.asJava, isrInfo.asJava)</span><br><span class="line">                    } <span class="keyword">else</span> <span class="keyword">if</span> (isrInfo.size &lt; isr.size) {</span><br><span class="line">                        <span class="comment">// 如果 ISR 集合中存在不可用的的副本，则返回 REPLICA_NOT_AVAILABLE 错误</span></span><br><span class="line">                        <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">PartitionMetadata</span>(<span class="type">Errors</span>.<span class="type">REPLICA_NOT_AVAILABLE</span>, partitionId, leader, replicaInfo.asJava, isrInfo.asJava)</span><br><span class="line">                    } <span class="keyword">else</span> {</span><br><span class="line">                        <span class="comment">// AR 集合和 ISR 集合中的副本都是可用的</span></span><br><span class="line">                        <span class="keyword">new</span> <span class="type">MetadataResponse</span>.<span class="type">PartitionMetadata</span>(<span class="type">Errors</span>.<span class="type">NONE</span>, partitionId, leader, replicaInfo.asJava, isrInfo.asJava)</span><br><span class="line">                    }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>方法 <code>MetadataCache#getPartitionMetadata</code> 会校验对应分区的 AR 集合和 ISR 集合中的副本是否可用，如果存在不可用的副本则会返回 <code>REPLICA_NOT_AVAILABLE</code> 错误，如果分区的副本均可用则会返回分区的元数据信息，包括分区 ID、leader 副本所在节点信息、AR 集合，以及 ISR 集合。</p>

        <h3 id="总结">
          <a href="#总结" class="heading-link"><i class="fas fa-link"></i></a>总结</h3>
      <p>本文我们分析了 Kafka 的分区副本实现机制，了解到 Kafka 会为每个 topic 分区设置多个副本，并基于 leader/follower 模式将这些副本分为一个 leader 角色和多个 follower 角色。在 topic 分区正常运行期间，由 leader 副本负责处理来自客户端的消息读写请求，而 follower 副本仅负责从 leader 副本同步消息数据。一旦 leader 副本失效，Kafka 会从位于 ISR 集合中的 follower 副本中选择一个成为新的 leader 副本，以保证对应的 topic 能够继续对外提供服务。</p>
<p>冗余策略在分布式计算和存储领域是一种简单且有效的可靠性保障措施，了解 Kafka 的分区副本实现机制能够指导我们更好的设计实现自己的分布式应用。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://plotor.github.io">zhenchao</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://plotor.github.io/2019/06/24/kafka/kafka-replica/">https://plotor.github.io/2019/06/24/kafka/kafka-replica/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://plotor.github.io/tags/Kafka/">Kafka</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2019/06/25/kafka/kafka-group-coordinator/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Kafka 源码解析：Group 协调管理机制</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2019/06/23/kafka/kafka-purgatory/"><span class="paginator-prev__text">Kafka 源码解析：延时任务调度策略</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="utterances-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Replica-%E7%BB%84%E4%BB%B6"><span class="toc-number">1.</span> <span class="toc-text">
          Replica 组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Partition-%E7%BB%84%E4%BB%B6"><span class="toc-number">2.</span> <span class="toc-text">
          Partition 组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E5%AF%B9%E8%B1%A1%E6%93%8D%E4%BD%9C"><span class="toc-number">2.1.</span> <span class="toc-text">
          副本对象操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E8%A7%92%E8%89%B2%E5%88%87%E6%8D%A2"><span class="toc-number">2.2.</span> <span class="toc-text">
          副本角色切换</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%87%E6%8D%A2%E6%9C%AC%E5%9C%B0%E5%89%AF%E6%9C%AC%E4%B8%BA-leader-%E8%A7%92%E8%89%B2"><span class="toc-number">2.2.1.</span> <span class="toc-text">
          切换本地副本为 leader 角色</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%87%E6%8D%A2%E6%9C%AC%E5%9C%B0%E5%89%AF%E6%9C%AC%E4%B8%BA-follower-%E8%A7%92%E8%89%B2"><span class="toc-number">2.2.2.</span> <span class="toc-text">
          切换本地副本为 follower 角色</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">2.3.</span> <span class="toc-text">
          日志数据操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ISR-%E9%9B%86%E5%90%88%E7%AE%A1%E7%90%86"><span class="toc-number">2.4.</span> <span class="toc-text">
          ISR 集合管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HW-%E5%92%8C-LEO-%E4%BD%8D%E7%BD%AE%E7%AE%A1%E7%90%86"><span class="toc-number">2.5.</span> <span class="toc-text">
          HW 和 LEO 位置管理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReplicaManager-%E7%BB%84%E4%BB%B6"><span class="toc-number">3.</span> <span class="toc-text">
          ReplicaManager 组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6"><span class="toc-number">3.1.</span> <span class="toc-text">
          消息同步机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E8%A7%92%E8%89%B2%E5%88%87%E6%8D%A2-1"><span class="toc-number">3.2.</span> <span class="toc-text">
          副本角色切换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E4%B8%8E%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86"><span class="toc-number">3.3.</span> <span class="toc-text">
          分区与副本管理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99"><span class="toc-number">3.4.</span> <span class="toc-text">
          日志数据读写</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E5%88%86%E5%8C%BA%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86"><span class="toc-number">3.5.</span> <span class="toc-text">
          集群分区状态管理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">
          总结</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/author.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">追求技术深度，注重文章质量</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/plotor" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://weibo.com/" target="_blank" rel="noopener" data-popover="微博" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weibo"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="微信" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weixin"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="QQ" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-qq"></i></span></a><a class="sidebar-ov-social-item" href="https://twitter.com/" target="_blank" rel="noopener" data-popover="Twitter" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-twitter"></i></span></a></div><div class="sidebar-ov-feed"><span class="sidebar-ov-feed-rss"><a class="sidebar-ov-feed-rss__link" href="/atom.xml" target="_blank" rel="noopener"><span class="sidebar-ov-feed-rss__icon"><i class="fas fa-rss"></i></span><span>RSS 订阅</span></a></span></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">96</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">14</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">29</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2015~2025</span><span class="footer__devider"></span><span>Zhenchao All Rights Reserved</span><span class="footer__devider">|</span><span>浙ICP备 16010916 号</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.3.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script>function initSearch() {
  var isXML = true;
  var search_path = 'search.json';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;
  $.ajax({
    url: path,
    dataType: isXML ? 'xml' : 'json',
    async: true,
    success: function (res) {
      var datas = isXML ? $('entry', res).map(function () {
        // 将 XML 转为 JSON
        return {
          title: $('title', this).text(),
          content: $('content', this).text(),
          url: $('url', this).text()
        };
      }).get() : res;
      var $input = $('.search-input input');
      var $result = $('.search-results');
      // 搜索对象（标题、内容）的权重，影响显示顺序
      var WEIGHT = { title: 100, content: 1 };
      var searchPost = function () {
        var searchText = $input.val().toLowerCase().trim();
        // 根据空白字符分隔关键字
        var keywords = searchText.split(/[\s]+/);
        // 搜索结果
        var matchPosts = [];

        // 有多个关键字时，将原文字整个保存下来
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        // 防止未输入字符时搜索
        if (searchText.length > 0) {
          datas.forEach(function (data) {
            var isMatch  = false;
            // 没有标题的文章使用预设的 i18n 变量代替
            var title = (data.title && data.title.trim()) || '[ 文章无标题 ]';
            var titleLower = title && title.toLowerCase();
            // 删除 HTML 标签 和 所有空白字符
            var content = data.content && data.content.replace(/<[^>]+>/g, '');
            var contentLower = content && content.toLowerCase();
            // 删除重复的 /
            var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');
            // 标题中匹配到的关键词
            var titleHitSlice = [];
            // 内容中匹配到的关键词
            var contentHitSlice = [];

            keywords.forEach(function (keyword) {
              /**
              * 获取匹配的关键词的索引
              * @param {String} keyword 要匹配的关键字
              * @param {String} text 原文字
              * @param {Boolean} caseSensitive 是否区分大小写
              * @param {Number} weight 匹配对象的权重。权重大的优先显示
              * @return {Array}
              */
              function getIndexByword (word, text, caseSensitive, weight) {
                if (!word || !text) {
                  return [];
                };

                var startIndex = 0; // 每次匹配的开始索引
                var index = -1;     // 匹配到的索引值
                var result = [];    // 匹配结果

                if (!caseSensitive) {
                  word = word.toLowerCase();
                  text = text.toLowerCase();
                }

                while((index = text.indexOf(word, startIndex)) !== -1) {
                  var hasMatch = false;
                  // 索引位置相同的关键词，保留长度较长的
                  titleHitSlice.forEach(function (hit) {
                    if (hit.index === index && hit.word.length < word.length) {
                      hit.word = word;
                      hasMatch = true;
                    }
                  });
                  startIndex = index + word.length;
                  !hasMatch && result.push({ index: index, word: word, weight: weight });
                }
                return result;
              }
              titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
              contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
            });

            var hitTitle = titleHitSlice.length;
            var hitContent = contentHitSlice.length;

            if (hitTitle > 0 || hitContent > 0) {
              isMatch = true;
            }
            if (isMatch) {
              ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                // 按照匹配文字的索引的递增顺序排序
                hit.sort(function (left, right) {
                  return left.index - right.index;
                });
              });
              /**
              * 给文本中匹配到的关键词添加标记，从而进行高亮显示
              * @param {String} text 原文本
              * @param {Array} hitSlice 匹配项的索引信息
              * @param {Number} start 开始索引
              * @param {Number} end 结束索引
              * @return {String}
              */
              function highlightKeyword (text, hitSlice, start, end) {
                if (!text || !hitSlice || !hitSlice.length) {
                  return;
                }

                var result = '';
                var startIndex = start;
                var endIndex = end;
                hitSlice.forEach(function (hit) {
                  if (hit.index < startIndex) {
                    return;
                  }

                  var hitWordEnd = hit.index + hit.word.length;
                  result += text.slice(startIndex, hit.index);
                  result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                  startIndex = hitWordEnd;
                });
                result += text.slice(startIndex, endIndex);
                return result;
              }

              var postData = {};
              // 文章总的搜索权重
              var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
              // 标记匹配关键词后的标题
              var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
              // 标记匹配关键词后的内容
              var postContent;
              // 显示内容的长度
              var SHOW_WORD_LENGTH = 200;
              // 命中关键词前的字符显示长度
              var SHOW_WORD_FRONT_LENGTH = 20;
              var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

              // 截取匹配的第一个字符，前后共 200 个字符来显示
              if (contentHitSlice.length > 0) {
                var firstIndex = contentHitSlice[0].index;
                var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                var end = firstIndex + SHOW_WORD_END_LENGTH;
                postContent = highlightKeyword(content, contentHitSlice, start, end);
              } else { // 未匹配到内容，直接截取前 200 个字符来显示
                postContent = content.slice(0, SHOW_WORD_LENGTH);
              }
              postData.title = postTitle;
              postData.content = postContent;
              postData.url = postURL;
              postData.weight = postWeight;
              matchPosts.push(postData);
            }
          });
        }

        var resultInnerHtml = '';
        if (matchPosts.length) {
          // 按权重递增的顺序排序，使权重大的优先显示
          matchPosts.sort(function (left, right) {
            return right.weight - left.weight;
          });
          resultInnerHtml += '<ul>';
          matchPosts.forEach(function (post) {
            resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
            resultInnerHtml += post.title;
            resultInnerHtml += '</a><div class="search-results-content">';
            resultInnerHtml += post.content;
            resultInnerHtml += '</div></li>';
          });
          resultInnerHtml += '</ul>';
        } else {
          resultInnerHtml += '<div class="search-results-none"><i class="far fa-meh"></i></div>';
        }
        $result.html(resultInnerHtml);
      };
      $input.on('input', searchPost);
      $input.on('keyup', function (e) {
        if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
          searchPost();
        }
      });
    }
  });
}

function closeSearch () {
  $('body').css({ overflow: 'auto' });
  $('.search-popup').css({ display: 'none' });
  $('.search-mask').css({ display: 'none' });
}

window.addEventListener('DOMContentLoaded', function () {
  Stun.utils.pjaxReloadLocalSearch = function () {
    $('.header-nav-search').on('click', function (e) {
      e.stopPropagation();
      $('body').css('overflow', 'hidden');
      $('.search-popup')
        .velocity('stop')
        .velocity('transition.expandIn', {
          duration: 300,
          complete: function () {
            $('.search-popup input').focus();
          }
        });
      $('.search-mask')
        .velocity('stop')
        .velocity('transition.fadeIn', {
          duration: 300
        });

      initSearch();
    });
    $('.search-mask, .search-close').on('click', function () {
      closeSearch();
    });
    $(document).on('keydown', function (e) {
      // Escape <=> 27
      if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
        closeSearch();
      }
    });
  };

  Stun.utils.pjaxReloadLocalSearch();
}, false);

function safeOpenUrl(url) {
  var newTab = window.open();
  newTab.opener = null;
  newTab.location = url;
}

function extSearch(engine) {
  var engines = {
    google: 'https://www.google.com/search?q=',
    bing: 'https://cn.bing.com/search?q=',
    baidu: 'https://www.baidu.com/s?ie=UTF-8&wd=',
  };
  var host = window.location.host;
  var query = $('.search-input input').val().toLowerCase().trim();
  var uri = engines[engine] + query + ' site:' + host;

  if (query) {
    safeOpenUrl(uri);
  } else {
    Stun.utils.popAlert('warning', '请输入字符');
  }
}

var assistSearchList = window.CONFIG.assistSearch;

if (Array.isArray(assistSearchList)) {
  assistSearchList.forEach(function (name) {
    document.querySelector('.search-btns-item--' + name).addEventListener('click', function () {
      extSearch(name);
    }, false);
  });
}</script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload",".header-inner"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (true) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"></div><script data-pjax="">function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'plotor/hexo-comments');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (true) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (true) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.6.1"></script><script src="/js/stun-boot.js?v=2.6.1"></script><script src="/js/scroll.js?v=2.6.1"></script><script src="/js/header.js?v=2.6.1"></script><script src="/js/sidebar.js?v=2.6.1"></script><script type="application/json" src="/search.json"></script></body></html>