<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/favicon_16.png?v=2.6.1" type="image/png" sizes="16x16"><link rel="icon" href="/images/favicon_32.png?v=2.6.1" type="image/png" sizes="32x32"><meta name="google-site-verification" content="O5CNgi37yYXs3qQp7Xz61oL_AmGiwM28d7hRt5yh2to"><meta name="baidu-site-verification" content="pnKVynCWMP"><meta name="description" content="日志数据（亦称消息数据）的存储机制在 Kafka 整个设计与实现中既基础又核心。Kafka 采用本地文件系统对日志数据进行存储，并允许为一个 broker 节点设置多个 log 文件目录，每个 log 目录下存储的数据又按照 topic 分区进行划分，其中包含了一个 topic 分区名下消息数据对应的多组日志和索引文件。 Kafka 定义了 LogSegment 类和 Log 类对日志和索引数据进">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka 源码解析：日志数据存储机制">
<meta property="og:url" content="https://plotor.github.io/2019/06/22/kafka/kafka-log-manage/index.html">
<meta property="og:site_name" content="指  间">
<meta property="og:description" content="日志数据（亦称消息数据）的存储机制在 Kafka 整个设计与实现中既基础又核心。Kafka 采用本地文件系统对日志数据进行存储，并允许为一个 broker 节点设置多个 log 文件目录，每个 log 目录下存储的数据又按照 topic 分区进行划分，其中包含了一个 topic 分区名下消息数据对应的多组日志和索引文件。 Kafka 定义了 LogSegment 类和 Log 类对日志和索引数据进">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://plotor.github.io/images/2019/kafka-log-organization.png">
<meta property="og:image" content="https://plotor.github.io/images/2019/kafka-log-file.png">
<meta property="article:published_time" content="2019-06-22T03:56:18.000Z">
<meta property="article:modified_time" content="2024-07-27T08:09:38.249Z">
<meta property="article:author" content="zhenchao">
<meta property="article:tag" content="Kafka">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://plotor.github.io/images/2019/kafka-log-organization.png"><title>Kafka 源码解析：日志数据存储机制 | 指  间</title><link ref="canonical" href="https://plotor.github.io/2019/06/22/kafka/kafka-log-manage/"><link rel="alternate" href="/atom.xml" type="application/atom+xml"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"carbon","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: {"avoidBanner":false},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user-circle"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-search"><span class="header-nav-search__icon"><i class="fas fa-search"></i></span><span class="header-nav-search__text">搜索</span></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Kafka 源码解析：日志数据存储机制</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2019-06-22</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">20.5k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">97分</span></span></div></header><div class="post-body"><p>日志数据（亦称消息数据）的存储机制在 Kafka 整个设计与实现中既基础又核心。Kafka 采用本地文件系统对日志数据进行存储，并允许为一个 broker 节点设置多个 log 文件目录，每个 log 目录下存储的数据又按照 topic 分区进行划分，其中包含了一个 topic 分区名下消息数据对应的多组日志和索引文件。</p>
<p>Kafka 定义了 LogSegment 类和 Log 类对日志和索引数据进行管理，并定义了 LogManager 类管理一个 broker 节点下的所有 Log 对象，同时基于 Log 对象提供了对日志数据的加载、创建、删除，以及查询等功能，同时还维护了多个定时任务对日志数据执行清理、删除、刷盘，以及记录 HW 位置等操作，并提供了对 key 重复的消息数据执行压缩的机制。<a id="more"></a></p>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2019/kafka-log-organization.png" alt="image">
      </p>
<p>上图展示了 topic、partition、replica、Log 和 LogSegment 之间的组织关系。在具体实现时组织如下：</p>
<ul>
<li>一个 broker 节点允许指定多个 log 目录，每个目录下包含多个以“topic-partition”命名的目录，即一个 log 目录下存储了多个 topic 分区对应的消息数据，并且一个 topic 分区只允许属于一个 log 目录。</li>
<li>每个 topic 分区目录下包含多组日志（log）和索引（index、timeindex）文件，Kafka 定义了 LogSegment 类用于封装一组日志和索引文件。</li>
<li>每个 topic 分区对应一个 Log 类对象（一个 broker 节点上只允许存放分区的一个副本，所以从 broker 视角来看一个分区对应一个 Log 类对象），其中包含了一系列隶属对应 topic 分区的 LogSegment 对象，Log 类采用跳跃表（SkipList）数据结构对这些 LogSegment 对象进行管理。</li>
</ul>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2019/kafka-log-file.png" alt="image">
      </p>
<p>上图进一步展示了 Log 与 LogSegment 之间的组织关系，以及 LogSegment 在 Log 中基于 SkipList 的组织形式（其中青色小圆圈表示单个 LogSegment 对象）。</p>

        <h3 id="LogSegment-组件">
          <a href="#LogSegment-组件" class="heading-link"><i class="fas fa-link"></i></a>LogSegment 组件</h3>
      <p>每个 topic 分区目录下通常会包含多个 log 文件，这些 log 文件 <strong>以其中保存的消息的起始 offset 命名</strong> 。每个 log 文件由一个 LogSegment 对象进行管理，其中还包含了对应的 index 和 timeindex 文件。下面是关于某个 topic 分区目录下的文件列表（生产环境中一个 topic 分区目录下一般存在多组类似下面这样的文件）：</p>
<figure class="highlight bash"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls topic-default-0/</span><br><span class="line">00000000000000000122.index  00000000000000000122.log  00000000000000000122.timeindex</span><br></pre></td></tr></tbody></table></div></figure>
<p>LogSegment 类的字段定义如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogSegment</span>(<span class="params">val log: <span class="type">FileRecords</span>, // log 文件对象</span></span></span><br><span class="line"><span class="class"><span class="params">                 val index: <span class="type">OffsetIndex</span>, // index 文件对象</span></span></span><br><span class="line"><span class="class"><span class="params">                 val timeIndex: <span class="type">TimeIndex</span>, // timeindex 文件对象</span></span></span><br><span class="line"><span class="class"><span class="params">                 val baseOffset: <span class="type">Long</span>, // 当前日志分片文件中第一条消息的 offset 值</span></span></span><br><span class="line"><span class="class"><span class="params">                 val indexIntervalBytes: <span class="type">Int</span>, // 索引项之间间隔的最小字节数，对应 index.interval.bytes 配置</span></span></span><br><span class="line"><span class="class"><span class="params">                 val rollJitterMs: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 time: <span class="type">Time</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 当前 LogSegment 的创建时间 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> created = time.milliseconds</span><br><span class="line">    <span class="comment">/** 自上次添加索引项后，在 log 文件中累计加入的消息字节数 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> bytesSinceLastIndexEntry = <span class="number">0</span></span><br><span class="line">    <span class="comment">/** The timestamp we used for time based log rolling */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> rollingBasedTimestamp: <span class="type">Option</span>[<span class="type">Long</span>] = <span class="type">None</span></span><br><span class="line">    <span class="comment">/** 已追加消息的最大时间戳 */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> maxTimestampSoFar = timeIndex.lastEntry.timestamp</span><br><span class="line">    <span class="comment">/** 已追加的具备最大时间戳的消息对应的 offset */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> offsetOfMaxTimestamp = timeIndex.lastEntry.offset</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>其中 FileRecords 类用于封装和管理对应的 log 文件，OffsetIndex 类用于封装和管理对应的 index 文件，TimeIndex 类用于封装和管理对应的 timeindex 文件。这是支撑 Kafka 日志数据存储的 3 个基础类，要理解 Kafka 的日志存储机制，我们需要先理解这 3 个类的定义。</p>
<ul>
<li><strong>FileRecords</strong></li>
</ul>
<p>FileRecords 类用于描述和管理日志（分片）文件数据，对应一个 log 文件，其字段定义如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileRecords</span> <span class="keyword">extends</span> <span class="title">AbstractRecords</span> <span class="keyword">implements</span> <span class="title">Closeable</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 标识是否为日志文件分片 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> isSlice;</span><br><span class="line">    <span class="comment">/** 分片的起始位置 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> start;</span><br><span class="line">    <span class="comment">/** 分片的结束位置 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> end;</span><br><span class="line">    <span class="comment">/** 浅层拷贝 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Iterable&lt;FileChannelLogEntry&gt; shallowEntries;</span><br><span class="line">    <span class="comment">/** 如果是分片则表示分片的大小（end - start），如果不是分片则表示整个日志文件的大小 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> AtomicInteger size;</span><br><span class="line">    <span class="comment">/** 读写对应的日志文件的通道 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FileChannel channel;</span><br><span class="line">    <span class="comment">/** 日志文件对象 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> File file;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>FileRecords 主要定义了对日志数据的追加、读取、删除、查找、截断，以及刷盘等操作，并依赖于 LogEntry 类对单条日志数据的 offset 和 value 进行封装，同时提供了对 log 文件中日志数据的 <strong>浅层遍历</strong> 和 <strong>深层遍历</strong> 操作。日志数据在追加到 log 文件中之前可能会执行压缩操作，所谓浅层遍历是指在遍历 log 文件中的日志数据时将压缩后的数据看做是一个整体，而深层遍历则会尝试对这部分日志数据执行解压缩，并返回解压缩后的单条消息。</p>
<p>下面的示例中展示了一个具体的日志文件数据的部分内容（即前面提及的 <code>00000000000000000122.log</code> 文件）：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">LogEntry(122, Record(magic = 1, attributes = 0, compression = NONE, crc = 300223964, CreateTime = 1553937143494, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(123, Record(magic = 1, attributes = 0, compression = NONE, crc = 1516889930, CreateTime = 1553937143505, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(124, Record(magic = 1, attributes = 0, compression = NONE, crc = 1201423931, CreateTime = 1553937143507, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(125, Record(magic = 1, attributes = 0, compression = NONE, crc = 1592544380, CreateTime = 1553937143507, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(126, Record(magic = 1, attributes = 0, compression = NONE, crc = 599198486, CreateTime = 1553937143508, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(127, Record(magic = 1, attributes = 0, compression = NONE, crc = 980691361, CreateTime = 1553937143509, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(128, Record(magic = 1, attributes = 0, compression = NONE, crc = 4047753804, CreateTime = 1553937143511, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(129, Record(magic = 1, attributes = 0, compression = NONE, crc = 4289660679, CreateTime = 1553937143511, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(130, Record(magic = 1, attributes = 0, compression = NONE, crc = 4016824904, CreateTime = 1553937143512, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(131, Record(magic = 1, attributes = 0, compression = NONE, crc = 3305927143, CreateTime = 1553937143512, key = 4 bytes, value = 16 bytes))</span><br><span class="line">LogEntry(132, Record(magic = 1, attributes = 0, compression = NONE, crc = 3847705666, CreateTime = 1553937143513, key = 4 bytes, value = 16 bytes))</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></tbody></table></div></figure>
<p>上面的示例中我们基于深层遍历调用 <code>LogEntry#toString</code> 方法打印了单条消息的概要信息。</p>
<ul>
<li><strong>OffsetIndex</strong></li>
</ul>
<p>OffsetIndex 类用于描述和管理索引文件数据，定义了对 index 文件的检索、追加，以及截断等功能。一个 OffsetIndex 对象对应一个 index 文件，用于提高消息检索的性能。下面的示例中展示了一个具体的 index 文件数据的部分内容（即前面提及的 <code>00000000000000000122.index</code> 文件）：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">165, 8910</span><br><span class="line">252, 13608</span><br><span class="line">355, 19170</span><br><span class="line">658, 35532</span><br><span class="line">961, 51894</span><br><span class="line">1191, 64314</span><br><span class="line">1494, 80676</span><br><span class="line">1797, 97038</span><br><span class="line">2100, 113400</span><br><span class="line">2403, 129762</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></tbody></table></div></figure>
<p>OffsetIndex 的索引项由 8 个字节构成，其中前面 4 个字节表示消息的相对 offset，后面 4 个字节表示消息所在文件的物理地址（position），其中相对 offset 参考的偏移量是对应文件的起始 offset，这样的设计将原本 long 类型（8 字节）的消息 offset 转换成 int 类型（4 字节）的相对 offset 进行存储，能够减少空间占用。此外，Kafka 在构造 index 文件（包括下面要介绍的 timeindex 文件）时并不会针对每个 offset 都建立对应的索引项，而是采用隔一段区间打一个点的稀疏索引机制，以进一步减少对磁盘空间的消耗。</p>
<ul>
<li><strong>TimeIndex</strong></li>
</ul>
<p>TimeIndex 类同样用于描述和管理索引文件数据，提供了基于时间戳检索日志数据的功能，对应 timeindex 文件。区别于 OffsetIndex 的地方在于 TimeIndex 的索引项由 12 个字节构成，其中前面 8 个字节表示当前 offset 之前已追加消息的最大时间戳（毫秒），后面 4 个字节表示相对 offset，等价于 OffsetIndex 索引项的前 4 个字节。下面的示例中展示了一个具体的 timeindex 文件数据的部分内容（即前面提及的 <code>00000000000000000122.timeindex</code> 文件）：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1553937143565, 251</span><br><span class="line">1553937143570, 284</span><br><span class="line">1553937143594, 649</span><br><span class="line">1553937143609, 944</span><br><span class="line">1553937143631, 1166</span><br><span class="line">1553937143652, 1483</span><br><span class="line">1553937143669, 1770</span><br><span class="line">1553937143691, 2096</span><br><span class="line">1553937143707, 2378</span><br><span class="line">1553937143714, 2676</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></tbody></table></div></figure>
<p>LogSegment 可以看做是对一组日志和索引文件数据的封装，并提供了对这些数据执行追加、读取、截断、删除、刷盘，以及重建等功能。本小节接下来的内容，我们重点分析一下 LogSegment 中主要的日志和索引文件数据操作方法，包括：<code>LogSegment#append</code>、<code>LogSegment#read</code> 和 <code>LogSegment#recover</code> 方法，其它方法在实现上都比较简单，读者要是感兴趣的话可以自己阅读源码。</p>

        <h4 id="追加日志数据">
          <a href="#追加日志数据" class="heading-link"><i class="fas fa-link"></i></a>追加日志数据</h4>
      <p>本小节来看一下 <code>LogSegment#append</code> 方法的实现，该方法用于往当前 LogSegment 对应的 log 文件中追加消息数据，并在需要时更新对应的 index 和 timeindex 索引数据。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(firstOffset: <span class="type">Long</span>, <span class="comment">// 待追加消息的起始 offset</span></span><br><span class="line">           largestOffset: <span class="type">Long</span>, <span class="comment">// 待追加消息中的最大 offset</span></span><br><span class="line">           largestTimestamp: <span class="type">Long</span>, <span class="comment">// 待追加消息中的最大时间戳</span></span><br><span class="line">           shallowOffsetOfMaxTimestamp: <span class="type">Long</span>, <span class="comment">// 最大时间戳消息对应的 offset</span></span><br><span class="line">           records: <span class="type">MemoryRecords</span>) { <span class="comment">// 待追加的消息数据</span></span><br><span class="line">    <span class="keyword">if</span> (records.sizeInBytes &gt; <span class="number">0</span>) {</span><br><span class="line">        trace(<span class="string">"Inserting %d bytes at offset %d at position %d with largest timestamp %d at shallow offset %d"</span></span><br><span class="line">                .format(records.sizeInBytes, firstOffset, log.sizeInBytes(), largestTimestamp, shallowOffsetOfMaxTimestamp))</span><br><span class="line">        <span class="comment">// 获取物理位置（当前分片的大小）</span></span><br><span class="line">        <span class="keyword">val</span> physicalPosition = log.sizeInBytes()</span><br><span class="line">        <span class="keyword">if</span> (physicalPosition == <span class="number">0</span>) rollingBasedTimestamp = <span class="type">Some</span>(largestTimestamp)</span><br><span class="line"></span><br><span class="line">        require(canConvertToRelativeOffset(largestOffset), <span class="string">"largest offset in message set can not be safely converted to relative offset."</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将消息数据追加到 log 文件</span></span><br><span class="line">        <span class="keyword">val</span> appendedBytes = log.append(records)</span><br><span class="line">        trace(<span class="string">s"Appended <span class="subst">$appendedBytes</span> to <span class="subst">${log.file()}</span> at offset <span class="subst">$firstOffset</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 更新已追加的消息对应的最大时间戳，及其 offset</span></span><br><span class="line">        <span class="keyword">if</span> (largestTimestamp &gt; maxTimestampSoFar) {</span><br><span class="line">            maxTimestampSoFar = largestTimestamp</span><br><span class="line">            offsetOfMaxTimestamp = shallowOffsetOfMaxTimestamp</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果当前累计追加的日志字节数超过阈值（对应 index.interval.bytes 配置）</span></span><br><span class="line">        <span class="keyword">if</span> (bytesSinceLastIndexEntry &gt; indexIntervalBytes) {</span><br><span class="line">            <span class="comment">// 更新 index 和 timeindex 文件</span></span><br><span class="line">            index.append(firstOffset, physicalPosition)</span><br><span class="line">            timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)</span><br><span class="line">            bytesSinceLastIndexEntry = <span class="number">0</span> <span class="comment">// 重置当前累计追加的日志字节数</span></span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 更新累计加入的日志字节数</span></span><br><span class="line">        bytesSinceLastIndexEntry += records.sizeInBytes</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果当前追加的消息数据是有效的，则 LogSegment 会调用 <code>FileRecords#append</code> 方法将消息数据追加到对应的 log 文件中，并更新本地记录的已追加消息的最大时间戳及其 offset。前面我们介绍了 Kafka 并不会对每条消息都建立索引，而是采用稀疏索引的策略间隔指定大小的字节数（对应 <code>index.interval.bytes</code> 配置）建立索引项，如果当前累计追加的消息字节数超过该配置值，则 Kafka 会更新对应的 index 和 timeindex 数据。</p>

        <h4 id="读取日志数据">
          <a href="#读取日志数据" class="heading-link"><i class="fas fa-link"></i></a>读取日志数据</h4>
      <p>下面来看一下 <code>LogSegment#read</code> 方法，该方法用于从 LogSegment 对应的 log 文件中读取指定区间的消息数据，读取的消息内容由 startOffset、maxOffset、maxSize 和 maxPosition 这 4 个参数确定。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, <span class="comment">// 读取消息的起始 offset</span></span><br><span class="line">         maxOffset: <span class="type">Option</span>[<span class="type">Long</span>], <span class="comment">// 读取消息的结束 offset</span></span><br><span class="line">         maxSize: <span class="type">Int</span>, <span class="comment">// 读取消息的最大字节数</span></span><br><span class="line">         maxPosition: <span class="type">Long</span> = size, <span class="comment">// 读取消息的最大物理地址</span></span><br><span class="line">         minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = {</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (maxSize &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Invalid max size for log read (%d)"</span>.format(maxSize))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取当前 log 文件的字节大小</span></span><br><span class="line">    <span class="keyword">val</span> logSize = log.sizeInBytes <span class="comment">// this may change, need to save a consistent copy</span></span><br><span class="line">    <span class="comment">// 获取小于等于 startOffset 的最大 offset 对应的物理地址 position</span></span><br><span class="line">    <span class="keyword">val</span> startOffsetAndSize = <span class="keyword">this</span>.translateOffset(startOffset)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果读取的位置超出了当前文件，直接返回 null</span></span><br><span class="line">    <span class="keyword">if</span> (startOffsetAndSize == <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> startPosition = startOffsetAndSize.position <span class="comment">// 起始 position</span></span><br><span class="line">    <span class="keyword">val</span> offsetMetadata = <span class="keyword">new</span> <span class="type">LogOffsetMetadata</span>(startOffset, baseOffset, startPosition)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 更新读取消息的最大字节数</span></span><br><span class="line">    <span class="keyword">val</span> adjustedMaxSize = <span class="keyword">if</span> (minOneMessage) math.max(maxSize, startOffsetAndSize.size) <span class="keyword">else</span> maxSize</span><br><span class="line">    <span class="comment">// 如果请求读取的消息最大字节数为 0，则返回一个空的结果对象</span></span><br><span class="line">    <span class="keyword">if</span> (adjustedMaxSize == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算待读取的字节数</span></span><br><span class="line">    <span class="keyword">val</span> length = maxOffset <span class="keyword">match</span> {</span><br><span class="line">        <span class="comment">// 如果未指定读取消息的结束位置</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            <span class="comment">// 直接读取到指定的最大物理地址</span></span><br><span class="line">            min((maxPosition - startPosition).toInt, adjustedMaxSize)</span><br><span class="line">        <span class="comment">// 如果指定了读取消息的结束位置</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(offset) =&gt;</span><br><span class="line">            <span class="comment">// 如果结束位置小于起始位置，则直接返回一个空的结果对象</span></span><br><span class="line">            <span class="keyword">if</span> (offset &lt; startOffset)</span><br><span class="line">                <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(offsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">            <span class="comment">// 将结束位置 offset 转换成对应的物理地址</span></span><br><span class="line">            <span class="keyword">val</span> mapping = <span class="keyword">this</span>.translateOffset(offset, startPosition)</span><br><span class="line">            <span class="comment">// 如果结束位置 maxOffset 超出当前日志文件，则使用日志文件长度</span></span><br><span class="line">            <span class="keyword">val</span> endPosition = <span class="keyword">if</span> (mapping == <span class="literal">null</span>) logSize <span class="keyword">else</span> mapping.position</span><br><span class="line">            <span class="comment">// 由 maxOffset、maxPosition，以及 maxSize 共同决定最终读取长度</span></span><br><span class="line">            min(min(maxPosition, endPosition) - startPosition, adjustedMaxSize).toInt</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取对应的消息数据，并封装成 FetchDataInfo 对象返回</span></span><br><span class="line">    <span class="type">FetchDataInfo</span>(</span><br><span class="line">        offsetMetadata,</span><br><span class="line">        log.read(startPosition, length),</span><br><span class="line">        firstEntryIncomplete = adjustedMaxSize &lt; startOffsetAndSize.size)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法的主要逻辑在于确定读取消息的起始位置和读取长度，并最终需要调用 <code>FileRecords#read</code> 方法读取消息数据，该方法接收 2 个参数：position 和 size。参数 position 指代读取消息的起始物理地址，而 size 指代读取消息的字节数，而上述方法的主要逻辑就在于基于参数给定的 4 个坐标来确定 position 和 size 值。</p>
<p>参数 startOffset 设置了当前要读取的消息的起始相对 offset，而 position 是物理地址，所以需要调用 <code>LogSegment#translateOffset</code> 方法进行转换，该方法基于 <strong>二分查找算法</strong> 从 index 文件中获取小于等于 startOffset 的最大 offset 对应的物理地址。实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">translateOffset</span></span>(offset: <span class="type">Long</span>, startingFilePosition: <span class="type">Int</span> = <span class="number">0</span>): <span class="type">LogEntryPosition</span> = {</span><br><span class="line">    <span class="comment">// 基于二分查找获取小于等于参数 offset 的最大 offset，返回 offset 与对应的物理地址</span></span><br><span class="line">    <span class="keyword">val</span> mapping = index.lookup(offset)</span><br><span class="line">    <span class="comment">// 查找对应的物理地址 position</span></span><br><span class="line">    log.searchForOffsetWithSize(offset, max(mapping.position, startingFilePosition))</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>确定好读取的起始物理地址之后，接下来就需要计算读取的消息字节数 size 值，另外 3 个参数（maxOffset、maxSize 和 maxPosition）用来约束生成 size 值，策略如下：</p>
<ol>
<li>如果未指定 maxOffset，则 size 等于 <code>max((maxPosition - startPosition), maxSize)</code>；</li>
<li>如果指定了 maxOffset，需要保证 maxOffset 大于等于 startOffset，然后获取 maxOffset 对应的物理地址，并将该物理地址与 maxPosition 进行比较，选择较小的一个与 startPosition 计算得到对应的 size 值，并保证该 size 值不超过 maxSize。</li>
</ol>
<p>如果能够基于参数计算得到正确的 position 和 size 值，则方法会依据这两个值调用 <code>FileRecords#read</code> 方法读取对应的消息数据，并封装成 FetchDataInfo 对象返回。</p>

        <h4 id="重建索引数据">
          <a href="#重建索引数据" class="heading-link"><i class="fas fa-link"></i></a>重建索引数据</h4>
      <p>最后来看一下 <code>LogSegment#recover</code> 方法，该方法用于对 log 文件重建相应的 index 和 timeindex 文件，并校验 log 中数据的有效性。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recover</span></span>(maxMessageSize: <span class="type">Int</span>): <span class="type">Int</span> = {</span><br><span class="line">    <span class="comment">// 清空 index 和 timeindex 文件</span></span><br><span class="line">    index.truncate()</span><br><span class="line">    index.resize(index.maxIndexSize)</span><br><span class="line">    timeIndex.truncate()</span><br><span class="line">    timeIndex.resize(timeIndex.maxIndexSize)</span><br><span class="line">    <span class="keyword">var</span> validBytes = <span class="number">0</span> <span class="comment">// 记录通过验证的字节数</span></span><br><span class="line">    <span class="keyword">var</span> lastIndexEntry = <span class="number">0</span> <span class="comment">// 最后一个索引项对应的物理地址</span></span><br><span class="line">    maxTimestampSoFar = <span class="type">Record</span>.<span class="type">NO_TIMESTAMP</span></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 遍历 log 文件，重建索引</span></span><br><span class="line">        <span class="keyword">for</span> (entry &lt;- log.shallowEntries(maxMessageSize).asScala) {</span><br><span class="line">            <span class="comment">// 获取对应的消息 Record 对象</span></span><br><span class="line">            <span class="keyword">val</span> record = entry.record</span><br><span class="line">            <span class="comment">// 校验消息数据的有效性，如果存在问题则抛出异常</span></span><br><span class="line">            record.ensureValid()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 更新本地记录的消息最大时间戳及其 offset 值</span></span><br><span class="line">            <span class="keyword">if</span> (record.timestamp &gt; maxTimestampSoFar) {</span><br><span class="line">                maxTimestampSoFar = record.timestamp</span><br><span class="line">                offsetOfMaxTimestamp = entry.offset</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果当前字节减去上一次记录索引的字节超过设置的索引项之间间隔的最小字节数，则添加索引项</span></span><br><span class="line">            <span class="keyword">if</span> (validBytes - lastIndexEntry &gt; indexIntervalBytes) {</span><br><span class="line">                <span class="keyword">val</span> startOffset = entry.firstOffset</span><br><span class="line">                index.append(startOffset, validBytes)</span><br><span class="line">                timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)</span><br><span class="line">                lastIndexEntry = validBytes</span><br><span class="line">            }</span><br><span class="line">            validBytes += entry.sizeInBytes()</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">CorruptRecordException</span> =&gt;</span><br><span class="line">            logger.warn(<span class="string">"Found invalid messages in log segment %s at byte offset %d: %s."</span>.format(log.file.getAbsolutePath, validBytes, e.getMessage))</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 截断日志和索引文件中无效的字节</span></span><br><span class="line">    <span class="keyword">val</span> truncated = log.sizeInBytes - validBytes</span><br><span class="line">    log.truncateTo(validBytes)</span><br><span class="line">    index.trimToValidSize()</span><br><span class="line">    <span class="comment">// A normally closed segment always appends the biggest timestamp ever seen into log segment, we do this as well.</span></span><br><span class="line">    timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp, skipFullCheck = <span class="literal">true</span>)</span><br><span class="line">    timeIndex.trimToValidSize()</span><br><span class="line">    truncated</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>重建的过程实际上就是遍历 log 文件，并依据设置的索引项最小间隔字节数（对应 <code>index.interval.bytes</code> 配置）区间建立稀疏索引，期间会基于 <code>Record#ensureValid</code> 方法采用 CRC 校验消息数据的有效性，如果存在无效的数据，则退出循环并移除之后的日志和索引。</p>

        <h3 id="Log-组件">
          <a href="#Log-组件" class="heading-link"><i class="fas fa-link"></i></a>Log 组件</h3>
      <p>在一个 log 目录下存在多个以“topic-partition”命名的分区目录，每个 topic 分区对应一个 Log 对象（更准确来说是一个分区副本对应一个 Log 对象），用于管理名下的 LogSegment 对象集合， <strong>Log 类使用 SkipList 数据结构对 LogSegment 进行组织和管理</strong> 。在 SkipList 中以 LogSegment 的 baseOffset 为 key，以 LogSegment 对象自身作为 value。当读取消息数据时，我们可以基于 offset 快速定位到对应的 LogSegment 对象，然后调用 <code>LogSegment#read</code> 方法读取消息数据。当写入消息时，Kafka 并不允许向 SkipList 中的任意一个 LogSegment 对象追加数据，而只允许往 SkipList 中的最后一个 LogSegment 追加数据，Log 类提供了 <code>Log#activeSegment</code> 用于获取该 LogSegment 对象，称之为 activeSegment。</p>
<p>Log 类的字段定义如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Log</span>(<span class="params">@volatile var dir: <span class="type">File</span>, // 当前 <span class="type">Log</span> 对象对应的 topic 分区目录</span></span></span><br><span class="line"><span class="class"><span class="params">          @volatile var config: <span class="type">LogConfig</span>, // 配置信息</span></span></span><br><span class="line"><span class="class"><span class="params">          @volatile var recoveryPoint: <span class="type">Long</span> = 0L, // 恢复操作的起始 offset，即 <span class="type">HW</span> 位置，之前的消息已经全部落盘</span></span></span><br><span class="line"><span class="class"><span class="params">          scheduler: <span class="type">Scheduler</span>, // 定时任务调度器</span></span></span><br><span class="line"><span class="class"><span class="params">          time: <span class="type">Time</span> = <span class="type">Time</span>.<span class="type">SYSTEM</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">KafkaMetricsGroup</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 最近一次执行 flush 操作的时间 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> lastflushedTime = <span class="keyword">new</span> <span class="type">AtomicLong</span>(time.milliseconds)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 用于记录分配给当前消息的 offset，也是当前副本的 LEO 值:</span></span><br><span class="line"><span class="comment">     * - messageOffset 记录了当前 Log 对象下一条待追加消息的 offset 值</span></span><br><span class="line"><span class="comment">     * - segmentBaseOffset 记录了 activeSegment 对象的 baseOffset</span></span><br><span class="line"><span class="comment">     * - relativePositionInSegment 记录了 activeSegment 对象的大小</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> nextOffsetMetadata: <span class="type">LogOffsetMetadata</span> = _</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 当前 Log 包含的 LogSegment 集合，SkipList 结构：</span></span><br><span class="line"><span class="comment">     * - 以 baseOffset 作为 key</span></span><br><span class="line"><span class="comment">     * - 以 LogSegment 对象作为 value</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> segments: <span class="type">ConcurrentNavigableMap</span>[java.lang.<span class="type">Long</span>, <span class="type">LogSegment</span>] = <span class="keyword">new</span> <span class="type">ConcurrentSkipListMap</span>[java.lang.<span class="type">Long</span>, <span class="type">LogSegment</span>]</span><br><span class="line">    <span class="comment">/** 基于 topic 分区目录解析得到对应的 topic 分区对象 */</span></span><br><span class="line">    <span class="keyword">val</span> topicPartition: <span class="type">TopicPartition</span> = <span class="type">Log</span>.parseTopicPartitionName(dir)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> tags = <span class="type">Map</span>(<span class="string">"topic"</span> -&gt; topicPartition.topic, <span class="string">"partition"</span> -&gt; topicPartition.partition.toString)</span><br><span class="line">    <span class="comment">/** 当前 Log 对象对应的分区目录名称 */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = dir.getName</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>

        <h4 id="初始化加载日志数据">
          <a href="#初始化加载日志数据" class="heading-link"><i class="fas fa-link"></i></a>初始化加载日志数据</h4>
      <p>Log 类在实例化时会调用 <code>Log#loadSegments</code> 方法加载对应 topic 分区目录下的 log、index 和 timeindex 文件。该方法主要做了以下 4 件事情：</p>
<ol>
<li>删除标记为 deleted 或 cleaned 的文件，将标记为 swap 的文件加入到交换集合中，等待后续继续完成交换过程；</li>
<li>加载 topic 分区目录下全部的 log 文件和 index 文件，如果对应的 index 不存在或数据不完整，则重建；</li>
<li>遍历处理 1 中记录的 swap 文件，使用压缩后的 LogSegment 替换压缩前的 LogSegment 集合，并删除压缩前的日志和索引文件；</li>
<li>后处理，如果对应 SkipList 为空则新建一个空的 activeSegment，如果不为空则校验 recoveryPoint 之后数据的完整性。</li>
</ol>
<p>方法 <code>Log#loadSegments</code> 的实现比较冗长，下面我们分步骤逐一分析各个过程，首先来看 <strong>步骤 1</strong> ，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 删除标记为 deleted 或 cleaned 的文件，将标记为 swap 的文件加入到交换集合中，等待后续继续完成交换过程</span></span><br><span class="line"><span class="keyword">for</span> (file &lt;- dir.listFiles <span class="keyword">if</span> file.isFile) {</span><br><span class="line">    <span class="keyword">if</span> (!file.canRead) <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"Could not read file "</span> + file)</span><br><span class="line">    <span class="keyword">val</span> filename = file.getName</span><br><span class="line">    <span class="comment">// 如果是标记为 deleted 或 cleaned 的文件，则删除：</span></span><br><span class="line">    <span class="comment">// - 其中 deleted 文件是指标识需要被删除的 log 文件或 index 文件</span></span><br><span class="line">    <span class="comment">// - 其中 cleaned 文件是指在执行日志压缩过程中宕机，文件中的数据状态不明确，无法正确恢复的文件</span></span><br><span class="line">    <span class="keyword">if</span> (filename.endsWith(<span class="type">DeletedFileSuffix</span>) || filename.endsWith(<span class="type">CleanedFileSuffix</span>)) {</span><br><span class="line">        file.delete()</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 如果是标记为 swap 的文件（可用于交换的临时文件），则说明日志压缩过程已完成，但是在执行交换过程中宕机，</span></span><br><span class="line">    <span class="comment">// 因为 swap 文件已经保存了日志压缩后的完整数据，可以进行恢复：</span></span><br><span class="line">    <span class="comment">// 1. 如果 swap 文件是 log 文件，则删除对应的 index 文件，稍后 swap 操作会重建索引</span></span><br><span class="line">    <span class="comment">// 2. 如果 swap 文件是 index 文件，则直接删除，后续加载 log 文件时会重建索引</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (filename.endsWith(<span class="type">SwapFileSuffix</span>)) {</span><br><span class="line">        <span class="comment">// 移除 swap 后缀</span></span><br><span class="line">        <span class="keyword">val</span> baseName = <span class="keyword">new</span> <span class="type">File</span>(<span class="type">CoreUtils</span>.replaceSuffix(file.getPath, <span class="type">SwapFileSuffix</span>, <span class="string">""</span>))</span><br><span class="line">        <span class="comment">// 如果是 index 文件，则直接删除，因为后续可以重建</span></span><br><span class="line">        <span class="keyword">if</span> (baseName.getPath.endsWith(<span class="type">IndexFileSuffix</span>)) {</span><br><span class="line">            file.delete()</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 如果是 log 文件，则删除对应的 index 文件</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (baseName.getPath.endsWith(<span class="type">LogFileSuffix</span>)) {</span><br><span class="line">            <span class="keyword">val</span> index = <span class="keyword">new</span> <span class="type">File</span>(<span class="type">CoreUtils</span>.replaceSuffix(baseName.getPath, <span class="type">LogFileSuffix</span>, <span class="type">IndexFileSuffix</span>))</span><br><span class="line">            index.delete()</span><br><span class="line">            swapFiles += file <span class="comment">// 将当前文件加入到 swap 集合中</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>这一步会遍历当前 topic 分区目录下的文件，并处理标记为 deleted、cleaned 和 swap 的文件（以这些名称作为文件后缀名）。这 3 类文件对应的含义为：</p>
<ul>
<li><strong>deleted 文件</strong> ：标识需要被删除的 log 文件和 index 文件。</li>
<li><strong>cleaned 文件</strong> ：在执行日志压缩过程中宕机，文件中的数据状态不明确，无法正确恢复的文件。</li>
<li><strong>swap 文件</strong> ：完成执行日志压缩后的文件，但是在替换原文件时宕机。</li>
</ul>
<p>针对 deleted 和 cleaned 文件直接删除即可，对于 swap 文件来说，因为其中的数据是完整的，所以可以继续使用，只需再次完成 swap 操作即可。Kafka 针对 swap 文件的处理策略为：</p>
<ol>
<li>如果 swap 文件是 log 文件，则删除对应的 index 文件，稍后的 swap 操作会重建索引。</li>
<li>如果 swap 文件是 index 文件，则直接删除，后续加载 log 文件时会重建索引。</li>
</ol>
<p>完成了对于一些异常状态文件的处理，<strong>步骤 2</strong> 开始真正执行加载 log 和 index 文件的操作，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2. 加载 topic 分区目录下全部的 log 文件和 index 文件，如果对应的 index 文件不存在或数据不完整，则重建</span></span><br><span class="line"><span class="keyword">for</span> (file &lt;- dir.listFiles <span class="keyword">if</span> file.isFile) {</span><br><span class="line">    <span class="keyword">val</span> filename = file.getName</span><br><span class="line">    <span class="comment">// 处理 index 和 timeindex 文件</span></span><br><span class="line">    <span class="keyword">if</span> (filename.endsWith(<span class="type">IndexFileSuffix</span>) || filename.endsWith(<span class="type">TimeIndexFileSuffix</span>)) {</span><br><span class="line">        <span class="comment">// 如果索引文件没有对应的 log 文件，则删除 index 文件</span></span><br><span class="line">        <span class="keyword">val</span> logFile =</span><br><span class="line">            <span class="keyword">if</span> (filename.endsWith(<span class="type">TimeIndexFileSuffix</span>))</span><br><span class="line">                <span class="keyword">new</span> <span class="type">File</span>(file.getAbsolutePath.replace(<span class="type">TimeIndexFileSuffix</span>, <span class="type">LogFileSuffix</span>))</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">new</span> <span class="type">File</span>(file.getAbsolutePath.replace(<span class="type">IndexFileSuffix</span>, <span class="type">LogFileSuffix</span>))</span><br><span class="line">        <span class="keyword">if</span> (!logFile.exists) {</span><br><span class="line">            warn(<span class="string">"Found an orphaned index file, %s, with no corresponding log file."</span>.format(file.getAbsolutePath))</span><br><span class="line">            file.delete()</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 处理 log 文件</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (filename.endsWith(<span class="type">LogFileSuffix</span>)) {</span><br><span class="line">        <span class="comment">// 获取 baseOffset 值</span></span><br><span class="line">        <span class="keyword">val</span> start = filename.substring(<span class="number">0</span>, filename.length - <span class="type">LogFileSuffix</span>.length).toLong</span><br><span class="line">        <span class="comment">// 创建对应的 index 文件对象</span></span><br><span class="line">        <span class="keyword">val</span> indexFile = <span class="type">Log</span>.indexFilename(dir, start)</span><br><span class="line">        <span class="comment">// 创建对应的 timeindex 文件对象</span></span><br><span class="line">        <span class="keyword">val</span> timeIndexFile = <span class="type">Log</span>.timeIndexFilename(dir, start)</span><br><span class="line">        <span class="keyword">val</span> indexFileExists = indexFile.exists()</span><br><span class="line">        <span class="keyword">val</span> timeIndexFileExists = timeIndexFile.exists()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建对应的 LogSegment 对象</span></span><br><span class="line">        <span class="keyword">val</span> segment = <span class="keyword">new</span> <span class="type">LogSegment</span>(</span><br><span class="line">            dir = dir,</span><br><span class="line">            startOffset = start,</span><br><span class="line">            indexIntervalBytes = config.indexInterval,</span><br><span class="line">            maxIndexSize = config.maxIndexSize,</span><br><span class="line">            rollJitterMs = config.randomSegmentJitter,</span><br><span class="line">            time = time,</span><br><span class="line">            fileAlreadyExists = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果对应的 index 文件存在，则校验数据完整性，如果不完整则重建</span></span><br><span class="line">        <span class="keyword">if</span> (indexFileExists) {</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                <span class="comment">// 校验 index 文件的完整性</span></span><br><span class="line">                segment.index.sanityCheck()</span><br><span class="line">                <span class="comment">// 如果对应的 timeindex 文件不存在，则重置对应的 mmb 对象</span></span><br><span class="line">                <span class="keyword">if</span> (!timeIndexFileExists)</span><br><span class="line">                    segment.timeIndex.resize(<span class="number">0</span>)</span><br><span class="line">                <span class="comment">// 校验 timeindex 文件的完整性</span></span><br><span class="line">                segment.timeIndex.sanityCheck()</span><br><span class="line">            } <span class="keyword">catch</span> {</span><br><span class="line">                <span class="comment">// 索引文件完整性异常，删除重建</span></span><br><span class="line">                <span class="keyword">case</span> e: java.lang.<span class="type">IllegalArgumentException</span> =&gt;</span><br><span class="line">                    warn(<span class="string">s"Found a corrupted index file due to <span class="subst">${e.getMessage}</span>}. deleting <span class="subst">${timeIndexFile.getAbsolutePath}</span>, "</span> + <span class="string">s"<span class="subst">${indexFile.getAbsolutePath}</span> and rebuilding index..."</span>)</span><br><span class="line">                    indexFile.delete()</span><br><span class="line">                    timeIndexFile.delete()</span><br><span class="line">                    segment.recover(config.maxMessageSize)</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 如果对应的 index 文件不存在，则重建</span></span><br><span class="line">        <span class="keyword">else</span> {</span><br><span class="line">            error(<span class="string">"Could not find index file corresponding to log file %s, rebuilding index..."</span>.format(segment.log.file.getAbsolutePath))</span><br><span class="line">            segment.recover(config.maxMessageSize)</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 记录 LogSegment 对象到 segments 集合中</span></span><br><span class="line">        segments.put(start, segment)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果当前文件是 index 文件，但对应的 log 文件不存在，则直接删除，因为没有继续保留的意义。如果当前是 log 文件，则这一步会创建 log 文件对应的 LogSegment 对象并记录到 SkipList 中。期间会校验 log 文件对应的 index 和 timeindex 文件，如果索引文件不存在或其中的数据不完整，则会调用前面介绍的 <code>LogSegment#recover</code> 方法重建索引。</p>
<p>步骤 1 中将需要继续执行 swap 操作的文件记录到了 swapFiles 集合中， <strong>步骤 3</strong> 的逻辑就是继续完成 swap 操作，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3. 遍历处理步骤 1 中记录的 swap 文件，使用压缩后的 LogSegment 替换压缩前的 LogSegment 集合，并删除压缩前的日志和索引文件</span></span><br><span class="line"><span class="keyword">for</span> (swapFile &lt;- swapFiles) {</span><br><span class="line">    <span class="comment">// 移除 “.swap” 后缀</span></span><br><span class="line">    <span class="keyword">val</span> logFile = <span class="keyword">new</span> <span class="type">File</span>(<span class="type">CoreUtils</span>.replaceSuffix(swapFile.getPath, <span class="type">SwapFileSuffix</span>, <span class="string">""</span>))</span><br><span class="line">    <span class="keyword">val</span> fileName = logFile.getName</span><br><span class="line">    <span class="comment">// 基于 log 文件名得到对应的 baseOffset 值</span></span><br><span class="line">    <span class="keyword">val</span> startOffset = fileName.substring(<span class="number">0</span>, fileName.length - <span class="type">LogFileSuffix</span>.length).toLong</span><br><span class="line">    <span class="keyword">val</span> indexFile = <span class="keyword">new</span> <span class="type">File</span>(<span class="type">CoreUtils</span>.replaceSuffix(logFile.getPath, <span class="type">LogFileSuffix</span>, <span class="type">IndexFileSuffix</span>) + <span class="type">SwapFileSuffix</span>) <span class="comment">// .index.swap</span></span><br><span class="line">    <span class="keyword">val</span> index = <span class="keyword">new</span> <span class="type">OffsetIndex</span>(indexFile, baseOffset = startOffset, maxIndexSize = config.maxIndexSize)</span><br><span class="line">    <span class="keyword">val</span> timeIndexFile = <span class="keyword">new</span> <span class="type">File</span>(<span class="type">CoreUtils</span>.replaceSuffix(logFile.getPath, <span class="type">LogFileSuffix</span>, <span class="type">TimeIndexFileSuffix</span>) + <span class="type">SwapFileSuffix</span>) <span class="comment">// .timeindex.swap</span></span><br><span class="line">    <span class="keyword">val</span> timeIndex = <span class="keyword">new</span> <span class="type">TimeIndex</span>(timeIndexFile, baseOffset = startOffset, maxIndexSize = config.maxIndexSize)</span><br><span class="line">    <span class="comment">// 创建对应的 LogSegment 对象</span></span><br><span class="line">    <span class="keyword">val</span> swapSegment = <span class="keyword">new</span> <span class="type">LogSegment</span>(<span class="type">FileRecords</span>.open(swapFile),</span><br><span class="line">        index = index,</span><br><span class="line">        timeIndex = timeIndex,</span><br><span class="line">        baseOffset = startOffset,</span><br><span class="line">        indexIntervalBytes = config.indexInterval,</span><br><span class="line">        rollJitterMs = config.randomSegmentJitter,</span><br><span class="line">        time = time)</span><br><span class="line">    info(<span class="string">"Found log file %s from interrupted swap operation, repairing."</span>.format(swapFile.getPath))</span><br><span class="line">    <span class="comment">// 依据 log 文件重建索引文件，同时校验 log 文件中消息的合法性</span></span><br><span class="line">    swapSegment.recover(config.maxMessageSize)</span><br><span class="line">    <span class="comment">// 查找 swapSegment 获取 [baseOffset, nextOffset] 区间对应的日志压缩前的 LogSegment 集合，</span></span><br><span class="line">    <span class="comment">// 区间中的 LogSegment 数据都压缩到了 swapSegment 中</span></span><br><span class="line">    <span class="keyword">val</span> oldSegments = <span class="keyword">this</span>.logSegments(swapSegment.baseOffset, swapSegment.nextOffset())</span><br><span class="line">    <span class="comment">// 将 swapSegment 对象加入到 segments 中，并将 oldSegments 中所有的 LogSegment 对象从 segments 中删除，</span></span><br><span class="line">    <span class="comment">// 同时删除对应的日志文件和索引文件，最后移除文件的 ".swap" 后缀</span></span><br><span class="line">    <span class="keyword">this</span>.replaceSegments(swapSegment, oldSegments.toSeq, isRecoveredSwapFile = <span class="literal">true</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>在完成对日志数据的压缩操作后，会将压缩的结果先保存为 swap 文件（以“.swap”作为文件后缀），并最终替换压缩前的日志文件，所以 swap 文件中的数据都是完整，只需要移除对应的“.swap”后缀，并构建对应的 LogSegment 对象即可。但是这里不能简单的将对应的 LogSegment 对象记录到 SkipList 中就万事大吉了，因为 SkipList 中还存在着压缩前的原文件对应的 LogSegment 对象集合，所以需要先将这些 LogSegment 对象集合及其对应的 log 文件和索引文件删除，这也是 <code>Log#replaceSegments</code> 方法的主要逻辑。</p>
<p>完成了前 3 步的工作， <strong>步骤 4</strong> 会对前面加载的数据进行校验，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 4. 后处理，如果对应 SkipList 为空，则新建一个空的 activeSegment，如果不为空则校验 HW 之后数据的完整性</span></span><br><span class="line"><span class="keyword">if</span> (logSegments.isEmpty) {</span><br><span class="line">    <span class="comment">// 如果 SkipList 为空，则需要创建一个 activeSegment，保证 SkipList 能够正常操作</span></span><br><span class="line">    segments.put(<span class="number">0</span>L, <span class="keyword">new</span> <span class="type">LogSegment</span>(dir = dir,</span><br><span class="line">        startOffset = <span class="number">0</span>,</span><br><span class="line">        indexIntervalBytes = config.indexInterval,</span><br><span class="line">        maxIndexSize = config.maxIndexSize,</span><br><span class="line">        rollJitterMs = config.randomSegmentJitter,</span><br><span class="line">        time = time,</span><br><span class="line">        fileAlreadyExists = <span class="literal">false</span>,</span><br><span class="line">        initFileSize = <span class="keyword">this</span>.initFileSize(),</span><br><span class="line">        preallocate = config.preallocate))</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">    <span class="comment">// 如果 SkipList 不为空，则需要对其中的数据进行验证</span></span><br><span class="line">    <span class="keyword">if</span> (!dir.getAbsolutePath.endsWith(<span class="type">Log</span>.<span class="type">DeleteDirSuffix</span>)) {</span><br><span class="line">        <span class="comment">// 处理 broker 节点异常关闭导致的数据异常，需要验证 [recoveryPoint, activeSegment] 中的所有消息，并移除验证失败的消息</span></span><br><span class="line">        <span class="keyword">this</span>.recoverLog()</span><br><span class="line">        <span class="comment">// reset the index size of the currently active log segment to allow more entries</span></span><br><span class="line">        activeSegment.index.resize(config.maxIndexSize)</span><br><span class="line">        activeSegment.timeIndex.resize(config.maxIndexSize)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果前面的步骤中并未加载到任何数据，则对应的 SkipList 是空的，为了保证 SkipList 能够正常工作，需要为其添加一个空的 activeSegment 对象。如果 SkipList 不为空则需要依据 log 目录下是否存在“.kafka_cleanshutdown”文件来判定之前 broker 是否是正常关闭的，如果为非正常关闭则需要对 recoveryPoint 之后的数据进行校验，如果数据存在不完整则进行丢弃，相关实现位于 <code>Log#recoverLog</code> 中，比较简单，不再展开。</p>

        <h4 id="追加日志数据-1">
          <a href="#追加日志数据-1" class="heading-link"><i class="fas fa-link"></i></a>追加日志数据</h4>
      <p>Log 类定义了 <code>Log#append</code> 方法，用于往 Log 对象中追加消息数据。需要注意的一点是，Log 对象使用 SkipList 管理多个 LogSegment，我们在执行追加消息时是不能够往 SkipList 中的任意 LogSegment 对象执行追加操作的，Kafka 设计仅允许往 activeSegment 对象中追加消息。方法 <code>Log#append</code> 实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append</span></span>(records: <span class="type">MemoryRecords</span>, assignOffsets: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">LogAppendInfo</span> = {</span><br><span class="line">    <span class="comment">// 1. 解析、校验待追加的消息数据，封装成 LogAppendInfo 对象</span></span><br><span class="line">    <span class="keyword">val</span> appendInfo = <span class="keyword">this</span>.analyzeAndValidateRecords(records)</span><br><span class="line">    <span class="comment">// 如果消息数据个数为 0，则直接返回</span></span><br><span class="line">    <span class="keyword">if</span> (appendInfo.shallowCount == <span class="number">0</span>) <span class="keyword">return</span> appendInfo</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 剔除待追加消息中未通过验证的字节部分</span></span><br><span class="line">    <span class="keyword">var</span> validRecords = <span class="keyword">this</span>.trimInvalidBytes(records, appendInfo)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 将待追加消息中剩余有效的字节追加到 Log 对象中</span></span><br><span class="line">        lock synchronized {</span><br><span class="line">            <span class="comment">// 3.1 如果指定需要分配 offset</span></span><br><span class="line">            <span class="keyword">if</span> (assignOffsets) {</span><br><span class="line">                <span class="comment">// 获取当前 Log 对象对应的最后一个 offset 值，以此开始向后分配 offset</span></span><br><span class="line">                <span class="keyword">val</span> offset = <span class="keyword">new</span> <span class="type">LongRef</span>(nextOffsetMetadata.messageOffset)</span><br><span class="line">                <span class="comment">// 更新待追加消息的 firstOffset 为 Log 对象最后一个 offset 值</span></span><br><span class="line">                appendInfo.firstOffset = offset.value</span><br><span class="line">                <span class="keyword">val</span> now = time.milliseconds</span><br><span class="line">                <span class="keyword">val</span> validateAndOffsetAssignResult = <span class="keyword">try</span> {</span><br><span class="line">                    <span class="comment">// 对消息（包括压缩后的）的 magic 值进行统一，验证数据完整性，并分配 offset，同时按要求更新消息的时间戳</span></span><br><span class="line">                    <span class="type">LogValidator</span>.validateMessagesAndAssignOffsets(</span><br><span class="line">                        validRecords,</span><br><span class="line">                        offset,</span><br><span class="line">                        now,</span><br><span class="line">                        appendInfo.sourceCodec,</span><br><span class="line">                        appendInfo.targetCodec,</span><br><span class="line">                        config.compact,</span><br><span class="line">                        config.messageFormatVersion.messageFormatVersion,</span><br><span class="line">                        config.messageTimestampType,</span><br><span class="line">                        config.messageTimestampDifferenceMaxMs)</span><br><span class="line">                } <span class="keyword">catch</span> {</span><br><span class="line">                    <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Error in validating messages while appending to log '%s'"</span>.format(name), e)</span><br><span class="line">                }</span><br><span class="line">                validRecords = validateAndOffsetAssignResult.validatedRecords</span><br><span class="line">                appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp</span><br><span class="line">                appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp</span><br><span class="line">                <span class="comment">// 更新待追加消息的 lastOffset 值</span></span><br><span class="line">                appendInfo.lastOffset = offset.value - <span class="number">1</span></span><br><span class="line">                <span class="comment">// 如果时间戳类型为 LOG_APPEND_TIME，则修改时间戳</span></span><br><span class="line">                <span class="keyword">if</span> (config.messageTimestampType == <span class="type">TimestampType</span>.<span class="type">LOG_APPEND_TIME</span>)</span><br><span class="line">                    appendInfo.logAppendTime = now</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 如果在执行 validateMessagesAndAssignOffsets 操作时修改了消息的长度，则需要重新验证，防止消息过长</span></span><br><span class="line">                <span class="keyword">if</span> (validateAndOffsetAssignResult.messageSizeMaybeChanged) {</span><br><span class="line">                    <span class="keyword">for</span> (logEntry &lt;- validRecords.shallowEntries.asScala) {</span><br><span class="line">                        <span class="keyword">if</span> (logEntry.sizeInBytes &gt; config.maxMessageSize) {</span><br><span class="line">                            <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)</span><br><span class="line">                            <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)</span><br><span class="line">                            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordTooLargeException</span>(</span><br><span class="line">                                <span class="string">"Message size is %d bytes which exceeds the maximum configured message size of %s."</span>.format(logEntry.sizeInBytes, config.maxMessageSize))</span><br><span class="line">                        }</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// 3.2 不需要分配 offset</span></span><br><span class="line">            <span class="keyword">else</span> {</span><br><span class="line">                <span class="comment">// 如果消息的 offset 不是单调递增，或者消息的 firstOffset 小于 Log 中记录的下一条消息 offset，则说明 appendInfo 非法</span></span><br><span class="line">                <span class="keyword">if</span> (!appendInfo.offsetsMonotonic || appendInfo.firstOffset &lt; nextOffsetMetadata.messageOffset)</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">"Out of order offsets found in "</span> + records.deepEntries.asScala.map(_.offset))</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 4. 校验待追加消息的长度，保证不超过了单个 LogSegment 所允许的最大长度（对应 segment.bytes 配置）</span></span><br><span class="line">            <span class="keyword">if</span> (validRecords.sizeInBytes &gt; config.segmentSize) {</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordBatchTooLargeException</span>(</span><br><span class="line">                    <span class="string">"Message set size is %d bytes which exceeds the maximum configured segment size of %s."</span>.format(validRecords.sizeInBytes, config.segmentSize))</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 5. 获取 activeSegment 对象，如果需要则创建新的 activeSegment 对象</span></span><br><span class="line">            <span class="keyword">val</span> segment = <span class="keyword">this</span>.maybeRoll(</span><br><span class="line">                messagesSize = validRecords.sizeInBytes,</span><br><span class="line">                maxTimestampInMessages = appendInfo.maxTimestamp,</span><br><span class="line">                maxOffsetInMessages = appendInfo.lastOffset)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 6. 往 activeSegment 中追加消息</span></span><br><span class="line">            segment.append(</span><br><span class="line">                firstOffset = appendInfo.firstOffset,</span><br><span class="line">                largestOffset = appendInfo.lastOffset,</span><br><span class="line">                largestTimestamp = appendInfo.maxTimestamp,</span><br><span class="line">                shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,</span><br><span class="line">                records = validRecords)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 7. 更新 LEO 中记录的当前 Log 最后一个 offset 值</span></span><br><span class="line">            <span class="keyword">this</span>.updateLogEndOffset(appendInfo.lastOffset + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            trace(<span class="string">"Appended message set to log %s with first offset: %d, next offset: %d, and messages: %s"</span></span><br><span class="line">                    .format(<span class="keyword">this</span>.name, appendInfo.firstOffset, nextOffsetMetadata.messageOffset, validRecords))</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 8. 如果刷盘时间间隔达到阈值（对应 flush.messages 配置），则执行刷盘</span></span><br><span class="line">            <span class="keyword">if</span> (unflushedMessages &gt;= config.flushInterval)</span><br><span class="line">                <span class="keyword">this</span>.flush() <span class="comment">// 将 [recoveryPoint, logEndOffset) 之间的数据刷盘</span></span><br><span class="line"></span><br><span class="line">            appendInfo</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">IOException</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaStorageException</span>(<span class="string">"I/O exception in append to log '%s'"</span>.format(name), e)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>追加消息数据操作的整体执行流程可以概括为：</p>
<ol>
<li>解析并校验待追加的消息集合，将其封装成 LogAppendInfo 对象；</li>
<li>剔除待追加消息集合中未通过验证的字节部分；</li>
<li>如果指定需要为消息分配 offset，则对消息（包括压缩后的）执行分配 offset 操作，并对消息执行 magic 值统一、数据完整性校验，以及按需更新消息时间戳等操作；</li>
<li>如果指定不需要为消息分配 offset，则需要保证消息已有 offset 是单调递增，且起始 offset 不能小于当前 Log 对象中记录的下一条待追加消息的 offset；</li>
<li>校验处理后消息集合的总长度，保证不超过单个 LogSegment 对象所允许的最大长度；</li>
<li>获取目标 activeSegment 对象，如果需要则创建一个新的 activeSegment 对象并返回；</li>
<li>往目标 activeSegment 对象中追加消息数据，并更新当前 Log 对象中记录的下一条待追加消息的 offset 值；</li>
<li>如果当前时间距离上次执行刷盘操作的时间超过配置的时间间隔，则执行刷盘操作。</li>
</ol>
<p>下面我们分步骤对整个执行过程进行进一步分析，首先来看 <strong>步骤 1</strong> ，实现位于 <code>Log#analyzeAndValidateRecords</code> 方法中，该方法对待追加的消息集合中的消息逐条进行解析和验证，并封装成 LogAppendInfo 对象返回。实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">analyzeAndValidateRecords</span></span>(records: <span class="type">MemoryRecords</span>): <span class="type">LogAppendInfo</span> = {</span><br><span class="line">    <span class="keyword">var</span> shallowMessageCount = <span class="number">0</span> <span class="comment">// 消息条数</span></span><br><span class="line">    <span class="keyword">var</span> validBytesCount = <span class="number">0</span> <span class="comment">// 通过验证的消息字节数</span></span><br><span class="line">    <span class="keyword">var</span> firstOffset = <span class="number">-1</span>L <span class="comment">// 第一条消息的 offset</span></span><br><span class="line">    <span class="keyword">var</span> lastOffset = <span class="number">-1</span>L <span class="comment">// 最后一条消息的 offset</span></span><br><span class="line">    <span class="keyword">var</span> sourceCodec: <span class="type">CompressionCodec</span> = <span class="type">NoCompressionCodec</span> <span class="comment">// 生产者使用的压缩方式</span></span><br><span class="line">    <span class="keyword">var</span> monotonic = <span class="literal">true</span> <span class="comment">// 标识生产者为消息分配的内部 offset 是否是单调递增的</span></span><br><span class="line">    <span class="keyword">var</span> maxTimestamp = <span class="type">Record</span>.<span class="type">NO_TIMESTAMP</span> <span class="comment">// 消息的最大时间戳</span></span><br><span class="line">    <span class="keyword">var</span> offsetOfMaxTimestamp = <span class="number">-1</span>L <span class="comment">// 最大时间戳消息对应的 offset</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 基于浅层迭代器迭代，对于压缩的消息不会解压缩</span></span><br><span class="line">    <span class="keyword">for</span> (entry &lt;- records.shallowEntries.asScala) {</span><br><span class="line">        <span class="comment">// 记录第一条消息的 offset</span></span><br><span class="line">        <span class="keyword">if</span> (firstOffset &lt; <span class="number">0</span>) firstOffset = entry.offset</span><br><span class="line">        <span class="comment">// 如果是单调递增的话，则在遍历过程中 lastOffset 应该始终小于当前的 offset</span></span><br><span class="line">        <span class="keyword">if</span> (lastOffset &gt;= entry.offset) monotonic = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 记录最后一条消息的 offset</span></span><br><span class="line">        lastOffset = entry.offset</span><br><span class="line">        <span class="comment">// 获取消息数据</span></span><br><span class="line">        <span class="keyword">val</span> record = entry.record</span><br><span class="line">        <span class="comment">// 如果待追加的消息长度大于允许的最大值（对应 max.message.bytes 配置），则抛出异常</span></span><br><span class="line">        <span class="keyword">val</span> messageSize = entry.sizeInBytes</span><br><span class="line">        <span class="keyword">if</span> (messageSize &gt; config.maxMessageSize) {</span><br><span class="line">            <span class="type">BrokerTopicStats</span>.getBrokerTopicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)</span><br><span class="line">            <span class="type">BrokerTopicStats</span>.getBrokerAllTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RecordTooLargeException</span>(<span class="string">"Message size is %d bytes which exceeds the maximum configured message size of %s."</span>.format(messageSize, config.maxMessageSize))</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// CRC 校验</span></span><br><span class="line">        record.ensureValid()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 记录当前消息集合中时间戳最大的消息，及其 offset</span></span><br><span class="line">        <span class="keyword">if</span> (record.timestamp &gt; maxTimestamp) {</span><br><span class="line">            maxTimestamp = record.timestamp</span><br><span class="line">            offsetOfMaxTimestamp = lastOffset</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 浅层消息数加 1</span></span><br><span class="line">        shallowMessageCount += <span class="number">1</span></span><br><span class="line">        <span class="comment">// 更新已验证的字节数</span></span><br><span class="line">        validBytesCount += messageSize</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 解析生产者使用的压缩方式</span></span><br><span class="line">        <span class="keyword">val</span> messageCodec = <span class="type">CompressionCodec</span>.getCompressionCodec(record.compressionType.id)</span><br><span class="line">        <span class="keyword">if</span> (messageCodec != <span class="type">NoCompressionCodec</span>) sourceCodec = messageCodec</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 解析服务端使用的压缩方式（对应 compression.type 配置）</span></span><br><span class="line">    <span class="keyword">val</span> targetCodec = <span class="type">BrokerCompressionCodec</span>.getTargetCompressionCodec(config.compressionType, sourceCodec)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 封装成 LogAppendInfo 对象返回</span></span><br><span class="line">    <span class="type">LogAppendInfo</span>(firstOffset, lastOffset, maxTimestamp, offsetOfMaxTimestamp,</span><br><span class="line">        <span class="type">Record</span>.<span class="type">NO_TIMESTAMP</span>, sourceCodec, targetCodec, shallowMessageCount, validBytesCount, monotonic)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>概括来说，上述方法主要做了以下 3 件事情：</p>
<ol>
<li>对待追加消息集合中的每条消息执行 CRC 校验；</li>
<li>对待追加消息集合中的每条消息的长度进行校验，保证不超过允许的最大值（对应 <code>max.message.bytes</code> 配置）；</li>
<li>计算待追加消息集合中的 firstOffset、lastOffset、消息的条数、有消息的字节数、offset 是否单调递增，以及获取生产者所指定的消息压缩方式。</li>
</ol>
<p><strong>步骤 2</strong> 会依据步骤 1 中对消息的校验结果，对未通过验证的消息字节部分进行截断，实现位于 <code>Log#trimInvalidBytes</code> 方法中：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">trimInvalidBytes</span></span>(records: <span class="type">MemoryRecords</span>, info: <span class="type">LogAppendInfo</span>): <span class="type">MemoryRecords</span> = {</span><br><span class="line">    <span class="comment">// 获取已验证的字节数</span></span><br><span class="line">    <span class="keyword">val</span> validBytes = info.validBytes</span><br><span class="line">    <span class="keyword">if</span> (validBytes &lt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">CorruptRecordException</span>(</span><br><span class="line">            <span class="string">"Illegal length of message set "</span> + validBytes + <span class="string">" Message set cannot be appended to log. Possible causes are corrupted produce requests"</span>)</span><br><span class="line">    <span class="comment">// 所有的字节都是已验证的，则直接返回</span></span><br><span class="line">    <span class="keyword">if</span> (validBytes == records.sizeInBytes) {</span><br><span class="line">        records</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 存在未通过验证的字节，对这些异常字节进行截断</span></span><br><span class="line">    <span class="keyword">else</span> {</span><br><span class="line">        <span class="keyword">val</span> validByteBuffer = records.buffer.duplicate()</span><br><span class="line">        validByteBuffer.limit(validBytes)</span><br><span class="line">        <span class="type">MemoryRecords</span>.readableRecords(validByteBuffer)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果在调用 <code>Log#append</code> 方法时设置了参数 <code>assignOffsets = true</code>，则在追加消息数据之前会为消息重新分配 offset（对应 <strong>步骤 3</strong> ），起始 offset 为当前 Log 对象中记录的下一条待追加消息的 offset 值。这一步主要做了以下几件事情：</p>
<ol>
<li>更新待追加消息集合的 firstOffset 为当前 Log 对象中记录的下一条待追加消息对应的 offset 值；</li>
<li>对消息（包括压缩后的）的 magic 值进行统一，验证数据完整性，并分配 offset，同时按要求更新消息的时间戳；</li>
<li>更新待追加消息集合的 lastOffset 值；</li>
<li>如果配置了 <code>message.timestamp.type=LogAppendTime</code>，则设置日志追加时间戳；</li>
<li>对待追加消息集合中的消息进行逐条校验，避免存在过长的消息。</li>
</ol>
<p>我们重点看一下第 2 步，这一步会执行 offset 分配操作，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[kafka] <span class="function"><span class="keyword">def</span> <span class="title">validateMessagesAndAssignOffsets</span></span>(records: <span class="type">MemoryRecords</span>, <span class="comment">// 待追加的消息集合</span></span><br><span class="line">                                                    offsetCounter: <span class="type">LongRef</span>, <span class="comment">// 消息对应的 offset 操作对象</span></span><br><span class="line">                                                    now: <span class="type">Long</span>, <span class="comment">// 当前时间戳</span></span><br><span class="line">                                                    sourceCodec: <span class="type">CompressionCodec</span>, <span class="comment">// 生产者指定的消息压缩方式</span></span><br><span class="line">                                                    targetCodec: <span class="type">CompressionCodec</span>, <span class="comment">// 服务端指定的消息压缩方式</span></span><br><span class="line">                                                    compactedTopic: <span class="type">Boolean</span> = <span class="literal">false</span>, <span class="comment">// 配置的消息清理策略：compact 或 delete</span></span><br><span class="line">                                                    messageFormatVersion: <span class="type">Byte</span> = <span class="type">Record</span>.<span class="type">CURRENT_MAGIC_VALUE</span>,</span><br><span class="line">                                                    messageTimestampType: <span class="type">TimestampType</span>,</span><br><span class="line">                                                    messageTimestampDiffMaxMs: <span class="type">Long</span>): <span class="type">ValidationAndOffsetAssignResult</span> = {</span><br><span class="line">    <span class="comment">// 如果未对消息进行压缩处理</span></span><br><span class="line">    <span class="keyword">if</span> (sourceCodec == <span class="type">NoCompressionCodec</span> &amp;&amp; targetCodec == <span class="type">NoCompressionCodec</span>) {</span><br><span class="line">        <span class="comment">// 存在消息的 magic 值与指定的 magic 值不一致</span></span><br><span class="line">        <span class="keyword">if</span> (!records.hasMatchingShallowMagic(messageFormatVersion)) {</span><br><span class="line">            <span class="comment">// 对消息的 magic 值进行统一，同时为消息分配 offset</span></span><br><span class="line">            convertAndAssignOffsetsNonCompressed(</span><br><span class="line">                records, offsetCounter, compactedTopic, now, messageTimestampType, messageTimestampDiffMaxMs, messageFormatVersion)</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 所有消息的 magic 值均一致，则执行 offset 分配，以及验证操作</span></span><br><span class="line">            assignOffsetsNonCompressed(records, offsetCounter, now, compactedTopic, messageTimestampType, messageTimestampDiffMaxMs)</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 如果对消息进行了压缩</span></span><br><span class="line">    <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 对消息进行解压缩，对深层消息进行 magic 值统一，并执行 offset 分配，以及验证操作</span></span><br><span class="line">        validateMessagesAndAssignOffsetsCompressed(</span><br><span class="line">            records, offsetCounter, now, sourceCodec, targetCodec, compactedTopic, messageFormatVersion, messageTimestampType, messageTimestampDiffMaxMs)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>由上面的实现可以看到，不管消息是否经过压缩，如果指定了需要为消息分配 offset，则需要处理所有的消息，包括经过压缩过的消息。方法 <code>LogValidator#validateMessagesAndAssignOffsets</code> 的主要工作也就是依据消息是否被压缩来分别调用对应的方法对待追加消息统一 magic 值，并执行 offset 分配、数据完整性校验，以及按需更新消息时间戳等操作，如果消息是经过压缩的，那么会对其进行解压缩。相关的方法实现比较冗长，这里不再继续深入。</p>
<p>如果指定不需要重新分配 offset 值，那么处理过程将会简单很多，仅仅需要验证消息已有的 offset 是否是单调递增的，并且待追加消息集合中消息的 firstOffset 不能小于 Log 对象中记录的下一条待追加消息的 offset 值，否则说明待追加的消息集合是非法的，这也是 <strong>步骤 4</strong> 的主要工作。</p>
<p><strong>步骤 5</strong> 会校验处理后消息集合的长度，保证不超过单个 LogSegment 对象所允许的最大长度（对应 <code>segment.bytes</code> 配置）。</p>
<p>在完成了一系列准备工作之后，接下去可以将处理后的待追加消息数据写入 activeSegment 对象中。 <strong>步骤 6</strong> 调用了 <code>Log#maybeRoll</code> 方法尝试从 SkipList 中获取目标 activeSegment 对象，并在需要时创建新的 activeSegment 对象。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeRoll</span></span>(messagesSize: <span class="type">Int</span>, <span class="comment">// 待追加的消息长度</span></span><br><span class="line">                      maxTimestampInMessages: <span class="type">Long</span>, <span class="comment">// 消息中的最大时间戳</span></span><br><span class="line">                      maxOffsetInMessages: <span class="type">Long</span> <span class="comment">// 消息的 lastOffset</span></span><br><span class="line">                     ): <span class="type">LogSegment</span> = {</span><br><span class="line">    <span class="comment">// 获取当前的 activeSegment 对象</span></span><br><span class="line">    <span class="keyword">val</span> segment = activeSegment</span><br><span class="line">    <span class="keyword">val</span> now = time.milliseconds</span><br><span class="line">    <span class="keyword">val</span> reachedRollMs = segment.timeWaitedForRoll(now, maxTimestampInMessages) &gt; config.segmentMs - segment.rollJitterMs</span><br><span class="line">    <span class="keyword">if</span> (segment.size &gt; config.segmentSize - messagesSize <span class="comment">// 当前 activeSegment 在追加本次消息之后，长度超过 LogSegment 允许的最大值</span></span><br><span class="line">            || (segment.size &gt; <span class="number">0</span> &amp;&amp; reachedRollMs) <span class="comment">// 当前 activeSegment 的存活时间超过了允许的最大时间</span></span><br><span class="line">            || segment.index.isFull || segment.timeIndex.isFull <span class="comment">// 索引文件满了</span></span><br><span class="line">            || !segment.canConvertToRelativeOffset(maxOffsetInMessages)) { <span class="comment">// 当前消息的 lastOffset 相对于 baseOffset 超过了 Integer.MAX_VALUE</span></span><br><span class="line">        <span class="comment">// 创建新的 activeSegment</span></span><br><span class="line">        <span class="keyword">this</span>.roll(maxOffsetInMessages - <span class="type">Integer</span>.<span class="type">MAX_VALUE</span>)</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 不需要创建新的 activeSegment，直接返回</span></span><br><span class="line">        segment</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果满足以下条件之一，则会创建一个新的 activeSegment 对象：</p>
<ol>
<li>当前 activeSegment 对象在追加本次消息之后，长度超过 LogSegment 允许的最大值（对应 <code>segment.bytes</code> 配置）。</li>
<li>当前 activeSegment 对象的存活时间超过了允许的最大时间（对应 <code>segment.ms</code> 配置）。</li>
<li>对应的索引文件（index 和 timeindex）满了。</li>
</ol>
<p>创建新 activeSegment 对象的过程位于 <code>Log#roll</code> 方法中，这里先不展开，后面会专门进行分析。</p>
<p>既然已经拿到了目标 activeSegment 对象，那么下一步（ <strong>步骤 7</strong> ）就是将待追加的消息数据写入 activeSegment 对象中（调用 <code>LogSegment#append</code> 方法，前面已经分析过）。写入成功之后需要更新 Log 对象本地记录的下一条待追加消息对应的 offset 值。</p>
<p>最后（ <strong>步骤 8</strong> ），方法会检测当前时间距离上一次执行刷盘的时间是否超过配置的时间间隔（对应 <code>flush.messages</code> 配置），是则执行刷盘操作。相关实现位于 <code>Log#flush</code> 方法中：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(): <span class="type">Unit</span> = <span class="keyword">this</span>.flush(<span class="keyword">this</span>.logEndOffset)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flush</span></span>(offset: <span class="type">Long</span>): <span class="type">Unit</span> = {</span><br><span class="line">    <span class="comment">// 如果 offset 小于等于 recoveryPoint，则直接返回，因为之前的已经全部落盘了</span></span><br><span class="line">    <span class="keyword">if</span> (offset &lt;= recoveryPoint)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    debug(<span class="string">"Flushing log '"</span> + name + <span class="string">" up to offset "</span> + offset + <span class="string">", last flushed: "</span> + lastFlushTime + <span class="string">" current time: "</span> + time.milliseconds + <span class="string">" unflushed = "</span> + unflushedMessages)</span><br><span class="line">    <span class="comment">// 获取 [recoveryPoint, offset) 之间的 LogSegment 对象</span></span><br><span class="line">    <span class="keyword">for</span> (segment &lt;- <span class="keyword">this</span>.logSegments(recoveryPoint, offset))</span><br><span class="line">        segment.flush() <span class="comment">// 执行刷盘操作，包括 log、index 和 timeindex 文件</span></span><br><span class="line">    lock synchronized {</span><br><span class="line">        <span class="comment">// 如果当前已经刷盘的 offset 大于之前记录的 recoveryPoint，则更新 recoveryPoint</span></span><br><span class="line">        <span class="keyword">if</span> (offset &gt; recoveryPoint) {</span><br><span class="line">            <span class="comment">// 更新 recoveryPoint 值</span></span><br><span class="line">            <span class="keyword">this</span>.recoveryPoint = offset</span><br><span class="line">            <span class="comment">// 更新最近一次执行 flush 的时间</span></span><br><span class="line">            lastflushedTime.set(time.milliseconds)</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>执行刷盘操作之前会先将当前 offset 与 recoveryPoint 变量进行比较，这里的 offset 对应当前 Log 对象中记录的下一条待追加消息的 offset，而 recoveryPoint 变量在当前 Log 对象创建时指定，并在运行过程中更新，用于表示当前已经刷盘的日志数据对应的最大 offset 值。如果当前 offset 小于等于 recoveryPoint，则无需执行刷盘操作，因为 recoveryPoint 之前的数据已经全部落盘了。否则会调用 <code>Log#logSegments</code> 方法从当前 Log 对象的 SkipList 中获取位于 <code>[recoveryPoint, offset)</code> 区间的 LogSegment 对象集合，并应用 <code>LogSegment#flush</code> 方法对 LogSegment 相关的文件执行刷盘操作，包括 log、index 和 timeindex 文件。同时会更新 recoveryPoint 和 lastflushedTime 字段，后者用于记录最近一次执行刷盘操作的时间戳。</p>

        <h4 id="创建-Active-Segment-对象">
          <a href="#创建-Active-Segment-对象" class="heading-link"><i class="fas fa-link"></i></a>创建 Active Segment 对象</h4>
      <p>既然上一小节提到了 <code>Log#roll</code> 方法，那么本小节就来分析一下该方法的实现，该方法用于创建一个新的 activeSegment 对象，并将上任的 activeSegment 对象中的数据落盘。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">roll</span></span>(expectedNextOffset: <span class="type">Long</span> = <span class="number">0</span>): <span class="type">LogSegment</span> = {</span><br><span class="line">    <span class="keyword">val</span> start = time.nanoseconds</span><br><span class="line">    lock synchronized {</span><br><span class="line">        <span class="comment">// 获取 LEO 值</span></span><br><span class="line">        <span class="keyword">val</span> newOffset = <span class="type">Math</span>.max(expectedNextOffset, logEndOffset)</span><br><span class="line">        <span class="keyword">val</span> logFile = <span class="type">Log</span>.logFile(dir, newOffset) <span class="comment">// 对应的 log 文件</span></span><br><span class="line">        <span class="keyword">val</span> indexFile = indexFilename(dir, newOffset) <span class="comment">// 对应的 index 文件</span></span><br><span class="line">        <span class="keyword">val</span> timeIndexFile = timeIndexFilename(dir, newOffset) <span class="comment">// 对应的 timeindex 文件</span></span><br><span class="line">        <span class="comment">// 遍历检查，如果文件存在则删除</span></span><br><span class="line">        <span class="keyword">for</span> (file &lt;- <span class="type">List</span>(logFile, indexFile, timeIndexFile); <span class="keyword">if</span> file.exists) {</span><br><span class="line">            warn(<span class="string">"Newly rolled segment file "</span> + file.getName + <span class="string">" already exists; deleting it first"</span>)</span><br><span class="line">            file.delete()</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理之前的 activeSegment 对象</span></span><br><span class="line">        segments.lastEntry() <span class="keyword">match</span> {</span><br><span class="line">            <span class="keyword">case</span> <span class="literal">null</span> =&gt;</span><br><span class="line">            <span class="keyword">case</span> entry =&gt;</span><br><span class="line">                <span class="keyword">val</span> seg: <span class="type">LogSegment</span> = entry.getValue</span><br><span class="line">                <span class="comment">// 追加最大时间戳与对应的 offset 到 timeindex 文件</span></span><br><span class="line">                seg.onBecomeInactiveSegment()</span><br><span class="line">                <span class="comment">// 对 log、index 和 timeindex 文件进行截断处理，仅保留有效字节</span></span><br><span class="line">                seg.index.trimToValidSize()</span><br><span class="line">                seg.timeIndex.trimToValidSize()</span><br><span class="line">                seg.log.trim()</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建新的 activeSegment 对象</span></span><br><span class="line">        <span class="keyword">val</span> segment = <span class="keyword">new</span> <span class="type">LogSegment</span>(</span><br><span class="line">            dir,</span><br><span class="line">            startOffset = newOffset,</span><br><span class="line">            indexIntervalBytes = config.indexInterval,</span><br><span class="line">            maxIndexSize = config.maxIndexSize,</span><br><span class="line">            rollJitterMs = config.randomSegmentJitter,</span><br><span class="line">            time = time,</span><br><span class="line">            fileAlreadyExists = <span class="literal">false</span>,</span><br><span class="line">            initFileSize = initFileSize(),</span><br><span class="line">            preallocate = config.preallocate)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 添加新的 activeSegment 到 segments 跳跃表中</span></span><br><span class="line">        <span class="keyword">val</span> prev = <span class="keyword">this</span>.addSegment(segment)</span><br><span class="line">        <span class="comment">// 如果对应位置已经存在 LogSegment，则抛出异常</span></span><br><span class="line">        <span class="keyword">if</span> (prev != <span class="literal">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">KafkaException</span>(<span class="string">"Trying to roll a new log segment for topic partition %s with start offset %d while it already exists."</span>.format(name, newOffset))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 因为有新的 activeSegment 对象创建，所以更新 Log 中记录的 activeSegment 的 baseOffset 值，及其物理地址</span></span><br><span class="line">        <span class="keyword">this</span>.updateLogEndOffset(nextOffsetMetadata.messageOffset)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行 flush 操作，将上任 activeSegment 的数据落盘</span></span><br><span class="line">        scheduler.schedule(<span class="string">"flush-log"</span>, () =&gt; <span class="keyword">this</span>.flush(newOffset))</span><br><span class="line"></span><br><span class="line">        info(<span class="string">"Rolled new log segment for '"</span> + name + <span class="string">"' in %.0f ms."</span>.format((<span class="type">System</span>.nanoTime - start) / (<span class="number">1000.0</span> * <span class="number">1000.0</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回新的 activeSegment 对象</span></span><br><span class="line">        segment</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>创建一个新的 activeSegment 对象的过程比较直观，无非是创建一个新的 activeSegment 对象，并将其添加到 SkipList 中，同时需要更新 Log 对象本地记录的 activeSegment 对象的 baseOffset 及其物理地址。此外，我们需要将上一任 activeSegment 对象中的数据落盘，Kafka 为此注册了一个名为 flush-log 的定时任务异步处理该过程，需要注意的是这里的 flush-log 任务仅运行一次。这里的刷盘操作是将 recoveryPoint 到新 activeSegment 对象 baseOffset （不包括）之间的数据落盘，具体的落盘操作交由 <code>Log#flush</code> 方法执行，我们在前面已经分析过该方法，这里不再重复撰述。</p>

        <h4 id="读取日志数据-1">
          <a href="#读取日志数据-1" class="heading-link"><i class="fas fa-link"></i></a>读取日志数据</h4>
      <p>下面接着来看一下从 Log 对象中读取日志数据的过程，位于 <code>Log#read</code> 方法中。不同于追加消息时只能操作 activeSegment 对象，读取消息可以从 SkipList 中任意一个 LogSegment 对象中进行读取。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(startOffset: <span class="type">Long</span>, <span class="comment">// 读取消息的起始 offset</span></span><br><span class="line">         maxLength: <span class="type">Int</span>, <span class="comment">// 读取消息的最大字节数</span></span><br><span class="line">         maxOffset: <span class="type">Option</span>[<span class="type">Long</span>] = <span class="type">None</span>, <span class="comment">// 读取消息的结束 offset</span></span><br><span class="line">         minOneMessage: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">FetchDataInfo</span> = {</span><br><span class="line"></span><br><span class="line">    trace(<span class="string">"Reading %d bytes from offset %d in log %s of length %d bytes"</span>.format(maxLength, startOffset, name, size))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将 nextOffsetMetadata 保存成局部变量，避免加锁带来的竞态条件</span></span><br><span class="line">    <span class="keyword">val</span> currentNextOffsetMetadata = nextOffsetMetadata</span><br><span class="line">    <span class="comment">// 获取 Log 本地记录的下一条待追加消息消息对应的 offset 值</span></span><br><span class="line">    <span class="keyword">val</span> next = currentNextOffsetMetadata.messageOffset</span><br><span class="line">    <span class="comment">// 边界检查</span></span><br><span class="line">    <span class="keyword">if</span> (startOffset == next)</span><br><span class="line">        <span class="keyword">return</span> <span class="type">FetchDataInfo</span>(currentNextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查找 baseOffset 小于等于 startOffset 且最大的 LogSegment 对象</span></span><br><span class="line">    <span class="keyword">var</span> entry = segments.floorEntry(startOffset)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 边界检查，Log 对象中记录的最后一条消息的真实 offset 应该是 next-1，next 指的是下一条追加消息的 offset</span></span><br><span class="line">    <span class="keyword">if</span> (startOffset &gt; next || entry == <span class="literal">null</span>)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OffsetOutOfRangeException</span>(<span class="string">"Request for offset %d but we only have log segments in the range %s to %d."</span>.format(startOffset, segments.firstKey, next))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (entry != <span class="literal">null</span>) {</span><br><span class="line">        <span class="comment">// 获取待读取的最大物理地址</span></span><br><span class="line">        <span class="keyword">val</span> maxPosition = {</span><br><span class="line">            <span class="comment">// 如果当前读取的是 activeSegment 对象</span></span><br><span class="line">            <span class="keyword">if</span> (entry == segments.lastEntry) {</span><br><span class="line">                <span class="comment">// 从 nextOffsetMetadata 对象中获取 activeSegment 对应的最大物理地址</span></span><br><span class="line">                <span class="keyword">val</span> exposedPos = nextOffsetMetadata.relativePositionInSegment.toLong</span><br><span class="line">                <span class="comment">// 如果期间正好创建了一个新的 activeSegment 对象，那么这里拿到的应该是上一任 activeSegment 对象，</span></span><br><span class="line">                <span class="comment">// 它已经不再活跃了，可以直接读取到结尾</span></span><br><span class="line">                <span class="keyword">if</span> (entry != segments.lastEntry)</span><br><span class="line">                    entry.getValue.size</span><br><span class="line">                <span class="comment">// 否则，直接返回 exposedPos，如果这里读取到 LogSegment 结尾的话，可能会出现 OffsetOutOfRangeException 异常</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    exposedPos</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// 如果当前读取的不是 activeSegment 对象，则直接读取到对应 LogSegment 的结尾</span></span><br><span class="line">            <span class="keyword">else</span> {</span><br><span class="line">                entry.getValue.size</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调用 LogSegment#read 方法读取消息</span></span><br><span class="line">        <span class="keyword">val</span> fetchInfo = entry.getValue.read(startOffset, maxOffset, maxLength, maxPosition, minOneMessage)</span><br><span class="line">        <span class="keyword">if</span> (fetchInfo == <span class="literal">null</span>) {</span><br><span class="line">            <span class="comment">// 如果没有读取到消息，则尝试读取下一个 LogSegment 对象</span></span><br><span class="line">            entry = segments.higherEntry(entry.getKey)</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="keyword">return</span> fetchInfo</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 未读取到 startOffset 之后的消息</span></span><br><span class="line">    <span class="type">FetchDataInfo</span>(nextOffsetMetadata, <span class="type">MemoryRecords</span>.<span class="type">EMPTY</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>读取日志数据的执行过程如代码注释，比较直观，在做好边界检查的前提下寻找小于 startOffset 的最大 baseOffset，并以此 offset 开始从 SkipList 中定位 LogSegment 对象，如果该 LogSegment 对象为空，则会继续读取下一个 LogSegment 对象。读取的过程区分是不是 activeSegment 对象，如果当前读取的 LogSegment 不是 activeSegment 对象，那么对应的 LogSegment 已经是“冷却”状态，所以我们可以直接将其中的数据全部读取出来返回，如果当前读取的是 activeSegment 对象，则需要以 Log 对象中记录的 activeSegment 对象的最大物理地址作为读取的上界，如果直接读取到 activeSegment 对象结尾可能导致 OffsetOutOfRangeException 异常。考虑下面这样一个场景（假设有读线程 A 和写线程 B）：</p>
<blockquote>
<ol>
<li>A 线程请求读取 startOffset 为 101 之后的数据，刚好该请求落在了 activeSegment 对象上；</li>
<li>B 线程调用 append 方法追加了 offset 为 [105, 109] 的消息集合，但是还未更新 Log 对象本地记录的下一条消息对应的 offset 值（此时仍为 105）；</li>
<li>A 线程读取到了 [101, 109] 之间的数据，并且继续请求 startOffset 为 110 之后的数据，但是因为 <code>startOffset &gt; next</code> 而抛出 OffsetOutOfRangeException 异常。</li>
</ol>
</blockquote>
<p>所以对于 activeSegment 对象而言，我们应该以 Log 对象中记录的 activeSegment 对应的最大物理地址作为上界。另外一个需要考虑的问题是在读取 activeSegment 对象过程中，因为追加消息而产生了新的 activeSegment 对象的情况，那么此时 <code>Log#read</code> 方法持有的 activeSegment 对象就变成前任了，也就不会再有写操作同时发生的问题，所以可以直接读取到该 activeSegment 对象的结尾位置。</p>

        <h4 id="删除日志数据">
          <a href="#删除日志数据" class="heading-link"><i class="fas fa-link"></i></a>删除日志数据</h4>
      <p>本章节的最后，一起来看一下 <code>Log#delete</code> 方法，该方法会删除当前 Log 对象对应 log 目录，以及目录下的所有文件，并清空 SkipList 对象。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">delete</span></span>() {</span><br><span class="line">    lock synchronized {</span><br><span class="line">        <span class="comment">// 遍历 SkipList 中每个 LogSegment 对应的 log、index 和 timeindex 文件</span></span><br><span class="line">        logSegments.foreach(_.delete())</span><br><span class="line">        <span class="comment">// 清空 SkipList 对象</span></span><br><span class="line">        segments.clear()</span><br><span class="line">        <span class="comment">// 删除 log 目录及其目录下的所有文件和目录</span></span><br><span class="line">        <span class="type">Utils</span>.delete(dir)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>具体逻辑如代码注释，比较简单。</p>

        <h3 id="LogManager-组件">
          <a href="#LogManager-组件" class="heading-link"><i class="fas fa-link"></i></a>LogManager 组件</h3>
      <p>LogManager 是 Kafka 日志数据操作的入口，基于上一节分析的 Log 类对象提供了对日志数据的加载、创建、删除，以及查询等功能。我们在配置 Kafka 服务时，可以通过 <code>log.dirs</code> 配置项为一个 broker 节点指定多个 log 目录，这些目录均由 LogManager 负责管理。LogManager 在启动时会校验 <code>log.dirs</code> 配置，确保指定的 log 目录没有重复的配置且都是可读的，同时对于不存在的目录会执行创建。每个 log 目录下包含多个 topic 分区目录，每个 topic 分区目录由一个 Log 类对象对其进行管理，LogManager 会记录每个 topic 分区对象及其对应的 Log 类对象之间的映射关系。LogManager 类的字段定义如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogManager</span>(<span class="params">val logDirs: <span class="type">Array</span>[<span class="type">File</span>], // log 目录集合，对应 log.dirs 配置，一般选择 log 数目最少的目录进行创建</span></span></span><br><span class="line"><span class="class"><span class="params">                 val topicConfigs: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">LogConfig</span>], // topic 相关配置</span></span></span><br><span class="line"><span class="class"><span class="params">                 val defaultConfig: <span class="type">LogConfig</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 val cleanerConfig: <span class="type">CleanerConfig</span>, // log cleaner 相关配置</span></span></span><br><span class="line"><span class="class"><span class="params">                 ioThreads: <span class="type">Int</span>, // 每个 log 目录下分配的执行加载任务的线程数目</span></span></span><br><span class="line"><span class="class"><span class="params">                 val flushCheckMs: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 val flushCheckpointMs: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 val retentionCheckMs: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                 scheduler: <span class="type">Scheduler</span>, // 定时任务调度器</span></span></span><br><span class="line"><span class="class"><span class="params">                 val brokerState: <span class="type">BrokerState</span>, // 当前 broker 节点的状态</span></span></span><br><span class="line"><span class="class"><span class="params">                 time: <span class="type">Time</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每个 log 目录下面都有一个 recovery-point-offset-checkpoint 文件，</span></span><br><span class="line"><span class="comment">     * 记录了当前 log 目录每个 Log 的 recoveryPoint 信息，用于在 broker 启动时恢复日志数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">val</span> <span class="type">RecoveryPointCheckpointFile</span> = <span class="string">"recovery-point-offset-checkpoint"</span></span><br><span class="line">    <span class="comment">/** 创建或删除 Log 时的锁对象 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> logCreationOrDeletionLock = <span class="keyword">new</span> <span class="type">Object</span></span><br><span class="line">    <span class="comment">/** 记录每个 topic 分区对象与 Log 对象之间的映射关系 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> logs = <span class="keyword">new</span> <span class="type">Pool</span>[<span class="type">TopicPartition</span>, <span class="type">Log</span>]()</span><br><span class="line">    <span class="comment">/** 记录需要被删除的 Log 对象 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> logsToBeDeleted = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">Log</span>]()</span><br><span class="line">    <span class="comment">/** 尝试对每个 log 目录在文件系统层面加锁，这里加的是进程锁 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> dirLocks = <span class="keyword">this</span>.lockLogDirs(logDirs)</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 遍历为每个 log 目录创建一个操作其名下 recovery-point-offset-checkpoint 文件的 OffsetCheckpoint 对象，</span></span><br><span class="line"><span class="comment">     * 并建立映射关系</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> recoveryPointCheckpoints = logDirs.map(</span><br><span class="line">        <span class="comment">// recovery-point-offset-checkpoint 文件</span></span><br><span class="line">        dir =&gt; (dir, <span class="keyword">new</span> <span class="type">OffsetCheckpoint</span>(<span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">RecoveryPointCheckpointFile</span>)))).toMap</span><br><span class="line">    <span class="comment">/** 用于清理过期或者过大的日志 */</span></span><br><span class="line">    <span class="keyword">val</span> cleaner: <span class="type">LogCleaner</span> = <span class="keyword">if</span> (cleanerConfig.enableCleaner) <span class="keyword">new</span> <span class="type">LogCleaner</span>(cleanerConfig, logDirs, logs, time = time) <span class="keyword">else</span> <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>LogManager 在实例化过程中会执行以下操作：</p>
<ol>
<li>遍历处理配置的 log 路径（对应 <code>log.dirs</code> 配置），如果对应的路径不存在则创建，同时校验路径是否存在重复、是否是目录，以及是否可读；</li>
<li>遍历配置的 log 目录，尝试对每个目录在文件系统层面加锁，这里加的是进程锁；</li>
<li>遍历配置的 log 目录，为每个目录下的 recovery-point-offset-checkpoint 文件创建对应的 OffsetCheckpoint 对象，用于管理每个 topic 分区对应的 HW offset 信息；</li>
<li>遍历配置的 log 目录，将每个 topic 分区对应的日志数据封装成 Log 对象，同时记录需要被删除的 topic 分区目录，等待后续删除。</li>
</ol>
<p>文件 recovery-point-offset-checkpoint 用于记录每个 topic 分区对应的 HW offset 信息，当 broker 节点重启时辅助恢复每个 topic 分区的日志数据。一个简单的文件示例如下：</p>
<figure class="highlight text"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">8</span><br><span class="line">topic-default 3 2271154</span><br><span class="line">topic-default 2 2271351</span><br><span class="line">topic-default 4 2271051</span><br><span class="line">topic-default 0 2270751</span><br><span class="line">topic-default 5 2271558</span><br><span class="line">topic-default 1 2272018</span><br><span class="line">topic-default 7 2271197</span><br><span class="line">topic-default 6 2270673</span><br></pre></td></tr></tbody></table></div></figure>
<p>其中第一行是版本号，第二行是记录条数，从第三行开始每一行都记录着“topic partition HW”信息。OffsetCheckpoint 类定义了 <code>OffsetCheckpoint#write</code> 和 <code>OffsetCheckpoint#read</code> 两个方法，用于对 recovery-point-offset-checkpoint 执行读写操作。</p>
<p>步骤 4 会执行加载每个 log 目录下的日志文件，并为每个 topic 分区对应的日志目录创建一个 Log 对象，对于标记为需要删除的 topic 分区目录（对应“-delete”后缀的目录），则将其 Log 对象添加到 <code>LogManager#logsToBeDeleted</code> 字段中，等待后面的周期性任务（kafka-delete-logs）对其进行删除。相关实现位于 <code>LogManager#loadLogs</code> 方法中：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">loadLogs</span></span>(): <span class="type">Unit</span> = {</span><br><span class="line">    info(<span class="string">"Loading logs."</span>)</span><br><span class="line">    <span class="keyword">val</span> startMs = time.milliseconds</span><br><span class="line">    <span class="comment">// 用于记录所有 log 目录对应的线程池</span></span><br><span class="line">    <span class="keyword">val</span> threadPools = mutable.<span class="type">ArrayBuffer</span>.empty[<span class="type">ExecutorService</span>]</span><br><span class="line">    <span class="keyword">val</span> jobs = mutable.<span class="type">Map</span>.empty[<span class="type">File</span>, <span class="type">Seq</span>[<span class="type">Future</span>[_]]]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历处理每个 log 目录</span></span><br><span class="line">    <span class="keyword">for</span> (dir &lt;- <span class="keyword">this</span>.logDirs) {</span><br><span class="line">        <span class="comment">// 为每个 log 目录创建一个 ioThreads 大小的线程池</span></span><br><span class="line">        <span class="keyword">val</span> pool = <span class="type">Executors</span>.newFixedThreadPool(ioThreads)</span><br><span class="line">        threadPools.append(pool)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 尝试获取 .kafka_cleanshutdown 文件，如果该文件存在则说明 broker 节点是正常关闭的</span></span><br><span class="line">        <span class="keyword">val</span> cleanShutdownFile = <span class="keyword">new</span> <span class="type">File</span>(dir, <span class="type">Log</span>.<span class="type">CleanShutdownFile</span>)</span><br><span class="line">        <span class="keyword">if</span> (cleanShutdownFile.exists) {</span><br><span class="line">            debug(<span class="string">"Found clean shutdown file. Skipping recovery for all logs in data directory: "</span> + dir.getAbsolutePath)</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 当前 broker 不是正常关闭，设置 broker 状态为 RecoveringFromUncleanShutdown，表示正在从上次异常关闭中恢复</span></span><br><span class="line">            brokerState.newState(<span class="type">RecoveringFromUncleanShutdown</span>)</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取每个 log 目录下的 recovery-point-offset-checkpoint 文件，返回 topic 分区对象与 HW 之间的映射关系</span></span><br><span class="line">        <span class="keyword">var</span> recoveryPoints = <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>]()</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            recoveryPoints = <span class="keyword">this</span>.recoveryPointCheckpoints(dir).read()</span><br><span class="line">        } <span class="keyword">catch</span> {</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">                warn(<span class="string">"Error occured while reading recovery-point-offset-checkpoint file of directory "</span> + dir, e)</span><br><span class="line">                warn(<span class="string">"Resetting the recovery checkpoint to 0"</span>)</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历当前 log 目录的子目录，仅处理目录，忽略文件</span></span><br><span class="line">        <span class="keyword">val</span> jobsForDir = <span class="keyword">for</span> {</span><br><span class="line">            dirContent &lt;- <span class="type">Option</span>(dir.listFiles).toList</span><br><span class="line">            logDir &lt;- dirContent <span class="keyword">if</span> logDir.isDirectory</span><br><span class="line">        } <span class="keyword">yield</span> {</span><br><span class="line">            <span class="comment">// 为每个 Log 目录创建一个 Runnable 任务</span></span><br><span class="line">            <span class="type">CoreUtils</span>.runnable {</span><br><span class="line">                debug(<span class="string">"Loading log '"</span> + logDir.getName + <span class="string">"'"</span>)</span><br><span class="line">                <span class="comment">// 依据目录名解析得到对应的 topic 分区对象</span></span><br><span class="line">                <span class="keyword">val</span> topicPartition = <span class="type">Log</span>.parseTopicPartitionName(logDir)</span><br><span class="line">                <span class="comment">// 获取当前 topic 分区对应的配置</span></span><br><span class="line">                <span class="keyword">val</span> config = topicConfigs.getOrElse(topicPartition.topic, defaultConfig)</span><br><span class="line">                <span class="comment">// 获取 topic 分区对应的 HW 值</span></span><br><span class="line">                <span class="keyword">val</span> logRecoveryPoint = recoveryPoints.getOrElse(topicPartition, <span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 创建对应的 Log 对象，每个 topic 分区目录对应一个 Log 对象</span></span><br><span class="line">                <span class="keyword">val</span> current = <span class="keyword">new</span> <span class="type">Log</span>(logDir, config, logRecoveryPoint, scheduler, time)</span><br><span class="line">                <span class="comment">// 如果当前 log 是需要被删除的文件，则记录到 logsToBeDeleted 队列中，会有周期性任务对其执行删除操作</span></span><br><span class="line">                <span class="keyword">if</span> (logDir.getName.endsWith(<span class="type">Log</span>.<span class="type">DeleteDirSuffix</span>)) { <span class="comment">// -delete</span></span><br><span class="line">                    logsToBeDeleted.add(current)</span><br><span class="line">                } <span class="keyword">else</span> {</span><br><span class="line">                    <span class="comment">// 建立 topic 分区对象与其 Log 对象之间的映射关系，不允许一个 topic 分区对象对应多个目录</span></span><br><span class="line">                    <span class="keyword">val</span> previous = logs.put(topicPartition, current)</span><br><span class="line">                    <span class="keyword">if</span> (previous != <span class="literal">null</span>) {</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</span><br><span class="line">                            <span class="string">"Duplicate log directories found: %s, %s!"</span>.format(current.dir.getAbsolutePath, previous.dir.getAbsolutePath))</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交上面创建的任务，并将提交结果封装到 jobs 集合中，jobsForDir 是 List[Runnable] 类型</span></span><br><span class="line">        jobs(cleanShutdownFile) = jobsForDir.map(pool.submit)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 阻塞等待上面提交的任务执行完成，即等待所有 log 目录下 topic 分区对应的目录文件加载完成</span></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">for</span> ((cleanShutdownFile, dirJobs) &lt;- jobs) {</span><br><span class="line">            dirJobs.foreach(_.get)</span><br><span class="line">            <span class="comment">// 删除对应的 .kafka_cleanshutdown 文件</span></span><br><span class="line">            cleanShutdownFile.delete()</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">ExecutionException</span> =&gt;</span><br><span class="line">            error(<span class="string">"There was an error in one of the threads during logs loading: "</span> + e.getCause)</span><br><span class="line">            <span class="keyword">throw</span> e.getCause</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        <span class="comment">// 遍历关闭线程池</span></span><br><span class="line">        threadPools.foreach(_.shutdown())</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    info(<span class="string">s"Logs loading complete in <span class="subst">${time.milliseconds - startMs}</span> ms."</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>LogManager 在实例化时会为每个 log 目录创建一个指定大小的线程池，然后对目录下的子目录（不包括文件）进行并发加载，最终将每个 topic 分区目录下的日志相关数据封装成 Log 对象，并记录到 <code>LogManager#logs</code> 字段中，这是一个 <code>Pool[K, V]</code> 类型的字段，基于 ConcurrentHashMap 实现，其中这里的 key 为 Log 对象所属的 topic 分区对象。</p>
<p>在 LogManager 启动时（对应 <code>LogManager#startup</code> 方法）会注册一个名为 kafka-delete-logs 的周期性任务，该任务会周期性调用 <code>LogManager#deleteLogs</code> 方法对标记为“-delete”的目录执行删除操作。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteLogs</span></span>(): <span class="type">Unit</span> = {</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">var</span> failed = <span class="number">0</span></span><br><span class="line">        <span class="comment">// 如果存在需要删除的目录</span></span><br><span class="line">        <span class="keyword">while</span> (!logsToBeDeleted.isEmpty &amp;&amp; failed &lt; logsToBeDeleted.size()) {</span><br><span class="line">            <span class="comment">// 获取需要删除的目录对应的 Log 对象</span></span><br><span class="line">            <span class="keyword">val</span> removedLog = logsToBeDeleted.take()</span><br><span class="line">            <span class="keyword">if</span> (removedLog != <span class="literal">null</span>) {</span><br><span class="line">                <span class="keyword">try</span> {</span><br><span class="line">                    <span class="comment">// 调用 Log.delete 方法执行删除操作</span></span><br><span class="line">                    removedLog.delete()</span><br><span class="line">                    info(<span class="string">s"Deleted log for partition <span class="subst">${removedLog.topicPartition}</span> in <span class="subst">${removedLog.dir.getAbsolutePath}</span>."</span>)</span><br><span class="line">                } <span class="keyword">catch</span> {</span><br><span class="line">                    <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">                        error(<span class="string">s"Exception in deleting <span class="subst">$removedLog</span>. Moving it to the end of the queue."</span>, e)</span><br><span class="line">                        failed = failed + <span class="number">1</span></span><br><span class="line">                        <span class="comment">// 如果删除异常，则归还，下一次周期性调用时再删除</span></span><br><span class="line">                        logsToBeDeleted.put(removedLog)</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">            error(<span class="string">s"Exception in kafka-delete-logs thread."</span>, e)</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>方法 <code>LogManager#deleteLogs</code> 会遍历 <code>LogManager#logsToBeDeleted</code> 队列，并对其中的 Log 对象调用 <code>Log#delete</code> 方法执行删除，如果删除异常则会归还到队列，并在下一次周期性调用时再尝试执行删除。方法 <code>Log#delete</code> 已经在前面分析过，这里不再重复撰述。</p>

        <h4 id="周期性定时任务">
          <a href="#周期性定时任务" class="heading-link"><i class="fas fa-link"></i></a>周期性定时任务</h4>
      <p>前面分析了启动过程中激活的 kafka-delete-logs 周期性任务，下面继续来看一下 <code>LogManager#startup</code> 方法的剩余实现，该方法主要的逻辑就是启动 4 个周期性任务。在 Kafka 服务启动时会创建 LogManager 实例，并调用 <code>LogManager#startup</code> 方法，该方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startup</span></span>() {</span><br><span class="line">    <span class="keyword">if</span> (scheduler != <span class="literal">null</span>) {</span><br><span class="line">        <span class="comment">// 1. 启动 kafka-log-retention 周期性任务，对过期或过大的日志文件执行清理工作</span></span><br><span class="line">        info(<span class="string">"Starting log cleanup with a period of %d ms."</span>.format(retentionCheckMs))</span><br><span class="line">        scheduler.schedule(<span class="string">"kafka-log-retention"</span>,</span><br><span class="line">            <span class="keyword">this</span>.cleanupLogs,</span><br><span class="line">            delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">            period = retentionCheckMs,</span><br><span class="line">            <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 启动 kafka-log-flusher 周期性任务，对日志文件执行刷盘操作</span></span><br><span class="line">        info(<span class="string">"Starting log flusher with a default period of %d ms."</span>.format(flushCheckMs))</span><br><span class="line">        scheduler.schedule(<span class="string">"kafka-log-flusher"</span>,</span><br><span class="line">            <span class="keyword">this</span>.flushDirtyLogs,</span><br><span class="line">            delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">            period = flushCheckMs,</span><br><span class="line">            <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 启动 kafka-recovery-point-checkpoint 周期性任务，更新 recovery-point-offset-checkpoint 文件</span></span><br><span class="line">        scheduler.schedule(<span class="string">"kafka-recovery-point-checkpoint"</span>,</span><br><span class="line">            <span class="keyword">this</span>.checkpointRecoveryPointOffsets,</span><br><span class="line">            delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">            period = flushCheckpointMs,</span><br><span class="line">            <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 启动 kafka-delete-logs 周期性任务，删除标记为需要被删除的 log 目录</span></span><br><span class="line">        scheduler.schedule(<span class="string">"kafka-delete-logs"</span>,</span><br><span class="line">            <span class="keyword">this</span>.deleteLogs,</span><br><span class="line">            delay = <span class="type">InitialTaskDelayMs</span>,</span><br><span class="line">            period = defaultConfig.fileDeleteDelayMs,</span><br><span class="line">            <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动 LogCleaner 线程</span></span><br><span class="line">    <span class="keyword">if</span> (cleanerConfig.enableCleaner) cleaner.startup()</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>LogManager 在启动过程中启动了 4 个周期性任务和 1 个 LogCleaner 线程，这 4 个周期性任务包括：</p>
<ol>
<li><strong>kafka-log-retention</strong> ：定期对过期或过大的日志文件执行清理操作。</li>
<li><strong>kafka-log-flusher</strong> ：定期对日志文件执行刷盘操作。</li>
<li><strong>kafka-recovery-point-checkpoint</strong> ：定期更新 recovery-point-offset-checkpoint 文件。</li>
<li><strong>kafka-delete-logs</strong> ：定期删除标记为需要被删除的 log 目录。</li>
</ol>
<p>其中任务 4 我们已经在前面分析过，下面逐个来看一下前 3 个任务。 <strong>任务 1</strong> 的实现位于 <code>LogManager#cleanupLogs</code> 方法中，该方法会遍历所有的 Log 对象，并从两个维度对执行清理工作：</p>
<ol>
<li>时间维度：即保证 Log 对象中所有的 LogSegment 都是有效的，对于过期的 LogSegment 执行删除操作。</li>
<li>空间维度：既保证 Log 对象不应过大，对于超出的部分会执行删除操作。</li>
</ol>
<p>实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cleanupLogs</span></span>() {</span><br><span class="line">    debug(<span class="string">"Beginning log cleanup..."</span>)</span><br><span class="line">    <span class="keyword">var</span> total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">val</span> startMs = time.milliseconds</span><br><span class="line">    <span class="comment">// 遍历处理每个 topic 分区对应的 Log 对象，只有对应 Log 配置了 cleanup.policy=delete 才会执行删除</span></span><br><span class="line">    <span class="keyword">for</span> (log &lt;- allLogs(); <span class="keyword">if</span> !log.config.compact) {</span><br><span class="line">        debug(<span class="string">"Garbage collecting '"</span> + log.name + <span class="string">"'"</span>)</span><br><span class="line">        <span class="comment">// 遍历删除当前 Log 对象中过期的 LogSegment 对象，并保证 Log 的大小在允许范围内（对应 retention.bytes 配置）</span></span><br><span class="line">        total += log.deleteOldSegments()</span><br><span class="line">    }</span><br><span class="line">    debug(<span class="string">"Log cleanup completed. "</span> + total + <span class="string">" files deleted in "</span> + (time.milliseconds - startMs) / <span class="number">1000</span> + <span class="string">" seconds"</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>清理操作仅处理配置了 <code>cleanup.policy=delete</code> 的 Log 对象，并调用 <code>Log#deleteOldSegments</code> 方法执行判定和删除操作。方法 <code>Log#deleteOldSegments</code> 中通过调用 <code>Log#deleteRetentionMsBreachedSegments</code> 对过期的 LogSegment 对象执行删除操作，并调用 <code>Log#deleteRetentionSizeBreachedSegments</code> 方法对当前 Log 对象的大小进行判定，如果超过设定大小，则会从 Log 对象中删除部分 LogSegment 对象，以保证最终的 Log 大小在允许范围内。这两个方法最终都是调用 <code>Log#deleteOldSegments</code> 方法执行具体的删除操作，该方法接收一个 <code>LogSegment =&gt; Boolean</code> 类的函数，如果某个 LogSegment 对象满足给定的谓语，则会应用 <code>Log#deleteSegment</code> 方法对该 LogSegment 执行删除操作。</p>
<p>其中 <code>Log#deleteRetentionMsBreachedSegments</code> 方法给定的判定条件很简单（如下），比较当前 LogSegment 对象最大消息时间戳距离当前时间是否超过 <code>retention.ms</code> 毫秒，如果超过则认为该 LogSegment 已过期。</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetentionMsBreachedSegments</span></span>(): <span class="type">Int</span> = {</span><br><span class="line">    <span class="keyword">if</span> (config.retentionMs &lt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">val</span> startMs = time.milliseconds</span><br><span class="line">    <span class="comment">// 如果 LogSegment 中最大时间戳距离当前已经超过配置时间，则删除</span></span><br><span class="line">    <span class="keyword">this</span>.deleteOldSegments(startMs - _.largestTimestamp &gt; config.retentionMs)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>而 <code>Log#deleteRetentionSizeBreachedSegments</code> 方法则会首先计算出当前 Log 超出设定值（对应 <code>retention.bytes</code> 配置）的字节数，然后对 Log 中的 LogSegment 对象遍历删除，直到 Log 的大小不再超出为止。实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteRetentionSizeBreachedSegments</span></span>(): <span class="type">Int</span> = {</span><br><span class="line">    <span class="keyword">if</span> (config.retentionSize &lt; <span class="number">0</span> || size &lt; config.retentionSize) <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="comment">// Log 的总大小减去允许的大小</span></span><br><span class="line">    <span class="keyword">var</span> diff = size - config.retentionSize</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">shouldDelete</span></span>(segment: <span class="type">LogSegment</span>): <span class="type">Boolean</span> = {</span><br><span class="line">        <span class="comment">// 大于等于 0 则说明仍然过大</span></span><br><span class="line">        <span class="keyword">if</span> (diff - segment.size &gt;= <span class="number">0</span>) {</span><br><span class="line">            diff -= segment.size</span><br><span class="line">            <span class="literal">true</span></span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="literal">false</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 删除 Log 中超出大小的部分</span></span><br><span class="line">    <span class="keyword">this</span>.deleteOldSegments(shouldDelete)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>接下来继续看一下公共逻辑 <code>Log#deleteOldSegments</code> 方法（实现如下），该方法会基于给定的谓语 predicate 从 Log 中选择需要被删除的 LogSegment 对象，并对每个需要被删除的 LogSegment 对象应用 <code>Log#deleteSegment</code> 方法进行删除，包括从 Log 对象中移除该 LogSegment 对象，以及删除 LogSegment 对应的 log、index 和 timeindex 文件。</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">deleteOldSegments</span></span>(predicate: <span class="type">LogSegment</span> =&gt; <span class="type">Boolean</span>): <span class="type">Int</span> = {</span><br><span class="line">    lock synchronized {</span><br><span class="line">        <span class="comment">// 检查当前 Log 中的 LogSegment 是否满足删除条件，并返回需要被删除的 LogSegment 对象集合</span></span><br><span class="line">        <span class="keyword">val</span> deletable = <span class="keyword">this</span>.deletableSegments(predicate)</span><br><span class="line">        <span class="keyword">val</span> numToDelete = deletable.size</span><br><span class="line">        <span class="keyword">if</span> (numToDelete &gt; <span class="number">0</span>) {</span><br><span class="line">            <span class="comment">// 如果当前 Log 中所有的 LogSegment 对象都需要被删除，则在删除之前创建一个新的 activeSegment 对象，保证 Log 可以正常运行</span></span><br><span class="line">            <span class="keyword">if</span> (segments.size == numToDelete) <span class="keyword">this</span>.roll()</span><br><span class="line">            <span class="comment">// 遍历删除需要删除的 LogSegment 对象及其相关数据文件</span></span><br><span class="line">            deletable.foreach(deleteSegment)</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 返回被删除的 LogSegment 数目</span></span><br><span class="line">        numToDelete</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果本次删除操作需要删除 Log 中全部的 LogSegment 对象，则会调用 <code>Log#roll</code> 方法为当前 Log 对象的 SkipList 创建一个新的 activeSegment 对象，以保证 Log 的正常运行，该方法的实现在前面已经分析过，不再重复撰述。</p>
<p>再来看一下周期性 <strong>任务 2</strong> ，该任务用于定期对日志文件执行刷盘（flush）操作。相关逻辑实现位于 <code>LogManager#flushDirtyLogs</code> 方法中，该方法会遍历处理每个 topic 分区对应的 Log 对象，通过记录在 Log 对象中的上次执行 flush 的时间戳与当前时间对比，如果时间差值超过一定的阈值（对应 <code>flush.ms</code> 配置），则调用 <code>Log#flush</code> 方法执行刷盘操作，该方法的实现同样在前面已经分析过，不再重复撰述。</p>
<p>接着来看一下周期性 <strong>任务 3</strong> ，该任务用于定期更新每个 log 目录名下的 recovery-point-offset-checkpoint 文件。相关实现位于 <code>LogManager#checkpointRecoveryPointOffsets</code> 中：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpointRecoveryPointOffsets</span></span>() {</span><br><span class="line">    <span class="comment">// 为每个 log 目录应用 checkpointLogsInDir 方法</span></span><br><span class="line">    logDirs.foreach(checkpointLogsInDir)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkpointLogsInDir</span></span>(dir: <span class="type">File</span>): <span class="type">Unit</span> = {</span><br><span class="line">    <span class="comment">// 获取指定 log 目录对应的 Map[TopicPartition, Log] 集合</span></span><br><span class="line">    <span class="keyword">val</span> recoveryPoints = logsByDir.get(dir.toString)</span><br><span class="line">    <span class="keyword">if</span> (recoveryPoints.isDefined) {</span><br><span class="line">        <span class="comment">// 更新对应的 recovery-point-offset-checkpoint 文件</span></span><br><span class="line">        <span class="keyword">this</span>.recoveryPointCheckpoints(dir).write(recoveryPoints.get.mapValues(_.recoveryPoint))</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>方法会获取位于指定 log 目录下所有 topic 分区对应的 recoveryPoint 值（即当前已经落盘的日志的最大 offset），并全量更新 log 目录下的 recovery-point-offset-checkpoint 文件。</p>

        <h4 id="重复日志数据清理">
          <a href="#重复日志数据清理" class="heading-link"><i class="fas fa-link"></i></a>重复日志数据清理</h4>
      <p>本小节来看一下 LogCleaner 线程，如果在配置中指定了 <code>log.cleaner.enable=true</code>，那么在 <code>LogManager#startup</code> 方法的最后会调用 <code>LogCleaner#startup</code> 方法启动 LogCleaner 线程对日志数据执行清理工作。前面我们在分析周期性任务 kafka-log-retention 时，已经知道该周期性任务会对日志中过大或过期的 LogSegment 对象执行清理操作，那么 LogCleaner 又是对什么执行清理呢？</p>
<p>我们知道 Kafka 对于生产者发来的消息都是顺序追加到日志文件中的，而 Kafka 又采用本地文件系统对日志文件进行存储，所以随着时间的流逝日志文件会越来越大，其中存储的相当一部分消息数据都具备相同的 key。如果配置了 <code>cleanup.policy=compact</code> 策略，那么 Kafka 的 LogCleaner 线程就会对具备相同 key 的消息进行清理操作，仅保留当前具备最大 offset 的 key 的消息。</p>
<p>LogCleaner 在执行清理操作时会将一个 log 分割成 clean 和 dirty 两部分。其中 clean 是上次完成清理的部分，Kafka 会在对应 log 目录下生成一个 cleaner-offset-checkpoint 文件，用于记录每个 topic 分区上一次执行清理操作的 offset 值，而 dirty 部分则是本次清理操作的目标区域，但是 dirty 中并不是所有的 LogSegment 对象都会执行清理操作，Kafka 又将这一部分分为了 cleanable 和 uncleanable 两块，能够被分为 uncleanable 的 LogSegment 对象包含两类：</p>
<ol>
<li>当前 Log 对象中的 activeSegment 对象。</li>
<li>LogSegment 对象中的最大消息时间戳距离当前时间位于配置的滞后压缩时间（对应 <code>min.compaction.lag.ms</code> 配置）范围内。</li>
</ol>
<p>其中不清理 activeSegment 对象，主要是为了防止竞态条件，因为 activeSegment 是可以写入的对象，这样会让清理操作变得复杂，且收益不大。</p>
<p>下面我们从 <code>LogCleaner#startup</code> 方法开始，整个清理工作主要涉及 LogCleaner、LogCleanerManager、CleanerThread，以及 Cleaner 这 4 个类。方法 <code>LogCleaner#startup</code> 的主要作用就是启动注册在 LogCleaner 中的 CleanerThread 线程集合。CleanerThread 继承自 ShutdownableThread 抽象类，所以 <code>CleanerThread#doWork</code> 方法是其处理入口，该方法只是简单调用了 <code>CleanerThread#cleanOrSleep</code> 方法，后者会选取一个最需要被清理的 LogSegment 区间，并执行清理工作。相关实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">cleanOrSleep</span></span>() {</span><br><span class="line">    <span class="comment">// 选取下一个最需要进行日志清理的 LogToClean 对象</span></span><br><span class="line">    <span class="keyword">val</span> cleaned = cleanerManager.grabFilthiestCompactedLog(time) <span class="keyword">match</span> {</span><br><span class="line">        <span class="comment">// 没有需要被清理的 LogToClean 对象，休息一会后继续尝试</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            <span class="literal">false</span></span><br><span class="line">        <span class="comment">// 执行消息清理操作</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(cleanable) =&gt;</span><br><span class="line">            <span class="keyword">var</span> endOffset = cleanable.firstDirtyOffset</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                <span class="comment">// 调用 Cleaner#clean 方法执行清理工作</span></span><br><span class="line">                <span class="keyword">val</span> (nextDirtyOffset, cleanerStats) = cleaner.clean(cleanable)</span><br><span class="line">                recordStats(cleaner.id, cleanable.log.name, cleanable.firstDirtyOffset, endOffset, cleanerStats)</span><br><span class="line">                endOffset = nextDirtyOffset</span><br><span class="line">            } <span class="keyword">catch</span> {</span><br><span class="line">                <span class="keyword">case</span> _: <span class="type">LogCleaningAbortedException</span> =&gt; <span class="comment">// task can be aborted, let it go.</span></span><br><span class="line">            } <span class="keyword">finally</span> {</span><br><span class="line">                <span class="comment">// 对 Log 的清理状态进行转换，如果当前 topic 分区的清理状态是 LogCleaningInProgress，则更新 cleaner-offset-checkpoint 文件</span></span><br><span class="line">                cleanerManager.doneCleaning(cleanable.topicPartition, cleanable.log.dir.getParentFile, endOffset)</span><br><span class="line">            }</span><br><span class="line">            <span class="literal">true</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取所有启用了 compact 和 delete 清理策略的 Log 对象，并将其对应的 topic 分区状态设置为 LogCleaningInProgress</span></span><br><span class="line">    <span class="keyword">val</span> deletable: <span class="type">Iterable</span>[(<span class="type">TopicPartition</span>, <span class="type">Log</span>)] = cleanerManager.deletableLogs()</span><br><span class="line">    deletable.foreach {</span><br><span class="line">        <span class="keyword">case</span> (topicPartition, log) =&gt;</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                <span class="comment">// 对设置了清理策略为 delete 的 LogSegment 执行删除操作，删除过期或过大的 LogSegment 对象。</span></span><br><span class="line">                log.deleteOldSegments()</span><br><span class="line">            } <span class="keyword">finally</span> {</span><br><span class="line">                <span class="comment">// 移除这些 topic 分区对应的 LogCleaningInProgress 状态</span></span><br><span class="line">                cleanerManager.doneDeleting(topicPartition)</span><br><span class="line">            }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果没有需要执行清理的 LogToClean 对象，则休息一会后继续重试</span></span><br><span class="line">    <span class="keyword">if</span> (!cleaned) backOffWaitLatch.await(config.backOffMs, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>清理操作的执行流程如下：</p>
<ol>
<li>选取一个最需要进行日志清理的 LogToClean 对象，如果存在则执行清理操作；</li>
<li>如果配置了 <code>cleanup.policy=delete</code> 策略，则对 Log 对象中过大或过期的 LogSegment 对象执行删除操作；</li>
<li>如果没有需要进行清理的 LogToClean 对象，则休息一会儿后重试。</li>
</ol>
<p>其中第 2 步与前面介绍的周期性任务 kafka-log-retention 类似，这里我们重点来看一下第 1 步，这一步的核心操作是调用 <code>LogCleanerManager#grabFilthiestCompactedLog</code> 方法选取下一个最需要被清理的 LogToClean 对象，然后调用 <code>Cleaner#clean</code> 依据该对象执行清理操作。</p>
<p>方法 <code>LogCleanerManager#grabFilthiestCompactedLog</code> 的实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabFilthiestCompactedLog</span></span>(time: <span class="type">Time</span>): <span class="type">Option</span>[<span class="type">LogToClean</span>] = {</span><br><span class="line">    inLock(lock) {</span><br><span class="line">        <span class="keyword">val</span> now = time.milliseconds</span><br><span class="line">        <span class="keyword">this</span>.timeOfLastRun = now</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取 log 目录下的 cleaner-offset-checkpoint 文件，获取每个 topic 分区上次清理操作的 offset 边界</span></span><br><span class="line">        <span class="keyword">val</span> lastClean = allCleanerCheckpoints</span><br><span class="line">        <span class="keyword">val</span> dirtyLogs = logs.filter {</span><br><span class="line">            <span class="comment">// 过滤掉 cleanup.policy 配置为 delete 的 Log 对象，因为不需要压缩</span></span><br><span class="line">            <span class="keyword">case</span> (_, log) =&gt; log.config.compact <span class="comment">// match logs that are marked as compacted</span></span><br><span class="line">        }.filterNot {</span><br><span class="line">            <span class="comment">// 过滤掉所有正在执行清理工作的 Log 对象</span></span><br><span class="line">            <span class="keyword">case</span> (topicPartition, _) =&gt; inProgress.contains(topicPartition) <span class="comment">// skip any logs already in-progress</span></span><br><span class="line">        }.map {</span><br><span class="line">            <span class="comment">// 将需要被清理的区间封装成 LogToClean 对象</span></span><br><span class="line">            <span class="keyword">case</span> (topicPartition, log) =&gt; <span class="comment">// create a LogToClean instance for each</span></span><br><span class="line">                <span class="comment">// 计算需要执行清理操作的 offset 区间</span></span><br><span class="line">                <span class="keyword">val</span> (firstDirtyOffset, firstUncleanableDirtyOffset) =</span><br><span class="line">                    <span class="type">LogCleanerManager</span>.cleanableOffsets(log, topicPartition, lastClean, now)</span><br><span class="line">                <span class="comment">// 构建清理区间对应的 LogToClean 对象</span></span><br><span class="line">                <span class="type">LogToClean</span>(topicPartition, log, firstDirtyOffset, firstUncleanableDirtyOffset)</span><br><span class="line">        }.filter(ltc =&gt; ltc.totalBytes &gt; <span class="number">0</span>) <span class="comment">// 忽略待清理区间数据为空的 LogToClean 对象</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取待清理区间最大的 cleanableRatio 比率</span></span><br><span class="line">        <span class="keyword">this</span>.dirtiestLogCleanableRatio = <span class="keyword">if</span> (dirtyLogs.nonEmpty) dirtyLogs.max.cleanableRatio <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="comment">// 过滤掉所有 cleanableRatio 小于等于配置值（对应 min.cleanable.dirty.ratio 配置）的 LogToClean 对象</span></span><br><span class="line">        <span class="keyword">val</span> cleanableLogs = dirtyLogs.filter(ltc =&gt; ltc.cleanableRatio &gt; ltc.log.config.minCleanableRatio)</span><br><span class="line">        <span class="keyword">if</span> (cleanableLogs.isEmpty) {</span><br><span class="line">            <span class="type">None</span></span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 基于需要清理的数据占比选择最需要执行清理的 LogToClean 对象</span></span><br><span class="line">            <span class="keyword">val</span> filthiest = cleanableLogs.max</span><br><span class="line">            <span class="comment">// 更新对应 topic 分区的清理状态为 LogCleaningInProgress</span></span><br><span class="line">            inProgress.put(filthiest.topicPartition, <span class="type">LogCleaningInProgress</span>)</span><br><span class="line">            <span class="type">Some</span>(filthiest)</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法的执行流程如下：</p>
<ol>
<li>读取每个 log 目录下 cleaner-offset-checkpoint 文件，解析每个 topic 分区上次执行清理操作对应的 offset 值；</li>
<li>遍历处理 Log 对象集合，筛选符合要求的 Log 对象（Log 对象配置的清理策略 <code>cleanup.policy=compact</code>，且该 Log 对象当前没有正在执行清理操作），并与其需要被清理的区间一起封装成对应的 LogToClean 对象；</li>
<li>过滤掉不包含数据，以及待清理数据占比不超过指定阈值（对应 <code>min.cleanable.dirty.ratio</code> 配置）的 LogToClean 对象，并从剩下的 LogToClean 集合中选择待清理数据占比最高的 LogToClean 对象。</li>
</ol>
<p>计算待清理区间的过程由 <code>LogCleanerManager#cleanableOffsets</code> 方法实现，区间值包括 dirty 部分的起始 offset 值和 uncleanable LogSegment 对象的 baseOffset 值。方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cleanableOffsets</span></span>(log: <span class="type">Log</span>, <span class="comment">// 待清理的 Log 对象</span></span><br><span class="line">                     topicPartition: <span class="type">TopicPartition</span>, <span class="comment">// 对应的 topic 分区对象</span></span><br><span class="line">                     lastClean: immutable.<span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>], <span class="comment">// 记录每个 topic 分区上一次清理操作的结束 offset</span></span><br><span class="line">                     now: <span class="type">Long</span>): (<span class="type">Long</span>, <span class="type">Long</span>) = {</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取当前 topic 分区上次清理的 offset，即下一次需要被清理的 Log 的起始 offset</span></span><br><span class="line">    <span class="keyword">val</span> lastCleanOffset: <span class="type">Option</span>[<span class="type">Long</span>] = lastClean.get(topicPartition)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取当前 Log 对象 SkipList 中首个 LogSegment 对应的 baseOffset</span></span><br><span class="line">    <span class="keyword">val</span> logStartOffset = log.logSegments.head.baseOffset</span><br><span class="line">    <span class="comment">// 计算下一次执行清理操作的起始 offset</span></span><br><span class="line">    <span class="keyword">val</span> firstDirtyOffset = {</span><br><span class="line">        <span class="comment">// 如果 cleaner-offset-checkpoint 中没有当前 topic 分区的相关记录或记录的 offset 小于 logStartOffset，</span></span><br><span class="line">        <span class="comment">// 则以当前 Log 对象 SkipList 中的起始 logStartOffset 作为下一次需要被清理的起始 offset 位置</span></span><br><span class="line">        <span class="keyword">val</span> offset = lastCleanOffset.getOrElse(logStartOffset)</span><br><span class="line">        <span class="keyword">if</span> (offset &lt; logStartOffset) {</span><br><span class="line">            <span class="comment">// don't bother with the warning if compact and delete are enabled.</span></span><br><span class="line">            <span class="keyword">if</span> (!isCompactAndDelete(log))</span><br><span class="line">                warn(<span class="string">s"Resetting first dirty offset to log start offset <span class="subst">$logStartOffset</span> since the checkpointed offset <span class="subst">$offset</span> is invalid."</span>)</span><br><span class="line">            logStartOffset</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            offset</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取需要被清理的 LogSegment 对象，即在 firstDirtyOffset 到 activeSegment 之间的 LogSegment 对象集合</span></span><br><span class="line">    <span class="keyword">val</span> dirtyNonActiveSegments = log.logSegments(firstDirtyOffset, log.activeSegment.baseOffset)</span><br><span class="line">    <span class="comment">// 获取配置的清理滞后时间（对应 min.compaction.lag.ms 配置）</span></span><br><span class="line">    <span class="keyword">val</span> compactionLagMs = math.max(log.config.compactionLagMs, <span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算本次不应该被清理的 LogSegment 对应的最小 offset 值</span></span><br><span class="line">    <span class="keyword">val</span> firstUncleanableDirtyOffset: <span class="type">Long</span> = <span class="type">Seq</span>(</span><br><span class="line">        <span class="comment">// activeSegment 不能执行清理操作，避免竞态条件</span></span><br><span class="line">        <span class="type">Option</span>(log.activeSegment.baseOffset),</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 寻找最大消息时间戳距离当前时间戳在清理滞后时间（compactionLagMs）范围内的 LogSegment 对应的最小 offset 值</span></span><br><span class="line">        <span class="keyword">if</span> (compactionLagMs &gt; <span class="number">0</span>) {</span><br><span class="line">            dirtyNonActiveSegments.find { s =&gt;</span><br><span class="line">                <span class="comment">// 如果 LogSegment 的最大消息时间戳距离当前在 compactionLagMs 范围内，则不能执行清理操作</span></span><br><span class="line">                <span class="keyword">val</span> isUncleanable = s.largestTimestamp &gt; now - compactionLagMs</span><br><span class="line">                debug(<span class="string">s"Checking if log segment may be cleaned: log='<span class="subst">${log.name}</span>' segment.baseOffset=<span class="subst">${s.baseOffset}</span> segment.largestTimestamp=<span class="subst">${s.largestTimestamp}</span>; now - compactionLag=<span class="subst">${now - compactionLagMs}</span>; is uncleanable=<span class="subst">$isUncleanable</span>"</span>)</span><br><span class="line">                isUncleanable</span><br><span class="line">            } map (_.baseOffset)</span><br><span class="line">        } <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">    ).flatten.min</span><br><span class="line"></span><br><span class="line">    debug(<span class="string">s"Finding range of cleanable offsets for log=<span class="subst">${log.name}</span> topicPartition=<span class="subst">$topicPartition</span>. Last clean offset=<span class="subst">$lastCleanOffset</span> now=<span class="subst">$now</span> =&gt; firstDirtyOffset=<span class="subst">$firstDirtyOffset</span> firstUncleanableOffset=<span class="subst">$firstUncleanableDirtyOffset</span> activeSegment.baseOffset=<span class="subst">${log.activeSegment.baseOffset}</span>"</span>)</span><br><span class="line"></span><br><span class="line">    (firstDirtyOffset, firstUncleanableDirtyOffset)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>清理区间的起始 offset，即 firstDirtyOffset，一般都对应着 cleaner-offset-checkpoint 文件中记录的上次执行清理操作的结束 offset，但是考虑到当前 topic 分区可能是第一次执行清理操作，或者 offset 对应的 LogSegment 可能已经被删除，所以需要将其与当前 Log 对象的首个 LogSegment 的 baseOffset 进行对比，选择较大值。</p>
<p>清理区间的结束 offset，即 firstUncleanableDirtyOffset，也就是 uncleanable 区间的起始 offset，我们在前面介绍了 uncleanable 区间包含 2 类 LogSegment 对象，即 activeSegment 对象和最大消息时间戳距离当前时间位于配置的滞后压缩时间范围内的 LogSegment 对象，在计算 firstUncleanableDirtyOffset 时，也就是从这两类 LogSegment 中寻找最小的 baseOffset 作为清理区间的结束 offset 值。</p>
<p>下面来看一下 <code>Cleaner#clean</code> 方法，清理操作的具体执行过程正位于此，方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">clean</span></span>(cleanable: <span class="type">LogToClean</span>): (<span class="type">Long</span>, <span class="type">CleanerStats</span>) = {</span><br><span class="line">    <span class="comment">// 记录消息清理的状态信息</span></span><br><span class="line">    <span class="keyword">val</span> stats = <span class="keyword">new</span> <span class="type">CleanerStats</span>()</span><br><span class="line"></span><br><span class="line">    info(<span class="string">"Beginning cleaning of log %s."</span>.format(cleanable.log.name))</span><br><span class="line">    <span class="keyword">val</span> log = cleanable.log <span class="comment">// 需要被清理的 Log 对象</span></span><br><span class="line"></span><br><span class="line">    info(<span class="string">"Building offset map for %s..."</span>.format(cleanable.log.name))</span><br><span class="line">    <span class="comment">// 清理操作的 offset 上界</span></span><br><span class="line">    <span class="keyword">val</span> upperBoundOffset = cleanable.firstUncleanableOffset</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 遍历处理待清理区间的 LogSegment 对象，填充 offsetMap 对象，主要记录每个消息 key 及其对应清理区间内的最大 offset 值</span></span><br><span class="line">    <span class="keyword">this</span>.buildOffsetMap(log, cleanable.firstDirtyOffset, upperBoundOffset, offsetMap, stats)</span><br><span class="line">    <span class="keyword">val</span> endOffset = offsetMap.latestOffset + <span class="number">1</span></span><br><span class="line">    stats.indexDone()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 计算删除标识</span></span><br><span class="line">    <span class="keyword">val</span> deleteHorizonMs = log.logSegments(<span class="number">0</span>, cleanable.firstDirtyOffset).lastOption <span class="keyword">match</span> {</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="number">0</span>L</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(seg) =&gt; seg.lastModified - log.config.deleteRetentionMs <span class="comment">// delete.retention.ms</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// determine the timestamp up to which the log will be cleaned，this is the lower of the last active segment and the compaction lag</span></span><br><span class="line">    <span class="keyword">val</span> cleanableHorizonMs = log.logSegments(<span class="number">0</span>, cleanable.firstUncleanableOffset).lastOption.map(_.lastModified).getOrElse(<span class="number">0</span>L)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 对 [0, endOffset) 区间的 LogSegment 进行分组，并以组为单位执行清理操作</span></span><br><span class="line">    info(<span class="string">"Cleaning log %s (cleaning prior to %s, discarding tombstones prior to %s)..."</span>.format(log.name, <span class="keyword">new</span> <span class="type">Date</span>(cleanableHorizonMs), <span class="keyword">new</span> <span class="type">Date</span>(deleteHorizonMs)))</span><br><span class="line">    <span class="keyword">for</span> (group &lt;- <span class="keyword">this</span>.groupSegmentsBySize(log.logSegments(<span class="number">0</span>, endOffset), log.config.segmentSize, log.config.maxIndexSize, cleanable.firstUncleanableOffset))</span><br><span class="line">        <span class="keyword">this</span>.cleanSegments(log, group, offsetMap, deleteHorizonMs, stats)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// record buffer utilization</span></span><br><span class="line">    stats.bufferUtilization = offsetMap.utilization</span><br><span class="line"></span><br><span class="line">    stats.allDone()</span><br><span class="line"></span><br><span class="line">    (endOffset, stats)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>整个清理过程中我们重点关注一下 offsetMap 的填充过程和分组清理数据的过程，这里的 offsetMap 是一个 Kafka 自定义实现的 SkimpyOffsetMap 类型，其中主要记录了每个消息的 key 和消息在清理区间的最大 offset 值的映射关系，后面需要依据该 offsetMap 来确定需要剔除和保留的消息。填充 offsetMap 的过程位于 <code>Cleaner#buildOffsetMap</code> 方法中，实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">buildOffsetMap</span></span>(log: <span class="type">Log</span>, <span class="comment">// 待清理的 Log 对象</span></span><br><span class="line">                                start: <span class="type">Long</span>, <span class="comment">// 清理区间起始 offset</span></span><br><span class="line">                                end: <span class="type">Long</span>, <span class="comment">// 清理区间结束 offset</span></span><br><span class="line">                                map: <span class="type">OffsetMap</span>, <span class="comment">// 记录消息 key 及其对应的最大 offset</span></span><br><span class="line">                                stats: <span class="type">CleanerStats</span>) {</span><br><span class="line">    map.clear()</span><br><span class="line">    <span class="comment">// 获取 [start, end) 之间的 LogSegment 对象，这些对象是本次需要执行清理操作的</span></span><br><span class="line">    <span class="keyword">val</span> dirty = log.logSegments(start, end).toBuffer</span><br><span class="line">    info(<span class="string">"Building offset map for log %s for %d segments in offset range [%d, %d)."</span>.format(log.name, dirty.size, start, end))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> full = <span class="literal">false</span> <span class="comment">// 标识 map 是否被填充满了</span></span><br><span class="line">    <span class="keyword">for</span> (segment &lt;- dirty <span class="keyword">if</span> !full) {</span><br><span class="line">        <span class="comment">// 检查当前分区的压缩状态，确保不是 LogCleaningAborted 状态</span></span><br><span class="line">        <span class="keyword">this</span>.checkDone(log.topicPartition)</span><br><span class="line">        <span class="comment">// 处理当前 LogSegment 中的消息集合，以消息的 key 作为 key，以遍历范围内最大 offset 作为 value，填充 offsetMap</span></span><br><span class="line">        full = <span class="keyword">this</span>.buildOffsetMapForSegment(log.topicPartition, segment, map, start, log.config.maxMessageSize, stats)</span><br><span class="line">    }</span><br><span class="line">    info(<span class="string">"Offset map for log %s complete."</span>.format(log.name))</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">buildOffsetMapForSegment</span></span>(topicPartition: <span class="type">TopicPartition</span>,</span><br><span class="line">                                     segment: <span class="type">LogSegment</span>,</span><br><span class="line">                                     map: <span class="type">OffsetMap</span>,</span><br><span class="line">                                     start: <span class="type">Long</span>,</span><br><span class="line">                                     maxLogMessageSize: <span class="type">Int</span>,</span><br><span class="line">                                     stats: <span class="type">CleanerStats</span>): <span class="type">Boolean</span> = {</span><br><span class="line">    <span class="comment">// 获取清理区间起始 offset 对应的消息物理地址</span></span><br><span class="line">    <span class="keyword">var</span> position = segment.index.lookup(start).position</span><br><span class="line">    <span class="comment">// 计算当前 map 的最大容量</span></span><br><span class="line">    <span class="keyword">val</span> maxDesiredMapSize = (map.slots * dupBufferLoadFactor).toInt</span><br><span class="line">    <span class="comment">// 遍历处理 LogSegment 对象中的消息</span></span><br><span class="line">    <span class="keyword">while</span> (position &lt; segment.log.sizeInBytes) {</span><br><span class="line">        <span class="comment">// 再次校验当前分区的状态，确保不是 LogCleaningAborted 状态</span></span><br><span class="line">        <span class="keyword">this</span>.checkDone(topicPartition)</span><br><span class="line">        readBuffer.clear()</span><br><span class="line">        <span class="comment">// 读取消息集合</span></span><br><span class="line">        segment.log.readInto(readBuffer, position)</span><br><span class="line">        <span class="keyword">val</span> records = <span class="type">MemoryRecords</span>.readableRecords(readBuffer)</span><br><span class="line">        throttler.maybeThrottle(records.sizeInBytes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> startPosition = position</span><br><span class="line">        <span class="comment">// 深层迭代遍历消息集合</span></span><br><span class="line">        <span class="keyword">for</span> (entry &lt;- records.deepEntries.asScala) {</span><br><span class="line">            <span class="keyword">val</span> message = entry.record</span><br><span class="line">            <span class="comment">// 仅处理具备 key，且 offset 位于 start 之后的消息</span></span><br><span class="line">            <span class="keyword">if</span> (message.hasKey &amp;&amp; entry.offset &gt;= start) {</span><br><span class="line">                <span class="comment">// 如果 map 未满，将消息的 key 及其 offset 放入 map 中，这里会覆盖 offset 较小的 key</span></span><br><span class="line">                <span class="keyword">if</span> (map.size &lt; maxDesiredMapSize) map.put(message.key, entry.offset)</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">true</span> <span class="comment">// 标识 map 已满</span></span><br><span class="line">            }</span><br><span class="line">            stats.indexMessagesRead(<span class="number">1</span>)</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">val</span> bytesRead = records.validBytes</span><br><span class="line">        <span class="comment">// 向前移动地址</span></span><br><span class="line">        position += bytesRead</span><br><span class="line">        stats.indexBytesRead(bytesRead)</span><br><span class="line">        <span class="comment">// 如果 position 未向前移动，则说明未读取到一个完整的消息，需要对 buffer 进行扩容</span></span><br><span class="line">        <span class="keyword">if</span> (position == startPosition) <span class="keyword">this</span>.growBuffers(maxLogMessageSize)</span><br><span class="line">    } <span class="comment">// ~ end while</span></span><br><span class="line">    <span class="comment">// 重置 buffer</span></span><br><span class="line">    <span class="keyword">this</span>.restoreBuffers()</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>填充的过程比较直观，上述方法会遍历清理区间的消息集合直到 offsetMap 被填满或到达区间边界为止，并在遍历过程中将持有 key 的消息及其 offset 添加到 offsetMap 中，因为消息是顺序追加的，所以能够保证 offsetMap 中记录的是当前已处理消息的对应的最大 <code>key-&gt;offset</code> 映射。</p>
<p>完成了 offsetMap 的填充，接下来方法会依据单个 LogSegment 对象和索引文件的大小上限对需要清理的 LogSegment 对象进行分组，以防止清理操作完成后生成的目标 LogSegment 对象过大或过小，保证尽量均衡。然后方法会遍历每个分组，对分组中的待清理 LogSegment 对象集合调用 <code>Cleaner#cleanSegments</code> 方法执行清理操作并生成最终的 LogSegment 对象替换清理操作前的 LogSegment 对象集合。方法的实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">cleanSegments</span></span>(log: <span class="type">Log</span>,</span><br><span class="line">                               segments: <span class="type">Seq</span>[<span class="type">LogSegment</span>],</span><br><span class="line">                               map: <span class="type">OffsetMap</span>,</span><br><span class="line">                               deleteHorizonMs: <span class="type">Long</span>,</span><br><span class="line">                               stats: <span class="type">CleanerStats</span>) {</span><br><span class="line">    <span class="comment">// 创建组内第一个 LogSegment 对象的 log 文件对应的“.cleaned”文件</span></span><br><span class="line">    <span class="keyword">val</span> logFile = <span class="keyword">new</span> <span class="type">File</span>(segments.head.log.file.getPath + <span class="type">Log</span>.<span class="type">CleanedFileSuffix</span>)</span><br><span class="line">    logFile.delete()</span><br><span class="line">    <span class="comment">// 创建 index 文件对应的“.cleaned”文件</span></span><br><span class="line">    <span class="keyword">val</span> indexFile = <span class="keyword">new</span> <span class="type">File</span>(segments.head.index.file.getPath + <span class="type">Log</span>.<span class="type">CleanedFileSuffix</span>)</span><br><span class="line">    <span class="comment">// 创建 timeindex 文件对应的“.cleaned”文件</span></span><br><span class="line">    <span class="keyword">val</span> timeIndexFile = <span class="keyword">new</span> <span class="type">File</span>(segments.head.timeIndex.file.getPath + <span class="type">Log</span>.<span class="type">CleanedFileSuffix</span>)</span><br><span class="line">    indexFile.delete()</span><br><span class="line">    timeIndexFile.delete()</span><br><span class="line">    <span class="keyword">val</span> records = <span class="type">FileRecords</span>.open(logFile, <span class="literal">false</span>, log.initFileSize(), log.config.preallocate)</span><br><span class="line">    <span class="keyword">val</span> index = <span class="keyword">new</span> <span class="type">OffsetIndex</span>(indexFile, segments.head.baseOffset, segments.head.index.maxIndexSize)</span><br><span class="line">    <span class="keyword">val</span> timeIndex = <span class="keyword">new</span> <span class="type">TimeIndex</span>(timeIndexFile, segments.head.baseOffset, segments.head.timeIndex.maxIndexSize)</span><br><span class="line">    <span class="comment">// 创建清理后数据对应的 LogSegment 对象</span></span><br><span class="line">    <span class="keyword">val</span> cleaned = <span class="keyword">new</span> <span class="type">LogSegment</span>(records, index, timeIndex,</span><br><span class="line">        segments.head.baseOffset, segments.head.indexIntervalBytes, log.config.randomSegmentJitter, time)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 遍历处理需要清理的 LogSegment 对象，将清理后的数据记录到 cleaned 文件中</span></span><br><span class="line">        <span class="keyword">for</span> (old &lt;- segments) {</span><br><span class="line">            <span class="keyword">val</span> retainDeletes = old.lastModified &gt; deleteHorizonMs</span><br><span class="line">            info(<span class="string">"Cleaning segment %s in log %s (largest timestamp %s) into %s, %s deletes."</span></span><br><span class="line">                    .format(old.baseOffset, log.name, <span class="keyword">new</span> <span class="type">Date</span>(old.largestTimestamp), cleaned.baseOffset, <span class="keyword">if</span> (retainDeletes) <span class="string">"retaining"</span> <span class="keyword">else</span> <span class="string">"discarding"</span>))</span><br><span class="line">            <span class="keyword">this</span>.cleanInto(log.topicPartition, old, cleaned, map, retainDeletes, log.config.maxMessageSize, stats)</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对 index 文件进行截断，剔除无效的字节</span></span><br><span class="line">        index.trimToValidSize()</span><br><span class="line">        <span class="comment">// 对 timeindex 文件进行截断，剔除无效的字节</span></span><br><span class="line">        cleaned.onBecomeInactiveSegment()</span><br><span class="line">        timeIndex.trimToValidSize()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将 LogSegment 对象相关的文件刷盘</span></span><br><span class="line">        cleaned.flush()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// update the modification date to retain the last modified date of the original files</span></span><br><span class="line">        <span class="keyword">val</span> modified = segments.last.lastModified</span><br><span class="line">        cleaned.lastModified = modified</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用清理后的 LogSegment 对象替换清理之前的 LogSegment 对象集合</span></span><br><span class="line">        info(<span class="string">"Swapping in cleaned segment %d for segment(s) %s in log %s."</span>.format(cleaned.baseOffset, segments.map(_.baseOffset).mkString(<span class="string">","</span>), log.name))</span><br><span class="line">        log.replaceSegments(cleaned, segments)</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">LogCleaningAbortedException</span> =&gt;</span><br><span class="line">            cleaned.delete()</span><br><span class="line">            <span class="keyword">throw</span> e</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>整个清理操作的执行流程可以概括如下：</p>
<ol>
<li>创建存储清理后数据的 LogSegment 对象，及其对应的 log、index 和 timeindex 文件，注意此时的文件都以 “.cleaned” 作为后缀；</li>
<li>遍历处理待清理 LogSegment 对象集合，对每个需要清理的 LogSegment 对象调用 <code>Cleaner#cleanInto</code> 方法执行清理操作，并将清理后的数据写入步骤 1 中创建的 LogSegment 对象中；</li>
<li>对 index 和 timeindex 文件进行截断，剔除无效字节；</li>
<li>将存储清理后数据的 LogSegment 对象相关文件进行刷盘；</li>
<li>使用存储清理后数据的 LogSegment 对象替换 SkipList 中对应的被清理之前的 LogSegment 对象集合。</li>
</ol>
<p>其中步骤 1 中创建的相关文件均以“.cleaned”作为文件名后缀，并在步骤 4 中将内存中的日志和索引数据落盘到对应文件中，而步骤 5 中除了会使用存储清理后数据的 LogSegment 对象替换 SkipList 中对应的被清理之前的 LogSegment 对象集合之外，还会将相关文件的后缀名由“.cleaned”改为“.swap”，并在完成剔除存储被清理之前数据的 LogSegment 对象集合后，移除文件的“.swap”后缀。前面我们在分析 <code>Log#loadSegments</code> 方法时曾说，如果当前 topic 分区目录下的 log 文件是以“.swap”作为后缀的，那么其中的数据是完整的，只是 broker 节点在执行交换（即移除“.swap”后缀）的过程中宕机了，再次加载时可以直接移除“.swap”后缀并加载，无需担心数据错乱或丢失，分析到这里应该对 broker 节点启动时加载数据文件的过程有更加深入的理解。</p>
<p>下面我们主要来看一下 <code>Cleaner#cleanInto</code> 方法的实现，分析清理操作的具体执行过程，方法实现如下：</p>
<figure class="highlight scala"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[log] <span class="function"><span class="keyword">def</span> <span class="title">cleanInto</span></span>(topicPartition: <span class="type">TopicPartition</span>, <span class="comment">// 当前操作的 Log 对应的 topic 分区对象</span></span><br><span class="line">                           source: <span class="type">LogSegment</span>, <span class="comment">// 需要被清理的 LogSegment</span></span><br><span class="line">                           dest: <span class="type">LogSegment</span>, <span class="comment">// 清理后得到 LogSegment</span></span><br><span class="line">                           map: <span class="type">OffsetMap</span>, <span class="comment">// offsetMap</span></span><br><span class="line">                           retainDeletes: <span class="type">Boolean</span>, <span class="comment">// source.lastModified &gt; deleteHorizonMs，当删除对应的 LogSegment 时，删除标记是否应该被保留</span></span><br><span class="line">                           maxLogMessageSize: <span class="type">Int</span>,</span><br><span class="line">                           stats: <span class="type">CleanerStats</span>) {</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义消息过滤器</span></span><br><span class="line">    <span class="keyword">val</span> logCleanerFilter = <span class="keyword">new</span> <span class="type">LogEntryFilter</span> {</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">shouldRetain</span></span>(logEntry: <span class="type">LogEntry</span>): <span class="type">Boolean</span> = shouldRetainMessage(source, map, retainDeletes, logEntry, stats)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> position = <span class="number">0</span></span><br><span class="line">    <span class="comment">// 遍历处理待清理的 LogSegment 对象中的消息</span></span><br><span class="line">    <span class="keyword">while</span> (position &lt; source.log.sizeInBytes) {</span><br><span class="line">        <span class="comment">// 校验对应 topic 分区的清理状态不为 LogCleaningAborted</span></span><br><span class="line">        <span class="keyword">this</span>.checkDone(topicPartition)</span><br><span class="line">        <span class="comment">// read a chunk of messages and copy any that are to be retained to the write buffer to be written out</span></span><br><span class="line">        readBuffer.clear()</span><br><span class="line">        writeBuffer.clear()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 读取消息到 buffer</span></span><br><span class="line">        source.log.readInto(readBuffer, position)</span><br><span class="line">        <span class="keyword">val</span> records = <span class="type">MemoryRecords</span>.readableRecords(readBuffer)</span><br><span class="line">        throttler.maybeThrottle(records.sizeInBytes)</span><br><span class="line">        <span class="comment">// 对消息进行过滤，对需要保留的消息写入到 buffer 中</span></span><br><span class="line">        <span class="keyword">val</span> result = records.filterTo(topicPartition, logCleanerFilter, writeBuffer, maxLogMessageSize)</span><br><span class="line">        stats.readMessages(result.messagesRead, result.bytesRead)</span><br><span class="line">        stats.recopyMessages(result.messagesRetained, result.bytesRetained)</span><br><span class="line"></span><br><span class="line">        position += result.bytesRead</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 对于需要保留的消息，将其追加到清理后的 LogSegment 对象中</span></span><br><span class="line">        <span class="keyword">val</span> outputBuffer = result.output</span><br><span class="line">        <span class="keyword">if</span> (outputBuffer.position &gt; <span class="number">0</span>) {</span><br><span class="line">            outputBuffer.flip()</span><br><span class="line">            <span class="keyword">val</span> retained = <span class="type">MemoryRecords</span>.readableRecords(outputBuffer)</span><br><span class="line">            dest.append(</span><br><span class="line">                firstOffset = retained.deepEntries.iterator.next().offset,</span><br><span class="line">                largestOffset = result.maxOffset,</span><br><span class="line">                largestTimestamp = result.maxTimestamp,</span><br><span class="line">                shallowOffsetOfMaxTimestamp = result.shallowOffsetOfMaxTimestamp,</span><br><span class="line">                records = retained)</span><br><span class="line">            throttler.maybeThrottle(outputBuffer.limit)</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果未能读取一条完整的消息，则需要对 buffer 进行扩容</span></span><br><span class="line">        <span class="keyword">if</span> (readBuffer.limit &gt; <span class="number">0</span> &amp;&amp; result.messagesRead == <span class="number">0</span>) growBuffers(maxLogMessageSize)</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 对 buffer 进行重置</span></span><br><span class="line">    <span class="keyword">this</span>.restoreBuffers()</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法会深层遍历待清理 LogSegment 对象中的每一条消息，并调用 <code>MemoryRecords#filterTo</code> 对消息执行过滤操作，保留同时满足以下条件的消息：</p>
<ol>
<li>消息必须具备 key，且 key 包含在 offsetMap 中；</li>
<li>消息的 offset 要大于等于 offsetMap 中记录的对应的 offset 值；</li>
<li>如果对应的消息是删除标记，只有在允许保留该标记是才会保留。</li>
</ol>
<p>上述条件对应方法 <code>Cleaner#shouldRetainMessage</code> 实现，这里不再展开。在完成对一个消息集合的筛选操作之后，如果所有的消息均需要被保留，则只需要将消息集合写入到目标 buffer 中即可。否则，如果只有部分消息需要被保留，则需要对这部分保留的消息重新压缩（如果需要的话），然后写入目标 buffer 中。</p>

        <h3 id="总结">
          <a href="#总结" class="heading-link"><i class="fas fa-link"></i></a>总结</h3>
      <p>本文我们按照日志数据的组织结构由下往上分析了 LogSegment、Log 和 LogManager 组件，了解了 Kafka 的日志存储机制，其中 Log 用于存储和管理一个 topic 分区下的所有有效的消息数据，并将消息及其索引数据分片采用 LogSegment 对象进行管理。LogManager 实现了 4 个周期性任务分别用于对日志和索引数据执行定期清理、删除、刷盘，以及记录 HW 等操作，同时还维护了一个清理线程对具备相同 key 的重复消息数据进行清理，以减少对磁盘空间的无用消耗。LogManager 并没有提供对日志数据的读写操作，而是委托给相应 topic 分区的 Log 对象执行。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://plotor.github.io">zhenchao</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://plotor.github.io/2019/06/22/kafka/kafka-log-manage/">https://plotor.github.io/2019/06/22/kafka/kafka-log-manage/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://plotor.github.io/tags/Kafka/">Kafka</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2019/06/23/kafka/kafka-purgatory/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Kafka 源码解析：延时任务调度策略</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2019/06/21/kafka/kafka-reactor/"><span class="paginator-prev__text">Kafka 源码解析：网络交互模型</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="utterances-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#LogSegment-%E7%BB%84%E4%BB%B6"><span class="toc-number">1.</span> <span class="toc-text">
          LogSegment 组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%BD%E5%8A%A0%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.</span> <span class="toc-text">
          追加日志数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE"><span class="toc-number">1.2.</span> <span class="toc-text">
          读取日志数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E5%BB%BA%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.</span> <span class="toc-text">
          重建索引数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Log-%E7%BB%84%E4%BB%B6"><span class="toc-number">2.</span> <span class="toc-text">
          Log 组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8A%A0%E8%BD%BD%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.</span> <span class="toc-text">
          初始化加载日志数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%BD%E5%8A%A0%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE-1"><span class="toc-number">2.2.</span> <span class="toc-text">
          追加日志数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-Active-Segment-%E5%AF%B9%E8%B1%A1"><span class="toc-number">2.3.</span> <span class="toc-text">
          创建 Active Segment 对象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE-1"><span class="toc-number">2.4.</span> <span class="toc-text">
          读取日志数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE"><span class="toc-number">2.5.</span> <span class="toc-text">
          删除日志数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LogManager-%E7%BB%84%E4%BB%B6"><span class="toc-number">3.</span> <span class="toc-text">
          LogManager 组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%91%A8%E6%9C%9F%E6%80%A7%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1"><span class="toc-number">3.1.</span> <span class="toc-text">
          周期性定时任务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E5%A4%8D%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%E6%B8%85%E7%90%86"><span class="toc-number">3.2.</span> <span class="toc-text">
          重复日志数据清理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">
          总结</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/author.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">追求技术深度，注重文章质量</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/plotor" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://weibo.com/" target="_blank" rel="noopener" data-popover="微博" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weibo"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="微信" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weixin"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="QQ" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-qq"></i></span></a><a class="sidebar-ov-social-item" href="https://twitter.com/" target="_blank" rel="noopener" data-popover="Twitter" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-twitter"></i></span></a></div><div class="sidebar-ov-feed"><span class="sidebar-ov-feed-rss"><a class="sidebar-ov-feed-rss__link" href="/atom.xml" target="_blank" rel="noopener"><span class="sidebar-ov-feed-rss__icon"><i class="fas fa-rss"></i></span><span>RSS 订阅</span></a></span></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">95</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">13</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">27</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2015~2024</span><span class="footer__devider"></span><span>Zhenchao All Rights Reserved</span><span class="footer__devider">|</span><span>浙ICP备 16010916 号</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.3.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script>function initSearch() {
  var isXML = true;
  var search_path = 'search.json';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;
  $.ajax({
    url: path,
    dataType: isXML ? 'xml' : 'json',
    async: true,
    success: function (res) {
      var datas = isXML ? $('entry', res).map(function () {
        // 将 XML 转为 JSON
        return {
          title: $('title', this).text(),
          content: $('content', this).text(),
          url: $('url', this).text()
        };
      }).get() : res;
      var $input = $('.search-input input');
      var $result = $('.search-results');
      // 搜索对象（标题、内容）的权重，影响显示顺序
      var WEIGHT = { title: 100, content: 1 };
      var searchPost = function () {
        var searchText = $input.val().toLowerCase().trim();
        // 根据空白字符分隔关键字
        var keywords = searchText.split(/[\s]+/);
        // 搜索结果
        var matchPosts = [];

        // 有多个关键字时，将原文字整个保存下来
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        // 防止未输入字符时搜索
        if (searchText.length > 0) {
          datas.forEach(function (data) {
            var isMatch  = false;
            // 没有标题的文章使用预设的 i18n 变量代替
            var title = (data.title && data.title.trim()) || '[ 文章无标题 ]';
            var titleLower = title && title.toLowerCase();
            // 删除 HTML 标签 和 所有空白字符
            var content = data.content && data.content.replace(/<[^>]+>/g, '');
            var contentLower = content && content.toLowerCase();
            // 删除重复的 /
            var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');
            // 标题中匹配到的关键词
            var titleHitSlice = [];
            // 内容中匹配到的关键词
            var contentHitSlice = [];

            keywords.forEach(function (keyword) {
              /**
              * 获取匹配的关键词的索引
              * @param {String} keyword 要匹配的关键字
              * @param {String} text 原文字
              * @param {Boolean} caseSensitive 是否区分大小写
              * @param {Number} weight 匹配对象的权重。权重大的优先显示
              * @return {Array}
              */
              function getIndexByword (word, text, caseSensitive, weight) {
                if (!word || !text) {
                  return [];
                };

                var startIndex = 0; // 每次匹配的开始索引
                var index = -1;     // 匹配到的索引值
                var result = [];    // 匹配结果

                if (!caseSensitive) {
                  word = word.toLowerCase();
                  text = text.toLowerCase();
                }

                while((index = text.indexOf(word, startIndex)) !== -1) {
                  var hasMatch = false;
                  // 索引位置相同的关键词，保留长度较长的
                  titleHitSlice.forEach(function (hit) {
                    if (hit.index === index && hit.word.length < word.length) {
                      hit.word = word;
                      hasMatch = true;
                    }
                  });
                  startIndex = index + word.length;
                  !hasMatch && result.push({ index: index, word: word, weight: weight });
                }
                return result;
              }
              titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
              contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
            });

            var hitTitle = titleHitSlice.length;
            var hitContent = contentHitSlice.length;

            if (hitTitle > 0 || hitContent > 0) {
              isMatch = true;
            }
            if (isMatch) {
              ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                // 按照匹配文字的索引的递增顺序排序
                hit.sort(function (left, right) {
                  return left.index - right.index;
                });
              });
              /**
              * 给文本中匹配到的关键词添加标记，从而进行高亮显示
              * @param {String} text 原文本
              * @param {Array} hitSlice 匹配项的索引信息
              * @param {Number} start 开始索引
              * @param {Number} end 结束索引
              * @return {String}
              */
              function highlightKeyword (text, hitSlice, start, end) {
                if (!text || !hitSlice || !hitSlice.length) {
                  return;
                }

                var result = '';
                var startIndex = start;
                var endIndex = end;
                hitSlice.forEach(function (hit) {
                  if (hit.index < startIndex) {
                    return;
                  }

                  var hitWordEnd = hit.index + hit.word.length;
                  result += text.slice(startIndex, hit.index);
                  result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                  startIndex = hitWordEnd;
                });
                result += text.slice(startIndex, endIndex);
                return result;
              }

              var postData = {};
              // 文章总的搜索权重
              var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
              // 标记匹配关键词后的标题
              var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
              // 标记匹配关键词后的内容
              var postContent;
              // 显示内容的长度
              var SHOW_WORD_LENGTH = 200;
              // 命中关键词前的字符显示长度
              var SHOW_WORD_FRONT_LENGTH = 20;
              var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

              // 截取匹配的第一个字符，前后共 200 个字符来显示
              if (contentHitSlice.length > 0) {
                var firstIndex = contentHitSlice[0].index;
                var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                var end = firstIndex + SHOW_WORD_END_LENGTH;
                postContent = highlightKeyword(content, contentHitSlice, start, end);
              } else { // 未匹配到内容，直接截取前 200 个字符来显示
                postContent = content.slice(0, SHOW_WORD_LENGTH);
              }
              postData.title = postTitle;
              postData.content = postContent;
              postData.url = postURL;
              postData.weight = postWeight;
              matchPosts.push(postData);
            }
          });
        }

        var resultInnerHtml = '';
        if (matchPosts.length) {
          // 按权重递增的顺序排序，使权重大的优先显示
          matchPosts.sort(function (left, right) {
            return right.weight - left.weight;
          });
          resultInnerHtml += '<ul>';
          matchPosts.forEach(function (post) {
            resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
            resultInnerHtml += post.title;
            resultInnerHtml += '</a><div class="search-results-content">';
            resultInnerHtml += post.content;
            resultInnerHtml += '</div></li>';
          });
          resultInnerHtml += '</ul>';
        } else {
          resultInnerHtml += '<div class="search-results-none"><i class="far fa-meh"></i></div>';
        }
        $result.html(resultInnerHtml);
      };
      $input.on('input', searchPost);
      $input.on('keyup', function (e) {
        if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
          searchPost();
        }
      });
    }
  });
}

function closeSearch () {
  $('body').css({ overflow: 'auto' });
  $('.search-popup').css({ display: 'none' });
  $('.search-mask').css({ display: 'none' });
}

window.addEventListener('DOMContentLoaded', function () {
  Stun.utils.pjaxReloadLocalSearch = function () {
    $('.header-nav-search').on('click', function (e) {
      e.stopPropagation();
      $('body').css('overflow', 'hidden');
      $('.search-popup')
        .velocity('stop')
        .velocity('transition.expandIn', {
          duration: 300,
          complete: function () {
            $('.search-popup input').focus();
          }
        });
      $('.search-mask')
        .velocity('stop')
        .velocity('transition.fadeIn', {
          duration: 300
        });

      initSearch();
    });
    $('.search-mask, .search-close').on('click', function () {
      closeSearch();
    });
    $(document).on('keydown', function (e) {
      // Escape <=> 27
      if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
        closeSearch();
      }
    });
  };

  Stun.utils.pjaxReloadLocalSearch();
}, false);

function safeOpenUrl(url) {
  var newTab = window.open();
  newTab.opener = null;
  newTab.location = url;
}

function extSearch(engine) {
  var engines = {
    google: 'https://www.google.com/search?q=',
    bing: 'https://cn.bing.com/search?q=',
    baidu: 'https://www.baidu.com/s?ie=UTF-8&wd=',
  };
  var host = window.location.host;
  var query = $('.search-input input').val().toLowerCase().trim();
  var uri = engines[engine] + query + ' site:' + host;

  if (query) {
    safeOpenUrl(uri);
  } else {
    Stun.utils.popAlert('warning', '请输入字符');
  }
}

var assistSearchList = window.CONFIG.assistSearch;

if (Array.isArray(assistSearchList)) {
  assistSearchList.forEach(function (name) {
    document.querySelector('.search-btns-item--' + name).addEventListener('click', function () {
      extSearch(name);
    }, false);
  });
}</script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload",".header-inner"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (true) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"></div><script data-pjax="">function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'plotor/hexo-comments');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (true) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (true) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.6.1"></script><script src="/js/stun-boot.js?v=2.6.1"></script><script src="/js/scroll.js?v=2.6.1"></script><script src="/js/header.js?v=2.6.1"></script><script src="/js/sidebar.js?v=2.6.1"></script><script type="application/json" src="/search.json"></script></body></html>