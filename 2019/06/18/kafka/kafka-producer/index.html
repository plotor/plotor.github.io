<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/favicon_16.png?v=2.6.1" type="image/png" sizes="16x16"><link rel="icon" href="/images/favicon_32.png?v=2.6.1" type="image/png" sizes="32x32"><meta name="google-site-verification" content="O5CNgi37yYXs3qQp7Xz61oL_AmGiwM28d7hRt5yh2to"><meta name="baidu-site-verification" content="pnKVynCWMP"><meta name="description" content="Kafka 生产者 KafkaProducer 是 Kafka 与开发者交互的媒介之一，肩负接收用户自定义消息（这里的消息指代往 Kafka 发送的各类数据），并投递给目标 topic 分区的职责。在设计上为了提升消息吞吐量，考量降低与服务端交互的压力等，每次发送消息的请求并非是直接与 Kafka 集群进行交互，而是一个异步的过程。 当调用 KafkaProducer#send 方法发送消息时，实">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka 源码解析：生产者运行机制">
<meta property="og:url" content="https://plotor.github.io/2019/06/18/kafka/kafka-producer/index.html">
<meta property="og:site_name" content="指  间">
<meta property="og:description" content="Kafka 生产者 KafkaProducer 是 Kafka 与开发者交互的媒介之一，肩负接收用户自定义消息（这里的消息指代往 Kafka 发送的各类数据），并投递给目标 topic 分区的职责。在设计上为了提升消息吞吐量，考量降低与服务端交互的压力等，每次发送消息的请求并非是直接与 Kafka 集群进行交互，而是一个异步的过程。 当调用 KafkaProducer#send 方法发送消息时，实">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://plotor.github.io/images/2019/kafka-producer.png">
<meta property="article:published_time" content="2019-06-18T06:10:00.000Z">
<meta property="article:modified_time" content="2024-12-09T05:12:58.121Z">
<meta property="article:author" content="zhenchao">
<meta property="article:tag" content="Kafka">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://plotor.github.io/images/2019/kafka-producer.png"><title>Kafka 源码解析：生产者运行机制 | 指  间</title><link ref="canonical" href="https://plotor.github.io/2019/06/18/kafka/kafka-producer/"><link rel="alternate" href="/atom.xml" type="application/atom+xml"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"carbon","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: {"avoidBanner":false},
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner header-inner--height header-inner--bgcolor"><nav class="header-nav header-nav--sticky"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user-circle"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-search"><span class="header-nav-search__icon"><i class="fas fa-search"></i></span><span class="header-nav-search__text">搜索</span></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">Kafka 源码解析：生产者运行机制</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2019-06-18</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">15.5k</span></span><span class="post-meta-item post-meta-item--readtime"><span class="post-meta-item__icon"><i class="far fa-clock"></i></span><span class="post-meta-item__info">阅读时长</span><span class="post-meta-item__value">70分</span></span></div></header><div class="post-body"><p>Kafka 生产者 KafkaProducer 是 Kafka 与开发者交互的媒介之一，肩负接收用户自定义消息（这里的消息指代往 Kafka 发送的各类数据），并投递给目标 topic 分区的职责。在设计上为了提升消息吞吐量，考量降低与服务端交互的压力等，每次发送消息的请求并非是直接与 Kafka 集群进行交互，而是一个异步的过程。</p>
<p>当调用 <code>KafkaProducer#send</code> 方法发送消息时，实际上只是将消息缓存到了本地的消息收集器中，Kafka 定义了一个 RecordAccumulator 收集器用于收集用户提交的消息数据，同时又在后台维护了一个 Sender 线程，以异步的方式不断将收集器中缓存的消息定期定量地投递给 Kafka 集群。<a id="more"></a></p>
<p>在本篇文章中，我们首先回忆一下 KafkaProducer 的使用方式，然后重点分析消息的收集、缓存、投递，以及响应的过程。</p>

        <h3 id="KafkaProducer-使用示例">
          <a href="#KafkaProducer-使用示例" class="heading-link"><i class="fas fa-link"></i></a>KafkaProducer 使用示例</h3>
      <p>KafkaProducer 往 Kafka 发送消息需要依赖于客户端 SDK，Kafka 提供了 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/KAFKA/Clients">多种语言的客户端</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 供开发者选择，这里我们以 Kafka 内置的 java 客户端为例，介绍如何向 Kafka 集群发送消息。示例：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"localhost:9092"</span>);</span><br><span class="line">properties.put(ProducerConfig.CLIENT_ID_CONFIG, <span class="string">"producer-demo"</span>);</span><br><span class="line">properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class.getName());</span><br><span class="line">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">KafkaProducer&lt;Integer, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> isAsync = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) {</span><br><span class="line">    <span class="keyword">if</span> (isAsync) {</span><br><span class="line">        <span class="comment">// 异步发送消息</span></span><br><span class="line">        producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(DEFAULT_TOPIC, i, <span class="string">"zhenchao"</span>), (metadata, e) -&gt; {</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">null</span> != e) {</span><br><span class="line">                <span class="comment">// ... 处理错误</span></span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            }</span><br><span class="line">            printResult(metadata); <span class="comment">// 打印结果信息</span></span><br><span class="line">        });</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 同步发送消息，指定 topic 和消息内容</span></span><br><span class="line">        Future&lt;RecordMetadata&gt; future = producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(DEFAULT_TOPIC, i, <span class="string">"zhenchao"</span>));</span><br><span class="line">        RecordMetadata metadata = future.get(<span class="number">10</span>, TimeUnit.SECONDS);</span><br><span class="line">        <span class="keyword">this</span>.printResult(metadata); <span class="comment">// 打印结果信息</span></span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>示例中发送消息依赖于 KafkaProducer 对象，KafkaProducer 类也是我们分析生产者运行机制的入口。创建该对象时我们需要指定 Kafka 集群地址，以及消息 key 和 value 的序列化器，但是客户端 ID 不是必须指定的，后面在分析源码时会看到如果未明确指定客户端 ID，Kafka 会自动为当前客户端创建一个。</p>
<p>接着我们可以调用 <code>KafkaProducer#send</code> 方法向 Kafka 集群指定的 topic 投递消息。消息在被投递之前需要封装成 ProducerRecord 对象，该对象封装了当前消息的目标 topic、目标分区，key、value，以及时间戳等信息。ProducerRecord 的字段定义如下，其中 <code>ProducerRecord#headers</code> 字段在 0.11 版本引入：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 主题 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="comment">/** 分区 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">    <span class="comment">/** 消息头 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Headers headers;</span><br><span class="line">    <span class="comment">/** 消息对应的 key */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="comment">/** 消息内容 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;</span><br><span class="line">    <span class="comment">/** 时间戳 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>示例中我们定义了 isAsync 参数，需要说明的一点是，isAsync 参数虽然表面意思是指以异步的方式发送消息，但是本质上不管该参数如何设置，Kafka 当下的版本都只有一种消息发送的方式，即异步发送。参数 isAsync 设置为 true 或者 false 的意义在于指定如何获取消息发送的响应结果，区别在于：</p>
<ul>
<li><code>isAsync=false</code>：以异步方式发送消息，但是通过 Future 模式阻塞等待消息的发送的响应结果。</li>
<li><code>isAsync=true</code>：以异步方式发送消息，但是通过 Callback 模式异步获取消息发送的响应结果，即不管消息发送成功还是失败，都会以回调的方式通知客户端，客户端期间不需要阻塞等待。</li>
</ul>

        <h3 id="消息收集与发送过程分析">
          <a href="#消息收集与发送过程分析" class="heading-link"><i class="fas fa-link"></i></a>消息收集与发送过程分析</h3>
      <p>在具体开始分析消息的发送过程之前，我们需要明确 <strong>消息发送是一个异步的过程</strong> ，该过程涉及到 2 个线程的协同工作，其中 1 个线程将待发送的消息写入缓冲区（即收集待发送消息），另外 1 个线程（Sender 线程）负责定期定量将缓冲区中的数据投递给 Kafka 集群，并反馈投递结果。</p>
<p>
        <img class="lazyload lazyload-gif" src="/images/loading.svg" data-src="/images/2019/kafka-producer.png" alt="image">
      </p>
<p>如上图描绘了生产者运行的基本结构。我们将业务线程称为主线程，业务在调用 <code>KafkaProducer#send</code> 方法向 Kafka 投递消息时，对应的消息会经过拦截器、序列化器，以及分区器等一系列处理，最终被缓存到消息收集器 RecordAccumulator 中。消息收集器为每个 topic 分区设置了一个双端队列用于记录待发往目标 topic 分区的消息集合。Sender 线程异步循环轮询消费消息收集器中的各个队列，将消息按照目标 broker 节点（即分区 Leader 副本所在的 broker 节点）进行分组，并封装成 RPC 请求发往目标节点。</p>
<p>这里我们只是简单概括了 KafkaProducer 的基本运行机制，下面将对各个环节展开进行深入分析。</p>

        <h4 id="收集待发送的消息">
          <a href="#收集待发送的消息" class="heading-link"><i class="fas fa-link"></i></a>收集待发送的消息</h4>
      
        <h5 id="KafkaProducer-的字段定义与构造方法">
          <a href="#KafkaProducer-的字段定义与构造方法" class="heading-link"><i class="fas fa-link"></i></a>KafkaProducer 的字段定义与构造方法</h5>
      <p>首先来看一下 KafkaProducer 类的字段定义，如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducer</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Producer</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** clientId 生成器，如果没有明确指定客户端 ID，则使用该字段顺序生成一个 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> AtomicInteger PRODUCER_CLIENT_ID_SEQUENCE = <span class="keyword">new</span> AtomicInteger(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/** 生产者唯一标识（对应 client.id 属性配置 ） */</span></span><br><span class="line">    <span class="keyword">private</span> String clientId;</span><br><span class="line">    <span class="comment">/** 分区选择器（对应 partitioner.class 属性配置），如果未明确指定分区，则基于一定的策略为消息选择合适的分区 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Partitioner partitioner;</span><br><span class="line">    <span class="comment">/** 消息的最大长度（对应 max.request.size 配置，包含消息头、序列化之后的 key 和 value） */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> maxRequestSize;</span><br><span class="line">    <span class="comment">/** 发送单条消息的缓冲区大小（对应 buffer.memory 配置） */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> totalMemorySize;</span><br><span class="line">    <span class="comment">/** kafka 集群元数据 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Metadata metadata;</span><br><span class="line">    <span class="comment">/** 消息收集器，用于收集并缓存消息，等待 Sender 线程的发送 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> RecordAccumulator accumulator;</span><br><span class="line">    <span class="comment">/** 消息发送线程对象 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Sender sender;</span><br><span class="line">    <span class="comment">/** 消息发送线程 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Thread ioThread;</span><br><span class="line">    <span class="comment">/** 压缩算法（对应 compression.type 配置） */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CompressionType compressionType;</span><br><span class="line">    <span class="comment">/** 时间戳工具 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Time time;</span><br><span class="line">    <span class="comment">/** key 序列化器（对应 key.serializer 配置） */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Serializer&lt;K&gt; keySerializer;</span><br><span class="line">    <span class="comment">/** value 序列化器（对应 value.serializer 配置） */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Serializer&lt;V&gt; valueSerializer;</span><br><span class="line">    <span class="comment">/** 封装配置信息 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ProducerConfig producerConfig;</span><br><span class="line">    <span class="comment">/** 等待更新 kafka 集群元数据的最大时长 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> maxBlockTimeMs;</span><br><span class="line">    <span class="comment">/** 消息发送的超时时间（从发送到收到 ACK 响应） */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> requestTimeoutMs;</span><br><span class="line">    <span class="comment">/** 发送拦截器（对应 interceptor.classes 配置），用于待发送的消息进行拦截并修改，也可以对 ACK 响应进行拦截处理 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ProducerInterceptors&lt;K, V&gt; interceptors;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>接下来继续看一下 KafkaProducer 类对象的构造过程，KafkaProducer 提供了多个重载版本的构造方法实现，其中最底层的构造方法实现如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="title">KafkaProducer</span><span class="params">(ProducerConfig config, Serializer&lt;K&gt; keySerializer, Serializer&lt;V&gt; valueSerializer)</span> </span>{</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        log.trace(<span class="string">"Starting the Kafka producer"</span>);</span><br><span class="line">        <span class="comment">// 获取用户配置信息</span></span><br><span class="line">        Map&lt;String, Object&gt; userProvidedConfigs = config.originals();</span><br><span class="line">        <span class="keyword">this</span>.producerConfig = config;</span><br><span class="line">        <span class="keyword">this</span>.time = Time.SYSTEM;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 尝试获取用户配置的 clientId，如果未配置则基于 PRODUCER_CLIENT_ID_SEQUENCE 顺序生成一个</span></span><br><span class="line">        <span class="keyword">this</span>.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);</span><br><span class="line">        <span class="keyword">if</span> (clientId.length() &lt;= <span class="number">0</span>) {</span><br><span class="line">            <span class="comment">// 用户未指定 clientId，基于 PRODUCER_CLIENT_ID_SEQUENCE 顺序生成一个</span></span><br><span class="line">            clientId = <span class="string">"producer-"</span> + PRODUCER_CLIENT_ID_SEQUENCE.getAndIncrement();</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ... 省略打点相关注册逻辑</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取配置的分区器对象（反射创建）</span></span><br><span class="line">        <span class="keyword">this</span>.partitioner = config.getConfiguredInstance(ProducerConfig.PARTITIONER_CLASS_CONFIG, Partitioner.class);</span><br><span class="line">        <span class="comment">// 获取生产者重试间隔</span></span><br><span class="line">        <span class="keyword">long</span> retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果参数未指定 key 序列化器，则尝试从配置中获取 key 序列化器对象（反射创建）</span></span><br><span class="line">        <span class="keyword">if</span> (keySerializer == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">this</span>.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, Serializer.class);</span><br><span class="line">            <span class="keyword">this</span>.keySerializer.configure(config.originals(), <span class="keyword">true</span>);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);</span><br><span class="line">            <span class="keyword">this</span>.keySerializer = keySerializer;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果参数未指定 value 序列化器，则尝试从配置中获取 value 序列化器对象（反射创建）</span></span><br><span class="line">        <span class="keyword">if</span> (valueSerializer == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">this</span>.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, Serializer.class);</span><br><span class="line">            <span class="keyword">this</span>.valueSerializer.configure(config.originals(), <span class="keyword">false</span>);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);</span><br><span class="line">            <span class="keyword">this</span>.valueSerializer = valueSerializer;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// load interceptors and make sure they get clientId</span></span><br><span class="line">        userProvidedConfigs.put(ProducerConfig.CLIENT_ID_CONFIG, clientId);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取注册的拦截器列表</span></span><br><span class="line">        List&lt;ProducerInterceptor&lt;K, V&gt;&gt; interceptorList = (List) (<span class="keyword">new</span> ProducerConfig(userProvidedConfigs, <span class="keyword">false</span>))</span><br><span class="line">                .getConfiguredInstances(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProducerInterceptor.class);</span><br><span class="line">        <span class="keyword">this</span>.interceptors = interceptorList.isEmpty() ? <span class="keyword">null</span> : <span class="keyword">new</span> ProducerInterceptors&lt;&gt;(interceptorList);</span><br><span class="line"></span><br><span class="line">        ClusterResourceListeners clusterResourceListeners =</span><br><span class="line">                <span class="keyword">this</span>.configureClusterResourceListeners(keySerializer, valueSerializer, interceptorList, reporters);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建并更新 kafka 集群的元数据信息</span></span><br><span class="line">        <span class="keyword">this</span>.metadata = <span class="keyword">new</span> Metadata(retryBackoffMs, config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG), <span class="keyword">true</span>, clusterResourceListeners);</span><br><span class="line">        <span class="comment">// 获取并设置生产者发送请求的大小</span></span><br><span class="line">        <span class="keyword">this</span>.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);</span><br><span class="line">        <span class="comment">// 获取并设置生产者内存缓冲区大小，用于缓存要发送到服务器的消息</span></span><br><span class="line">        <span class="keyword">this</span>.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);</span><br><span class="line">        <span class="comment">// 获取并设置消息压缩算法，可以设置为 snappy、gzip 或 lz4，默认不压缩。</span></span><br><span class="line">        <span class="keyword">this</span>.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ... 基于用户配置设置 maxBlockTimeMs 和 requestTimeoutMs，省略</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建消息收集器，用于异步发送消息</span></span><br><span class="line">        <span class="keyword">this</span>.accumulator = <span class="keyword">new</span> RecordAccumulator(</span><br><span class="line">                config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), <span class="comment">// 指定每个批次的大小（单位：字节）</span></span><br><span class="line">                <span class="keyword">this</span>.totalMemorySize,</span><br><span class="line">                <span class="keyword">this</span>.compressionType,</span><br><span class="line">                config.getLong(ProducerConfig.LINGER_MS_CONFIG), <span class="comment">// 消息缓存超时发送时间</span></span><br><span class="line">                retryBackoffMs,</span><br><span class="line">                metrics,</span><br><span class="line">                time);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 kafka 集群主机列表</span></span><br><span class="line">        List&lt;InetSocketAddress&gt; addresses = ClientUtils.parseAndValidateAddresses(config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG));</span><br><span class="line">        <span class="comment">// 更新 kafka 集群元数据信息</span></span><br><span class="line">        <span class="keyword">this</span>.metadata.update(Cluster.bootstrap(addresses), Collections.&lt;String&gt;emptySet(), time.milliseconds());</span><br><span class="line">        ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config.values());</span><br><span class="line">        <span class="comment">// 创建 NetworkClient 对象，NetworkClient 是 producer 网络 I/O 的核心</span></span><br><span class="line">        NetworkClient client = <span class="keyword">new</span> NetworkClient(</span><br><span class="line">                <span class="keyword">new</span> Selector(config.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), metrics, time, <span class="string">"producer"</span>, channelBuilder),</span><br><span class="line">                metadata,</span><br><span class="line">                clientId,</span><br><span class="line">                config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION),</span><br><span class="line">                config.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),</span><br><span class="line">                config.getInt(ProducerConfig.SEND_BUFFER_CONFIG),</span><br><span class="line">                config.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),</span><br><span class="line">                requestTimeoutMs,</span><br><span class="line">                time,</span><br><span class="line">                <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建并启动 Sender 线程</span></span><br><span class="line">        <span class="keyword">this</span>.sender = <span class="keyword">new</span> Sender(</span><br><span class="line">                client,</span><br><span class="line">                metadata,</span><br><span class="line">                accumulator,</span><br><span class="line">                config.getInt(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION) == <span class="number">1</span>,</span><br><span class="line">                config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),</span><br><span class="line">                (<span class="keyword">short</span>) parseAcks(config.getString(ProducerConfig.ACKS_CONFIG)),</span><br><span class="line">                config.getInt(ProducerConfig.RETRIES_CONFIG),</span><br><span class="line">                metrics,</span><br><span class="line">                Time.SYSTEM,</span><br><span class="line">                requestTimeoutMs);</span><br><span class="line">        String ioThreadName = <span class="string">"kafka-producer-network-thread"</span> + (clientId.length() &gt; <span class="number">0</span> ? <span class="string">" | "</span> + clientId : <span class="string">""</span>);</span><br><span class="line">        <span class="keyword">this</span>.ioThread = <span class="keyword">new</span> KafkaThread(ioThreadName, <span class="keyword">this</span>.sender, <span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">this</span>.ioThread.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印未使用的配置</span></span><br><span class="line">        config.logUnused();</span><br><span class="line">        AppInfoParser.registerAppInfo(JMX_PREFIX, clientId);</span><br><span class="line">        log.debug(<span class="string">"Kafka producer started"</span>);</span><br><span class="line">    } <span class="keyword">catch</span> (Throwable t) {</span><br><span class="line">        <span class="comment">// ... 省略异常处理</span></span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>具体实现如上述代码注释，一条消息的发送需要经过拦截器、序列化器、分区器，最后缓存到消息收集器中，并由 Sender 线程在后台异步往 Kafka 集群投递消息，所以在构造 KafkaProducer 对象时主要就是初始化这些组件。</p>

        <h5 id="消息收集的过程">
          <a href="#消息收集的过程" class="heading-link"><i class="fas fa-link"></i></a>消息收集的过程</h5>
      <p>了解了 KafkaProducer 的字段定义和对象的构造过程之后，下面正式开始对消息收集的过程进行分析，相关实现位于 <code>KafkaProducer#send</code> 方法中：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>{</span><br><span class="line">    <span class="comment">// 遍历注册的拦截器对待发送的消息执行拦截修改</span></span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptedRecord = <span class="keyword">this</span>.interceptors == <span class="keyword">null</span> ? record : <span class="keyword">this</span>.interceptors.onSend(record);</span><br><span class="line">    <span class="comment">// 调用 doSend 方法开始发送消息</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.doSend(interceptedRecord, callback);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>该方法只是简单应用了注册的 Producer 拦截器对发送的消息进行拦截修改，而具体消息收集的过程则封装在 <code>KafkaProducer#doSend</code> 方法中。先来看一下 Producer 拦截器，我们可以基于该拦截器机制实现对消息的剔除、修改，以及在响应回调之前增加一些定制化的需求等。拦截器 ProducerInterceptor 接口的定义如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Configurable</span> </span>{</span><br><span class="line">    <span class="function">ProducerRecord&lt;K, V&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>其中，方法 <code>ProducerInterceptor#onSend</code> 用于对待发送的消息进行前置拦截，具体的拦截时机是在消息被序列化和分配分区（如果未手动指定分区）之前，如上述 <code>KafkaProducer#send</code> 方法所示。方法 <code>ProducerInterceptor#onAcknowledgement</code> 用于对已发送到 Kafka 集群并得到确认的消息，以及发送失败的消息进行后置拦截，具体的拦截时机是在回调用户自定义的 Callback 逻辑之前。需要注意的一点是，方法 <code>ProducerInterceptor#onAcknowledgement</code> 在 Producer 的 I/O 线程中被调用，所以不建议在其中实现一些比较耗时的逻辑，以便影响整体消息发送的性能。</p>
<p>下面继续来看一下收集消息的过程，实现位于 <code>KafkaProducer#doSend</code> 方法中：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Future&lt;RecordMetadata&gt; <span class="title">doSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>{</span><br><span class="line">    TopicPartition tp = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 1. 获取 kafka 集群元数据信息，如果当前请求的是新 topic，或者指定的分区超过已知的分区范围，则会触发更新集群元数据信息</span></span><br><span class="line">        ClusterAndWaitTime clusterAndWaitTime = <span class="keyword">this</span>.waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</span><br><span class="line">        <span class="keyword">long</span> remainingWaitMs = Math.max(<span class="number">0</span>, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">        Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2 基于注册的序列化器对 key 执行序列化</span></span><br><span class="line">        <span class="keyword">byte</span>[] serializedKey;</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            serializedKey = keySerializer.serialize(record.topic(), record.key());</span><br><span class="line">        } <span class="keyword">catch</span> (ClassCastException cce) {</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert key of class "</span> + record.key().getClass().getName() +</span><br><span class="line">                    <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + <span class="string">" specified in key.serializer"</span>);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 基于注册的序列化器对 value 执行序列化</span></span><br><span class="line">        <span class="keyword">byte</span>[] serializedValue;</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            serializedValue = valueSerializer.serialize(record.topic(), record.value());</span><br><span class="line">        } <span class="keyword">catch</span> (ClassCastException cce) {</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert value of class "</span> + record.value().getClass().getName() +</span><br><span class="line">                    <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + <span class="string">" specified in value.serializer"</span>);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 为当前消息选择合适的分区，如果未明确指定的话，则基于注册的分区器为当前消息计算分区</span></span><br><span class="line">        <span class="keyword">int</span> partition = <span class="keyword">this</span>.partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 5. 将消息追加到消息收集器（RecordAccumulator）中 */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算当前消息大小，并校验消息是否过大</span></span><br><span class="line">        <span class="keyword">int</span> serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);</span><br><span class="line">        <span class="keyword">this</span>.ensureValidRecordSize(serializedSize);</span><br><span class="line">        tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition); <span class="comment">// 消息投递的目标 topic 分区</span></span><br><span class="line">        <span class="comment">// 如果未明确为当前消息指定时间戳，则设置为当前时间戳</span></span><br><span class="line">        <span class="keyword">long</span> timestamp = record.timestamp() == <span class="keyword">null</span> ? time.milliseconds() : record.timestamp();</span><br><span class="line">        log.trace(<span class="string">"Sending record {} with callback {} to topic {} partition {}"</span>, record, callback, record.topic(), partition);</span><br><span class="line">        <span class="comment">// producer callback will make sure to call both 'callback' and interceptor callback</span></span><br><span class="line">        Callback interceptCallback = <span class="keyword">this</span>.interceptors == <span class="keyword">null</span> ? callback : <span class="keyword">new</span> InterceptorCallback&lt;&gt;(callback, <span class="keyword">this</span>.interceptors, tp);</span><br><span class="line">        <span class="comment">// 追加消息到收集器中</span></span><br><span class="line">        RecordAccumulator.RecordAppendResult result = accumulator.append(</span><br><span class="line">                tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 6. 条件性唤醒消息发送线程 */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果队列中不止一个 RecordBatch，或者最后一个 RecordBatch 满了，或者有创建新的 RecordBatch，则唤醒 Sender 线程发送消息</span></span><br><span class="line">        <span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) {</span><br><span class="line">            log.trace(<span class="string">"Waking up the sender since topic {} partition {} is either full or getting a new batch"</span>, record.topic(), partition);</span><br><span class="line">            <span class="comment">// 唤醒 sender 线程，发送消息</span></span><br><span class="line">            <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> result.future;</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// ... 省略异常处理</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>我们可以将消息的发送过程概括为以下 6 个步骤：</p>
<ol>
<li>获取集群的元数据（Metadata）信息，如果请求的是新 topic，或者指定的分区 ID 超过了已知的合法区间，则触发更新本地缓存的集群元数据信息；</li>
<li>基于注册的 key 序列化器对消息的 key 执行序列化；</li>
<li>基于注册的 value 序列化器对消息的 value 执行序列化；</li>
<li>如果未指定目标 topic 分区，则基于注册的分区器为当前消息计算目标分区；</li>
<li>缓存消息到消息收集器 RecordAccumulator 中；</li>
<li>条件性唤醒消息发送 Sender 线程。</li>
</ol>
<p>下面逐一对上述过程中的 6 个步骤展开分析。首先来看一下获取集群元数据信息的过程（ <strong>步骤 1</strong> ），KafkaProducer 本地会缓存集群的元数据信息，包括集群的 topic 列表、每个 topic 的分区列表、分区 Leader 和 Follower 副本所在节点、分区 AR 和 ISR 集合，以及集群节点信息等，详细信息参考下面的 Metadata 类定义。</p>
<p>当客户端向集群投递消息时实际上是投递到了目标 topic 指定分区的 Leader 副本上。因为集群状态是动态变化的，Leader 副本所在的网络位置也会发生迁移，所以客户端在投递消息之前，需要确保本地所缓存的集群信息是最新的，否则需要标记当前集群信息需要更新，具体的更新操作由 Sender 线程完成。</p>
<p>KafkaProducer 在发送消息之前会先调用 <code>KafkaProducer#waitOnMetadata</code> 方法获取集群元数据信息，如果感知到本地缓存的集群元数据信息已经过期，则会通知 Sender 线程进行更新。首先来看一下保存集群元数据信息的 Metadata 类的字段定义：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Metadata</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 元数据最小更新时间间隔，默认是 100 毫秒，防止更新太频繁 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> refreshBackoffMs;</span><br><span class="line">    <span class="comment">/** 元数据更新时间间隔，默认为 5 分钟 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> metadataExpireMs;</span><br><span class="line">    <span class="comment">/** 元数据版本号，每更新成功一次则版本号加 1 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> version;</span><br><span class="line">    <span class="comment">/** 上一次更新元数据的时间戳，不管成功还是失败 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshMs;</span><br><span class="line">    <span class="comment">/** 上一次成功更新元数据的时间戳 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> lastSuccessfulRefreshMs;</span><br><span class="line">    <span class="comment">/** 集群信息 */</span></span><br><span class="line">    <span class="keyword">private</span> Cluster cluster;</span><br><span class="line">    <span class="comment">/** 标记是否需要更新集群元数据信息 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> needUpdate;</span><br><span class="line">    <span class="comment">/** 记录集群中所有的 topic 信息，key 是 topic，value 是 topic 过期的时间戳 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, Long&gt; topics;</span><br><span class="line">    <span class="comment">/** 元数据更新监听器 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Listener&gt; listeners;</span><br><span class="line">    <span class="comment">/** 标记是否需要更新所有 topic 的元数据信息，一般只更新当前用到的 topic 的元数据信息 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> needMetadataForAllTopics;</span><br><span class="line">    <span class="comment">/** 是否允许 topic 过期 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">boolean</span> topicExpiryEnabled;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>下面继续来看一下 <code>KafkaProducer#waitOnMetadata</code> 方法的实现：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> ClusterAndWaitTime <span class="title">waitOnMetadata</span><span class="params">(String topic, Integer partition, <span class="keyword">long</span> maxWaitMs)</span> <span class="keyword">throws</span> InterruptedException </span>{</span><br><span class="line">    <span class="comment">// 添加 topic 到集合中，如果是新 topic，标记需要更新集群元数据信息</span></span><br><span class="line">    metadata.add(topic);</span><br><span class="line">    <span class="comment">// 获取当前集群信息</span></span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line">    <span class="comment">// 获取指定 topic 的分区数目</span></span><br><span class="line">    Integer partitionsCount = cluster.partitionCountForTopic(topic);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果参数未指定分区，或指定的分区在当前记录的分区范围之内，则返回历史集群信息</span></span><br><span class="line">    <span class="keyword">if</span> (partitionsCount != <span class="keyword">null</span> &amp;&amp; (partition == <span class="keyword">null</span> || partition &lt; partitionsCount)) {</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ClusterAndWaitTime(cluster, <span class="number">0</span>);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 否则，当前缓存的集群元数据信息可能已经过期，需要进行更新 */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> begin = time.milliseconds();</span><br><span class="line">    <span class="keyword">long</span> remainingWaitMs = maxWaitMs; <span class="comment">// 剩余等待时间</span></span><br><span class="line">    <span class="keyword">long</span> elapsed;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 请求集群的元数据信息，直到获取到信息或者超时 */</span></span><br><span class="line">    <span class="keyword">do</span> {</span><br><span class="line">        log.trace(<span class="string">"Requesting metadata update for topic {}."</span>, topic);</span><br><span class="line">        <span class="comment">// 更新 Metadata 的 needUpdate 字段，并获取当前元数据的版本号</span></span><br><span class="line">        <span class="keyword">int</span> version = metadata.requestUpdate();</span><br><span class="line">        <span class="comment">// 唤醒 sender 线程，由 sender 线程负责更新元数据信息</span></span><br><span class="line">        sender.wakeup();</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// 等待元数据更新完成</span></span><br><span class="line">            metadata.awaitUpdate(version, remainingWaitMs);</span><br><span class="line">        } <span class="keyword">catch</span> (TimeoutException ex) {</span><br><span class="line">            <span class="comment">// 等待超时</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">"Failed to update metadata after "</span> + maxWaitMs + <span class="string">" ms."</span>);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取更新后的集群信息</span></span><br><span class="line">        cluster = metadata.fetch();</span><br><span class="line">        elapsed = time.milliseconds() - begin;</span><br><span class="line">        <span class="keyword">if</span> (elapsed &gt;= maxWaitMs) {</span><br><span class="line">            <span class="comment">// 等待超时</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">"Failed to update metadata after "</span> + maxWaitMs + <span class="string">" ms."</span>);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 权限检测</span></span><br><span class="line">        <span class="keyword">if</span> (cluster.unauthorizedTopics().contains(topic)) {</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TopicAuthorizationException(topic);</span><br><span class="line">        }</span><br><span class="line">        remainingWaitMs = maxWaitMs - elapsed; <span class="comment">// 更新剩余等待时间</span></span><br><span class="line">        partitionsCount = cluster.partitionCountForTopic(topic); <span class="comment">// 获取指定 topic 的分区数目</span></span><br><span class="line">    } <span class="keyword">while</span> (partitionsCount == <span class="keyword">null</span>); <span class="comment">// 更新集群信息失败，继续重试</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 更新集群信息成功 */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 参数指定的分区非法</span></span><br><span class="line">    <span class="keyword">if</span> (partition != <span class="keyword">null</span> &amp;&amp; partition &gt;= partitionsCount) {</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(String.format(<span class="string">"Invalid partition given with record: %d is not in the range [0...%d)."</span>, partition, partitionsCount));</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ClusterAndWaitTime(cluster, elapsed);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法首先会尝试将当前 topic 加入到本地缓存的 topic 集合中，因为客户端对于 topic 会有一个过期机制，对于长时间未使用的 topic 会从本地缓存中移除。这里一开始调用 <code>Metadata#add</code> 方法除了标记当前 topic 是活跃的之外，另外一个目的在于判断本地是否有该 topic 的缓存信息，如果没有则需要通知 Sender 线程更新集群元数据信息。通知的过程实际上只是简单将 <code>Metadata#needUpdate</code> 字段设置为 true，Sender 线程会检查该字段以更新集群元数据信息。</p>
<p>接下来会调用 <code>Metadata#fetch</code> 方法获取集群信息 Cluster 对象，Cluster 类是对集群节点、topic、分区等信息的一个封装，其字段定义如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Cluster</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** kafka 集群中的节点信息列表（包括 id、host、port 等信息） */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Node&gt; nodes;</span><br><span class="line">    <span class="comment">/** 未授权的 topic 集合 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Set&lt;String&gt; unauthorizedTopics;</span><br><span class="line">    <span class="comment">/** 内部 topic 集合 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Set&lt;String&gt; internalTopics;</span><br><span class="line">    <span class="comment">/** 记录 topic 分区与分区详细信息的映射关系 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;TopicPartition, PartitionInfo&gt; partitionsByTopicPartition;</span><br><span class="line">    <span class="comment">/** 记录 topic 及其分区信息的映射关系 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, List&lt;PartitionInfo&gt;&gt; partitionsByTopic;</span><br><span class="line">    <span class="comment">/** 记录 topic 及其分区信息的映射关系（必须包含 leader 副本） */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, List&lt;PartitionInfo&gt;&gt; availablePartitionsByTopic;</span><br><span class="line">    <span class="comment">/** 记录节点 ID 与分区信息的映射关系 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, List&lt;PartitionInfo&gt;&gt; partitionsByNode;</span><br><span class="line">    <span class="comment">/** key 是 brokerId，value 是 broker 节点信息，方便基于 brokerId 获取对应的节点信息 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, Node&gt; nodesById;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>其中 Node、TopicPartition 和 PartitionInfo 类定义比较简单，其作用分别为：</p>
<ul>
<li><strong>Node</strong> ：封装 Kafka 节点信息，包括 ID、主机名，以及端口号等信息。</li>
<li><strong>TopicPartition</strong> ：封装分区摘要信息，包含分区所属 topic 和分区编号。</li>
<li><strong>PartitionInfo</strong> ：封装分区详细信息，包括分区所属 topic、分区编号、Leader 副本所在节点、全部副本所在节点列表，以及 ISR 副本所在节点列表。</li>
</ul>
<p>继续回到 <code>KafkaProducer#waitOnMetadata</code> 方法。接下来方法会判断是否需要更新集群元数据信息，判断的依据是当前本地缓存的目标 topic 的分区数目不为空，同时如果发送消息时明确指定了分区编号，则此编号必须在本地认为合法的分区编号区间范围内。如果能够满足这些条件，则认为本地缓存的集群信息是合法的，可以直接拿来使用，否则就会触发更新集群元数据的逻辑。如果需要更新集群元数据，则会调用 <code>Metadata#requestUpdate</code> 方法设置标记位，同时唤醒 Sender 线程进行处理，并等待集群元数据更新完成。判定更新完成的策略就是判定本地缓存的集群元数据的版本号（<code>Metadata#version</code> 字段）是否被更新，因为集群元数据每更新成功一次，版本号会加 1。如果等待过程超时则会抛出 TimeoutException 异常。</p>
<p>此外，客户端也会定期触发元数据更新操作，默认元数据有效时间为 5 分钟，可以通过 <code>metadata.max.age.ms</code> 参数进行设定。</p>
<p>回到 <code>KafkaProducer#doSend</code> 方法，在拿到集群信息之后，方法会基于配置的 key 和 value 序列化器分别对消息 ID 和消息内容进行序列化（ <strong>步骤 2</strong> 和 <strong>步骤 3</strong> ），这一过程比较简单。 <strong>步骤 4</strong> 会为当前消息选择合适的分区，相关实现位于 <code>KafkaProducer#partition</code> 方法中：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>{</span><br><span class="line">    <span class="comment">// 获取当前待发送消息所指定的分区</span></span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="comment">// 如果未指定分区，则为当前消息计算一个分区编号</span></span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">            partition :</span><br><span class="line">            partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果我们在发送消息时明确指定了分区编号，那么这里只是简单的返回该编号，否则就需要基于注册的分区器计算当前消息对应的分区编号。Partitioner 接口是分区器的抽象，我们可以实现该接口自定义分区器，Kafka 也提供了默认的分区器实现 DefaultPartitioner，分区算法实现如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>{</span><br><span class="line">    <span class="comment">// 获取当前 topic 的分区详细信息</span></span><br><span class="line">    List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">    <span class="comment">// 获取当前 topic 对应的分区数</span></span><br><span class="line">    <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">    <span class="comment">// 如果没有设置 key，则基于轮询算法</span></span><br><span class="line">    <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) {</span><br><span class="line">        <span class="comment">// 获取当前 topic 对应的上次位置值加 1，如果是第一次则随机生成一个</span></span><br><span class="line">        <span class="keyword">int</span> nextValue = <span class="keyword">this</span>.nextValue(topic);</span><br><span class="line">        <span class="comment">// 获取当前 topic 包含 leader 副本的分区详细信息</span></span><br><span class="line">        List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) {</span><br><span class="line">            <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">            <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 如果指定了 key，则使用 murmur2 算法对 key 做哈希取模</span></span><br><span class="line">    <span class="keyword">else</span> {</span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>默认分区器 DefaultPartitioner 依据消息的 key 计算分区，如果在发送消息时未指定 key，则默认分区器会基于 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Round-robin_scheduling">Round-Robin</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 算法计算分区编号，以保证目标 topic 分区的负载均衡。否则会基于 32 位的 <span class="exturl"><a class="exturl__link" target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MurmurHash">murmur2</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 哈希算法计算 key 的哈希值，并与分区数取模得到最后的分区编号。</p>
<p><strong>步骤 5</strong> 会计算并校验当前消息的大小，同时为消息附加时间戳，并最终调用 <code>RecordAccumulator#append</code> 方法将消息缓存到收集器 RecordAccumulator 中，等待 Sender 线程投递给 Kafka 集群。RecordAccumulator 是生产者 SDK 中非常重要的一个类，可以将其看做是一个本地缓存消息的队列，消息收集线程将消息最终记录到收集器中，而 Sender 线程会定期定量从收集器中取出缓存的消息，并投递给 Kafka 集群。RecordAccumulator 类字段定义如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">RecordAccumulator</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 标识当前收集器是否被关闭，对应 producer 被关闭 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">boolean</span> closed;</span><br><span class="line">    <span class="comment">/** 记录正在执行 flush 操作的线程数 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> AtomicInteger flushesInProgress;</span><br><span class="line">    <span class="comment">/** 记录正在执行 append 操作的线程数 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> AtomicInteger appendsInProgress;</span><br><span class="line">    <span class="comment">/** 指定每个 RecordBatch 中 ByteBuffer 的大小 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> batchSize;</span><br><span class="line">    <span class="comment">/** 消息压缩类型 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CompressionType compression;</span><br><span class="line">    <span class="comment">/** 通过参数 linger.ms 指定，当本地消息缓存时间超过该值时，即使消息量未达到阈值也会进行投递 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> lingerMs;</span><br><span class="line">    <span class="comment">/** 生产者重试时间间隔 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> retryBackoffMs;</span><br><span class="line">    <span class="comment">/** 缓存（ByteBuffer）管理工具 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> BufferPool free;</span><br><span class="line">    <span class="comment">/** 时间戳工具 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Time time;</span><br><span class="line">    <span class="comment">/** 记录 topic 分区与 RecordBatch 的映射关系，对应的消息都是发往对应的 topic 分区 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; batches;</span><br><span class="line">    <span class="comment">/** 记录未发送完成（即未收到服务端响应）的消息集合 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> IncompleteRecordBatches incomplete;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消息顺序性保证，</span></span><br><span class="line"><span class="comment">     * 缓存当前待发送消息的目标 topic 分区，防止对于同一个 topic 分区同时存在多个未完成的消息，可能导致消息顺序性错乱</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Set&lt;TopicPartition&gt; muted;</span><br><span class="line">    <span class="comment">/** 记录 drain 方法批量导出消息时上次的偏移量 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> drainIndex;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>既然 RecordAccumulator 可以看做是一个消息缓存队列，那么这里先了解一下其消息存储的模式。这其中涉及到 RecordAccumulator、RecordBatch、MemoryRecords 和 MemoryRecordsBuilder 4 个类。从上面 RecordAccumulator 类的字段列表中我们看到有一个 <code>ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt;</code> 类型的 batches 字段，这里的 key 对应 topic 的某个分区，而 value 是一个 Deque 类型，其中封装了一批 RecordBatch 对象，这些对象中记录了待发送的消息集合，而这些消息的一个共同点就是都是发往相同的 topic 分区。RecordBatch 类字段定义如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">RecordBatch</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** 当前 RecordBatch 创建的时间戳 */</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> createdMs;</span><br><span class="line">    <span class="comment">/** 当前缓存的消息的目标 topic 分区 */</span></span><br><span class="line">    <span class="keyword">final</span> TopicPartition topicPartition;</span><br><span class="line">    <span class="comment">/** 标识当前 RecordBatch 发送之后的状态 */</span></span><br><span class="line">    <span class="keyword">final</span> ProduceRequestResult produceFuture;</span><br><span class="line">    <span class="comment">/** 消息的 Callback 队列，每个消息都对应一个 Callback 对象 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Thunk&gt; thunks = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="comment">/** 用来存储数据的 {<span class="doctag">@link</span> MemoryRecords} 对应的 builder 对象 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> MemoryRecordsBuilder recordsBuilder;</span><br><span class="line">    <span class="comment">/** 发送当前 RecordBatch 的重试次数 */</span></span><br><span class="line">    <span class="keyword">volatile</span> <span class="keyword">int</span> attempts;</span><br><span class="line">    <span class="comment">/** 最后一次重试发送的时间戳` */</span></span><br><span class="line">    <span class="keyword">long</span> lastAttemptMs;</span><br><span class="line">    <span class="comment">/** 记录保存的 record 个数 */</span></span><br><span class="line">    <span class="keyword">int</span> recordCount;</span><br><span class="line">    <span class="comment">/** 记录最大的 record 字节数 */</span></span><br><span class="line">    <span class="keyword">int</span> maxRecordSize;</span><br><span class="line">    <span class="comment">/** 记录上次投递当前 BatchRecord 的时间戳 */</span></span><br><span class="line">    <span class="keyword">long</span> drainedMs;</span><br><span class="line">    <span class="comment">/** 追后一次向当前 RecordBatch 追加消息的时间戳 */</span></span><br><span class="line">    <span class="keyword">long</span> lastAppendTime;</span><br><span class="line">    <span class="comment">/** 标记是否正在重试 */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> retry;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ... 省略方法定义</span></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>我们可以从字段定义中看到 RecordBatch 持有一个 MemoryRecordsBuilder 类型的字段，MemoryRecordsBuilder 是 MemoryRecords 的构造和管理器，也就是说 RecordBatch 本质上是以 MemoryRecords 作为存储介质。</p>
<p>了解了 RecordAccumulator 类在存储模式上的设计之后，我们接下来分析 <code>RecordAccumulator#append</code> 方法的实现：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 Callback callback,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> maxTimeToBlock)</span> <span class="keyword">throws</span> InterruptedException </span>{</span><br><span class="line">    <span class="comment">// 记录正在向收集器中追加消息的线程数</span></span><br><span class="line">    appendsInProgress.incrementAndGet();</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 获取当前 topic 分区对应的 Deque，如果不存在则创建一个</span></span><br><span class="line">        Deque&lt;RecordBatch&gt; dq = <span class="keyword">this</span>.getOrCreateDeque(tp);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) {</span><br><span class="line">            <span class="keyword">if</span> (closed) {</span><br><span class="line">                <span class="comment">// producer 已经被关闭了，抛出异常</span></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Cannot send after the producer is closed."</span>);</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// 向 Deque 中最后一个 RecordBatch 追加 Record，并返回对应的 RecordAppendResult 对象</span></span><br><span class="line">            RecordAppendResult appendResult = <span class="keyword">this</span>.tryAppend(timestamp, key, value, callback, dq);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) {</span><br><span class="line">                <span class="comment">// 追加成功，直接返回</span></span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 追加 Record 失败，尝试申请新的 buffer */</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, Records.LOG_OVERHEAD + Record.recordSize(key, value));</span><br><span class="line">        log.trace(<span class="string">"Allocating a new {} byte message buffer for topic {} partition {}"</span>, size, tp.topic(), tp.partition());</span><br><span class="line">        <span class="comment">// 申请新的 buffer</span></span><br><span class="line">        ByteBuffer buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) {</span><br><span class="line">            <span class="keyword">if</span> (closed) {</span><br><span class="line">                <span class="comment">// 再次校验 producer 状态，如果已经被关闭了，抛出异常</span></span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Cannot send after the producer is closed."</span>);</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 再次尝试向 Deque 中最后一个 RecordBatch 追加 Record</span></span><br><span class="line">            RecordAppendResult appendResult = <span class="keyword">this</span>.tryAppend(timestamp, key, value, callback, dq);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) {</span><br><span class="line">                <span class="comment">// 追加成功则返回，同时归还之前申请的 buffer</span></span><br><span class="line">                free.deallocate(buffer);</span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">/* 仍然追加失败，创建一个新的 RecordBatch 进行追加 */</span></span><br><span class="line"></span><br><span class="line">            MemoryRecordsBuilder recordsBuilder = MemoryRecords.builder(buffer, compression, TimestampType.CREATE_TIME, <span class="keyword">this</span>.batchSize);</span><br><span class="line">            RecordBatch batch = <span class="keyword">new</span> RecordBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">            <span class="comment">// 在新创建的 RecordBatch 中追加 Record</span></span><br><span class="line">            FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds()));</span><br><span class="line">            dq.addLast(batch);</span><br><span class="line">            <span class="comment">// 追加到未完成的集合中</span></span><br><span class="line">            incomplete.add(batch);</span><br><span class="line">            <span class="comment">// 封装成 RecordAppendResult 对象返回</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>);</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        appendsInProgress.decrementAndGet();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>追加消息到收集器的过程首先会获取指定 topic 分区对应的发送队列，如果不存在则会创建一个。然后同步往该队列的最后一个 RecordBatch 对象中追加数据，追加的过程位于 <code>RecordAccumulator#tryAppend</code> 方法中。如果追加失败，一般都是因为该 RecordBatch 没有足够的空间足以容纳，则方法会尝试申请新的空间，然后继续尝试追加。如果还是失败，则方法会创建一个新的 RecordBatch 对象进行追加。</p>
<p>Kafka 定义了 BufferPool 类以实现对 ByteBuffer 的复用，避免频繁创建和释放所带来的性能开销。不过需要注意的一点是，并不是所有的 ByteBuffer 对象都会被复用，BufferPool 对所管理的 ByteBuffer 对象的大小是有限制的（默认大小为 16KB，可以依据具体的应用场景适当调整 <code>batch.size</code> 配置进行修改），只有大小等于该值的 ByteBuffer 对象才会被 BufferPool 管理。</p>
<p>上述过程多次调用到 <code>RecordAccumulator#tryAppend</code> 方法，下面来看一下该方法的实现：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> RecordAppendResult <span class="title">tryAppend</span><span class="params">(<span class="keyword">long</span> timestamp, <span class="keyword">byte</span>[] key, <span class="keyword">byte</span>[] value, Callback callback, Deque&lt;RecordBatch&gt; deque)</span> </span>{</span><br><span class="line">    <span class="comment">// 获取 deque 的最后一个 RecordBatch</span></span><br><span class="line">    RecordBatch last = deque.peekLast();</span><br><span class="line">    <span class="keyword">if</span> (last != <span class="keyword">null</span>) {</span><br><span class="line">        <span class="comment">// 尝试往该 RecordBatch 末尾追加消息</span></span><br><span class="line">        FutureRecordMetadata future = last.tryAppend(timestamp, key, value, callback, time.milliseconds());</span><br><span class="line">        <span class="keyword">if</span> (future == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="comment">// 追加失败</span></span><br><span class="line">            last.close();</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 追加成功，将结果封装成 RecordAppendResult 对象返回</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, deque.size() &gt; <span class="number">1</span> || last.isFull(), <span class="keyword">false</span>);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// org.apache.kafka.clients.producer.internals.RecordBatch#tryAppend</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> FutureRecordMetadata <span class="title">tryAppend</span><span class="params">(<span class="keyword">long</span> timestamp, <span class="keyword">byte</span>[] key, <span class="keyword">byte</span>[] value, Callback callback, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    <span class="comment">// 检测是否还有多余的空间容纳该消息</span></span><br><span class="line">    <span class="keyword">if</span> (!recordsBuilder.hasRoomFor(key, value)) {</span><br><span class="line">        <span class="comment">// 没有多余的空间则直接返回，后面会尝试申请新的空间</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 添加当前消息到 MemoryRecords，并返回消息对应的 CRC32 校验码</span></span><br><span class="line">    <span class="keyword">long</span> checksum = <span class="keyword">this</span>.recordsBuilder.append(timestamp, key, value);</span><br><span class="line">    <span class="comment">// 更新最大 record 字节数</span></span><br><span class="line">    <span class="keyword">this</span>.maxRecordSize = Math.max(<span class="keyword">this</span>.maxRecordSize, Record.recordSize(key, value));</span><br><span class="line">    <span class="comment">// 更新最后一次追加记录时间戳</span></span><br><span class="line">    <span class="keyword">this</span>.lastAppendTime = now;</span><br><span class="line">    FutureRecordMetadata future = <span class="keyword">new</span> FutureRecordMetadata(</span><br><span class="line">            produceFuture, recordCount,</span><br><span class="line">            timestamp, checksum,</span><br><span class="line">            key == <span class="keyword">null</span> ? -<span class="number">1</span> : key.length,</span><br><span class="line">            value == <span class="keyword">null</span> ? -<span class="number">1</span> : value.length);</span><br><span class="line">    <span class="keyword">if</span> (callback != <span class="keyword">null</span>) {</span><br><span class="line">        <span class="comment">// 如果指定了 Callback，将 Callback 和 FutureRecordMetadata 封装到 Trunk 中</span></span><br><span class="line">        thunks.add(<span class="keyword">new</span> Thunk(callback, future));</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">this</span>.recordCount++;</span><br><span class="line">    <span class="keyword">return</span> future;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上面过程最终调用 <code>MemoryRecordsBuilder#append</code> 方法将消息追加到 MemoryRecords 相应的位置进行存储，并返回消息的 CRC32 校验码，至于 MemoryRecords 存储消息的细节这里不再继续深入。消息追加成功之后，如果在发送消息时指定了 Callback 函数，那么这里会将其封装成 Thunk 类对象，至于其作用这里先不展开分析，等到后面分析 Sender 线程的执行过程时再一探究竟，这里初步猜测 Sender 线程在向集群投递完消息并收到来自集群的响应时会循环遍历 thunks 集合，并应用 Callback 对应的回调方法。</p>
<p>回到 <code>KafkaProducer#doSend</code> 方法，来看最后一步（ <strong>步骤 6</strong> ）。上面追加的过程会返回一个 RecordAppendResult 对象，该对象通过 <code>RecordAppendResult#batchIsFull</code> 和 <code>RecordAppendResult#newBatchCreated</code> 两个字段分别标记了追加过程中末端的 RecordBatch 是否已满，以及追加过程中是否有创建新的 RecordBatch 对象，如果这两个条件满足其中之一，则会唤醒 Sender 线程尝试向集群投递收集的消息数据。</p>
<p>最后提一点，RecordAccumulator 作为消息的收集器，其内存容量是有上限的，默认为 32MB（可以通过 <code>buffer.memory</code> 参数配置），当容量已满时调用 <code>KafkaProducer#send</code> 方法发送消息会被阻塞，当阻塞超过一定时间（默认为 60 秒，可以通过 <code>max.block.ms</code> 参数配置）则抛出异常。</p>

        <h4 id="投递待发送的消息">
          <a href="#投递待发送的消息" class="heading-link"><i class="fas fa-link"></i></a>投递待发送的消息</h4>
      <p>前面曾提出一个概念，即客户端发送消息的过程实际上是一个异步的过程，由 2 个线程协同执行，其中 1 个线程将待发送的消息写入缓冲区，另外 1 个线程（Sender 线程）负责定期定量将缓冲区中的数据投递给远端 Kafka 集群，并反馈投递结果。上面我们分析了过程 1，下面我们继续分析过程 2，即将缓存的消息发送给 Kafka 集群。</p>
<p>这一过程由 Sender 线程负责执行，前面的分析中曾多次唤醒过该线程，下面来看一下其实现，位于 Sender 类中，该类实现了 <code>java.lang.Runnable</code> 接口，其 <code>Sender#run</code> 方法实现如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 主循环，一直运行直到 KafkaProducer 被关闭</span></span><br><span class="line">    <span class="keyword">while</span> (running) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="keyword">this</span>.run(time.milliseconds());</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Uncaught error in kafka producer I/O thread: "</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 如果 KafkaProducer 被关闭，尝试发送剩余的消息 */</span></span><br><span class="line">    <span class="keyword">while</span> (!forceClose <span class="comment">// 不是强制关闭</span></span><br><span class="line">            <span class="comment">// 存在未发送或已发送待响应的请求</span></span><br><span class="line">            &amp;&amp; (<span class="keyword">this</span>.accumulator.hasUnsent() || <span class="keyword">this</span>.client.inFlightRequestCount() &gt; <span class="number">0</span>)) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="keyword">this</span>.run(time.milliseconds());</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Uncaught error in kafka producer I/O thread: "</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果是强制关闭，忽略所有未发送和已发送待响应的请求</span></span><br><span class="line">    <span class="keyword">if</span> (forceClose) {</span><br><span class="line">        <span class="comment">// 丢弃所有未发送完成的消息</span></span><br><span class="line">        <span class="keyword">this</span>.accumulator.abortIncompleteBatches();</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 关闭网络连接</span></span><br><span class="line">        <span class="keyword">this</span>.client.close();</span><br><span class="line">    } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">        log.error(<span class="string">"Failed to close network client"</span>, e);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>由上述方法实现可知，Sender 线程在启动后会一直循环执行另外一个重载版本的 <code>Sender#run</code> 方法，其中包含了 Sender 线程的主要逻辑。如果客户端被关闭（一般都是调用 <code>KafkaProducer#close</code> 方法），在不是强制关闭的前提下，Sender 线程会继续处理本地未发送和已发送但未收到服务端确认的消息，如果是强制关闭（在调用 <code>KafkaProducer#close</code> 方法时允许指定超时等待时间，如果在既定时间内客户端仍未完成对缓存消息的处理，则会触发强制关闭机制），则会丢弃本地缓存的所有未发送的消息，最后关闭到 Kafka 集群的网络连接。</p>
<p>下面来看一下 Sender 线程的核心实现，即重载版本的 <code>Sender#run</code> 方法：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">(<span class="keyword">long</span> now)</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 计算需要以及可以向哪些节点发送请求</span></span><br><span class="line">    Cluster cluster = metadata.fetch(); <span class="comment">// 获取 kafka 集群信息</span></span><br><span class="line">    RecordAccumulator.ReadyCheckResult result = <span class="keyword">this</span>.accumulator.ready(cluster, now); <span class="comment">// 计算需要向哪些节点发送请求</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 如果存在未知的 leader 副本对应的节点（对应的 topic 分区正在执行 leader 选举，或者对应的 topic 已经失效），标记需要更新缓存的集群元数据信息</span></span><br><span class="line">    <span class="keyword">if</span> (!result.unknownLeaderTopics.isEmpty()) {</span><br><span class="line">        <span class="keyword">for</span> (String topic : result.unknownLeaderTopics) <span class="keyword">this</span>.metadata.add(topic);</span><br><span class="line">        <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 遍历处理待发送请求的目标节点，基于网络 IO 检查对应节点是否可用，对于不可用的节点则剔除</span></span><br><span class="line">    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">    <span class="keyword">long</span> notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) {</span><br><span class="line">        Node node = iter.next();</span><br><span class="line">        <span class="comment">// 检查目标节点是否准备好接收请求，如果未准备好但目标节点允许创建连接，则创建到目标节点的连接</span></span><br><span class="line">        <span class="keyword">if</span> (!<span class="keyword">this</span>.client.ready(node, now)) {</span><br><span class="line">            <span class="comment">// 对于未准备好的节点，则从 ready 集合中删除</span></span><br><span class="line">            iter.remove();</span><br><span class="line">            notReadyTimeout = Math.min(notReadyTimeout, <span class="keyword">this</span>.client.connectionDelay(node, now));</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 获取每个节点待发送消息集合，其中 key 是目标 leader 副本所在节点 ID</span></span><br><span class="line">    Map&lt;Integer, List&lt;RecordBatch&gt;&gt; batches =</span><br><span class="line">            <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes, <span class="keyword">this</span>.maxRequestSize, now);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 如果需要保证消息的强顺序性，则缓存对应 topic 分区对象，防止同一时间往同一个 topic 分区发送多条处于未完成状态的消息</span></span><br><span class="line">    <span class="keyword">if</span> (guaranteeMessageOrder) {</span><br><span class="line">        <span class="comment">// 将所有 RecordBatch 的 topic 分区对象加入到 muted 集合中</span></span><br><span class="line">        <span class="comment">// 防止同一时间往同一个 topic 分区发送多条处于未完成状态的消息</span></span><br><span class="line">        <span class="keyword">for</span> (List&lt;RecordBatch&gt; batchList : batches.values()) {</span><br><span class="line">            <span class="keyword">for</span> (RecordBatch batch : batchList)</span><br><span class="line">                <span class="keyword">this</span>.accumulator.mutePartition(batch.topicPartition);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6. 处理本地过期的消息，返回 TimeoutException，并释放空间</span></span><br><span class="line">    List&lt;RecordBatch&gt; expiredBatches = <span class="keyword">this</span>.accumulator.abortExpiredBatches(<span class="keyword">this</span>.requestTimeout, now);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果存在待发送的消息，则设置 pollTimeout 等于 0，这样可以立即发送请求，从而能够缩短剩余消息的缓存时间，避免堆积</span></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class="line">    <span class="keyword">if</span> (!result.readyNodes.isEmpty()) {</span><br><span class="line">        log.trace(<span class="string">"Nodes with data ready to send: {}"</span>, result.readyNodes);</span><br><span class="line">        pollTimeout = <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7. 发送请求到服务端，并处理服务端响应</span></span><br><span class="line">    <span class="keyword">this</span>.sendProduceRequests(batches, now);</span><br><span class="line">    <span class="keyword">this</span>.client.poll(pollTimeout, now);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>发送收集器 RecordAccumulator 中缓存的消息到 Kafka 集群的整体执行流程可以概括为如下 7 个步骤：</p>
<ol>
<li>计算需要向哪些 broker 节点投递消息；</li>
<li>如果步骤 1 中发现一些 topic 分区的 Leader 副本所在 broker 节点失效，则需要标记更新本地缓存的集群元数据信息；</li>
<li>遍历处理步骤 1 中获取到的 broker 节点集合，基于 I/O 检测对应节点是否可用，如果不可用则剔除；</li>
<li>以 broker 节点 ID 为键，获取发往目标节点的消息集合；</li>
<li>如果需要对消息顺序进行强一致性保证，则需要缓存当前目标 topic 分区对象，防止同一时间往同一个 topic 分区发送多条处于未完成状态的消息；</li>
<li>处理本地已过期的消息，返回超时异常，并释放占据的空间；</li>
<li>发送消息到服务端，并处理服务端的响应。</li>
</ol>
<p>下面就各个步骤展开说明，首先来看 <strong>步骤 1</strong> ，该步骤用于计算需要向哪些节点投递消息，实现位于 <code>RecordAccumulator#ready</code> 方法中：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ReadyCheckResult <span class="title">ready</span><span class="params">(Cluster cluster, <span class="keyword">long</span> nowMs)</span> </span>{</span><br><span class="line">    <span class="comment">// 用于记录接收请求的节点</span></span><br><span class="line">    Set&lt;Node&gt; readyNodes = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    <span class="comment">// 记录下次执行 ready 判断的时间间隔</span></span><br><span class="line">    <span class="keyword">long</span> nextReadyCheckDelayMs = Long.MAX_VALUE;</span><br><span class="line">    <span class="comment">// 记录找不到 leader 副本的分区对应的 topic 集合</span></span><br><span class="line">    Set&lt;String&gt; unknownLeaderTopics = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 是否有线程在等待 BufferPool 分配空间</span></span><br><span class="line">    <span class="keyword">boolean</span> exhausted = <span class="keyword">this</span>.free.queued() &gt; <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 遍历每个 topic 分区及其 RecordBatch 队列，对每个分区的 leader 副本所在的节点执行判定</span></span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; entry : <span class="keyword">this</span>.batches.entrySet()) {</span><br><span class="line">        TopicPartition part = entry.getKey();</span><br><span class="line">        Deque&lt;RecordBatch&gt; deque = entry.getValue();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取当前 topic 分区 leader 副本所在的节点</span></span><br><span class="line">        Node leader = cluster.leaderFor(part);</span><br><span class="line">        <span class="keyword">synchronized</span> (deque) {</span><br><span class="line">            <span class="comment">// 当前分区 leader 副本未知，但存在发往该分区的消息</span></span><br><span class="line">            <span class="keyword">if</span> (leader == <span class="keyword">null</span> &amp;&amp; !deque.isEmpty()) {</span><br><span class="line">                unknownLeaderTopics.add(part.topic());</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// 如果需要保证消息顺序性，则不应该存在多个发往该 leader 副本节点且未完成的消息</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (!readyNodes.contains(leader) &amp;&amp; !muted.contains(part)) {</span><br><span class="line">                RecordBatch batch = deque.peekFirst();</span><br><span class="line">                <span class="keyword">if</span> (batch != <span class="keyword">null</span>) {</span><br><span class="line">                    <span class="comment">// 当前为重试操作，且重试时间间隔未达到阈值时间</span></span><br><span class="line">                    <span class="keyword">boolean</span> backingOff = batch.attempts &gt; <span class="number">0</span> &amp;&amp; batch.lastAttemptMs + retryBackoffMs &gt; nowMs;</span><br><span class="line">                    <span class="keyword">long</span> waitedTimeMs = nowMs - batch.lastAttemptMs; <span class="comment">// 重试等待的时间</span></span><br><span class="line">                    <span class="keyword">long</span> timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;</span><br><span class="line">                    <span class="keyword">long</span> timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, <span class="number">0</span>);</span><br><span class="line">                    <span class="keyword">boolean</span> full = deque.size() &gt; <span class="number">1</span> || batch.isFull();</span><br><span class="line">                    <span class="keyword">boolean</span> expired = waitedTimeMs &gt;= timeToWaitMs;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 标记当前节点是否可以接收请求</span></span><br><span class="line">                    <span class="keyword">boolean</span> sendable = full <span class="comment">// 1. 队列中有多个 RecordBatch，或第一个 RecordBatch 已满</span></span><br><span class="line">                            || expired <span class="comment">// 2. 当前等待重试的时间过长</span></span><br><span class="line">                            || exhausted <span class="comment">// 3. 有其他线程在等待 BufferPool 分配空间，即本地消息缓存已满</span></span><br><span class="line">                            || closed <span class="comment">// 4. producer 已经关闭</span></span><br><span class="line">                            || flushInProgress(); <span class="comment">// 5. 有线程正在等待 flush 操作完成</span></span><br><span class="line">                    <span class="keyword">if</span> (sendable &amp;&amp; !backingOff) {</span><br><span class="line">                        <span class="comment">// 允许发送消息，且当前为首次发送，或者重试等待时间已经较长，则记录目标 leader 副本所在节点</span></span><br><span class="line">                        readyNodes.add(leader);</span><br><span class="line">                    } <span class="keyword">else</span> {</span><br><span class="line">                        <span class="comment">// 更新下次执行 ready 判定的时间间隔</span></span><br><span class="line">                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 封装结果返回</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>整个计算的逻辑就是遍历我们之前缓存到收集器 RecordAccumulator 中的消息集合，并按照下面 5 个条件进行判定，如果满足其中一个则认为需要往目标节点投递消息：</p>
<ol>
<li>当前 topic 名下的消息队列持有多个 RecordBatch，或者第 1 个 RecordBatch 已满。</li>
<li>当前 topic 分区等待重试的时间过长，如果是首次发送则无需校验重试等待时间。</li>
<li>当前 topic 分区下有其他线程在等待 BufferPool 分配空间，即本地缓存已满。</li>
<li>Producer 被关闭，需要立即投递剩余未完成的消息。</li>
<li>有线程正在等待 flush 操作完成，则需要立即投递消息，避免线程等待时间过长。</li>
</ol>
<p>如果遍历过程中发现某个 topic 分区对应的 Leader 副本所在节点失效（对应的 topic 分区正在执行 Leader 副本选举，或者对应的 topic 已经失效），但是本地又缓存了发往该分区的消息，则需要标记当前本地缓存的集群元数据需要更新（ <strong>步骤 2</strong> ）。上面获取目标 broker 节点的过程是站在收集器 RecordAccumulator 的角度看的，对于一个节点是否可用，还需要从网络 I/O 的角度检查其连通性，这也是 <strong>步骤 3</strong> 所要做的工作，这一步基于 <code>KafkaClient#ready</code> 方法检查目标节点的是否连通，如果目标节点并未准备好接收请求，则需要从待请求节点集合中剔除。</p>
<p>知道了需要向哪些节点投递消息，接下来自然而然就需要获取发往每个节点的数据， <strong>步骤 4</strong> 的实现位于 <code>RecordAccumulator#drain</code> 方法中：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;RecordBatch&gt;&gt; drain(Cluster cluster, Set&lt;Node&gt; nodes, <span class="keyword">int</span> maxSize, <span class="keyword">long</span> now) {</span><br><span class="line">    <span class="keyword">if</span> (nodes.isEmpty()) {</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录转换后的结果，key 是目标节点 ID</span></span><br><span class="line">    Map&lt;Integer, List&lt;RecordBatch&gt;&gt; batches = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Node node : nodes) {</span><br><span class="line">        <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 获取当前节点上的分区信息</span></span><br><span class="line">        List&lt;PartitionInfo&gt; parts = cluster.partitionsForNode(node.id());</span><br><span class="line">        <span class="comment">// 记录待发往当前节点的 RecordBatch 集合</span></span><br><span class="line">        List&lt;RecordBatch&gt; ready = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * drainIndex 用于记录上次发送停止的位置，本次继续从当前位置开始发送，</span></span><br><span class="line"><span class="comment">         * 如果每次都是从 0 位置开始，可能会导致排在后面的分区饿死，可以看做是一个简单的负载均衡策略</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">int</span> start = drainIndex = drainIndex % parts.size();</span><br><span class="line">        <span class="keyword">do</span> {</span><br><span class="line">            PartitionInfo part = parts.get(drainIndex);</span><br><span class="line">            TopicPartition tp = <span class="keyword">new</span> TopicPartition(part.topic(), part.partition());</span><br><span class="line">            <span class="comment">// 如果需要保证消息强顺序性，则不应该同时存在多个发往目标分区的消息</span></span><br><span class="line">            <span class="keyword">if</span> (!muted.contains(tp)) {</span><br><span class="line">                <span class="comment">// 获取当前分区对应的 RecordBatch 集合</span></span><br><span class="line">                Deque&lt;RecordBatch&gt; deque = <span class="keyword">this</span>.getDeque(<span class="keyword">new</span> TopicPartition(part.topic(), part.partition()));</span><br><span class="line">                <span class="keyword">if</span> (deque != <span class="keyword">null</span>) {</span><br><span class="line">                    <span class="keyword">synchronized</span> (deque) {</span><br><span class="line">                        RecordBatch first = deque.peekFirst();</span><br><span class="line">                        <span class="keyword">if</span> (first != <span class="keyword">null</span>) {</span><br><span class="line">                            <span class="comment">// 重试 &amp;&amp; 重试时间间隔未达到阈值时间</span></span><br><span class="line">                            <span class="keyword">boolean</span> backoff = first.attempts &gt; <span class="number">0</span> &amp;&amp; first.lastAttemptMs + retryBackoffMs &gt; now;</span><br><span class="line">                            <span class="comment">// 仅发送第一次发送，或重试等待时间较长的消息</span></span><br><span class="line">                            <span class="keyword">if</span> (!backoff) {</span><br><span class="line">                                <span class="keyword">if</span> (size + first.sizeInBytes() &gt; maxSize &amp;&amp; !ready.isEmpty()) {</span><br><span class="line">                                    <span class="comment">// 单次消息数据量已达到上限，结束循环，一般对应一个请求的大小，防止请求消息过大</span></span><br><span class="line">                                    <span class="keyword">break</span>;</span><br><span class="line">                                } <span class="keyword">else</span> {</span><br><span class="line">                                    <span class="comment">// 每次仅获取第一个 RecordBatch，并放入 read 列表中，这样给每个分区一个机会，保证公平，防止饥饿</span></span><br><span class="line">                                    RecordBatch batch = deque.pollFirst();</span><br><span class="line">                                    <span class="comment">// 将当前 RecordBatch 设置为只读</span></span><br><span class="line">                                    batch.close();</span><br><span class="line">                                    size += batch.sizeInBytes();</span><br><span class="line">                                    ready.add(batch);</span><br><span class="line">                                    batch.drainedMs = now;</span><br><span class="line">                                }</span><br><span class="line">                            }</span><br><span class="line">                        }</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// 更新 drainIndex</span></span><br><span class="line">            <span class="keyword">this</span>.drainIndex = (<span class="keyword">this</span>.drainIndex + <span class="number">1</span>) % parts.size();</span><br><span class="line">        } <span class="keyword">while</span> (start != drainIndex);</span><br><span class="line">        batches.put(node.id(), ready);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> batches;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法的返回类型是 <code>Map&lt;Integer, List&lt;RecordBatch&gt;&gt;</code>，其中 key 是目标节点的 ID，value 是本次待发往该节点的消息集合。为了防止饥饿，方法会轮询从当前 topic 的每个分区队列对头取数据，并记录每次轮询的偏移量，下次轮询即从该偏移量位置开始，以保证尽量的公平。</p>
<p>下面来看一下 <strong>步骤 5</strong> ，这是客户端保证消息绝对有序的逻辑。在具体分析之前，我们先来看一个导致消息顺序错乱的场景。假设生产者发送了 2 条指向同一个目标 topic 分区的消息 A 和 B，但是 A 发送失败，B 却成功了，此时生产者会重发消息 A，结果就变成了 B 消息排在了 A 消息的前面。解决该问题的方法就是将参数 <code>max.in.flight.requests.per.connection</code> 参数设置为 1，以禁止生产者往同一个分区一次发送多条消息，不过这样会严重降低系统吞吐量，只有在对消息顺序有严格要求时才推荐这样做。步骤 5 的参数 <code>guaranteeMessageOrder=true</code> 对应着 <code>max.in.flight.requests.per.connection=1</code>，客户端解决上述问题的实现方式也很简单，就是在本地缓存有处于发送中消息对应的目标 topic 分区对象，保证该分区上的消息在被正确响应之前不会再投递第 2 条消息。</p>
<p>下面继续来看 <strong>步骤 6</strong> ，这一步会遍历收集器 RecordAccumulator 中缓存的 RecordBatch，并调用 <code>RecordBatch#maybeExpire</code> 方法检测当前 RecordBatch 是否过期，对于已经过期的 RecordBatch 会执行相应的 <code>RecordBatch#done</code> 方法（下一步中会对该方法展开说明），并释放占用的内存空间。</p>
<p>最后我们来看一下消息发送的过程（ <strong>步骤 7</strong> ），位于 <code>Sender#sendProduceRequests</code> 方法中：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequests</span><span class="params">(Map&lt;Integer, List&lt;RecordBatch&gt;&gt; collated, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    <span class="comment">// 遍历处理待发送消息集合，key 是目标节点 ID</span></span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Integer, List&lt;RecordBatch&gt;&gt; entry : collated.entrySet())</span><br><span class="line">        <span class="keyword">this</span>.sendProduceRequest(now, entry.getKey(), acks, requestTimeout, entry.getValue());</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;RecordBatch&gt; batches)</span> </span>{</span><br><span class="line">    <span class="comment">// 遍历 RecordBatch 集合，整理成 produceRecordsByPartition 和 recordsByPartition</span></span><br><span class="line">    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, RecordBatch&gt; recordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line">    <span class="keyword">for</span> (RecordBatch batch : batches) {</span><br><span class="line">        TopicPartition tp = batch.topicPartition;</span><br><span class="line">        produceRecordsByPartition.put(tp, batch.records());</span><br><span class="line">        recordsByPartition.put(tp, batch);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 ProduceRequest 请求构造器</span></span><br><span class="line">    ProduceRequest.Builder requestBuilder = <span class="keyword">new</span> ProduceRequest.Builder(acks, timeout, produceRecordsByPartition);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建回调对象，用于处理响应</span></span><br><span class="line">    RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() {</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>{</span><br><span class="line">            handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line">        }</span><br><span class="line">    };</span><br><span class="line"></span><br><span class="line">    String nodeId = Integer.toString(destination);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 ClientRequest 请求对象，如果 acks 不等于 0 则表示期望获取服务端响应</span></span><br><span class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>, callback);</span><br><span class="line">    <span class="comment">// 缓存 ClientRequest 请求对象到 InFlightRequests 中</span></span><br><span class="line">    client.send(clientRequest, now);</span><br><span class="line">    log.trace(<span class="string">"Sent produce request to {}: {}"</span>, nodeId, requestBuilder);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>这一步主要逻辑就是创建客户端请求 ClientRequest 对象，并通过 <code>NetworkClient#send</code> 方法将请求加入到网络 I/O 通道（KafkaChannel）中。同时将该对象缓存到 InFlightRequests 中，等接收到服务端响应时会通过缓存的 ClientRequest 对象调用对应的 callback 方法。最后调用 <code>NetworkClient#poll</code> 方法执行具体的网络请求和响应。</p>
<p>InFlightRequests 类的主要作用是缓存那些已经发送出去但是还未收到响应的请求，并支持控制对单个节点的最大未完成请求数（默认值为 5，可以通过 <code>max.in.flight.requests.per.connection</code> 参数进行配置，但是上限不允许超过 5 个）。</p>
<p>下面来看一下 <code>NetworkClient#poll</code> 方法的具体实现：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;ClientResponse&gt; <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 如果距离上次更新超过指定时间，且存在负载小的目标节点，</span></span><br><span class="line"><span class="comment">     * 则创建 MetadataRequest 请求更新本地缓存的集群元数据信息，并在下次执行 poll 操作时一并送出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">long</span> metadataTimeout = metadataUpdater.maybeUpdate(now);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 发送网络请求 */</span></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">this</span>.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs));</span><br><span class="line">    } <span class="keyword">catch</span> (IOException e) {</span><br><span class="line">        log.error(<span class="string">"Unexpected error during I/O"</span>, e);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 处理服务端响应 */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> updatedNow = <span class="keyword">this</span>.time.milliseconds();</span><br><span class="line">    List&lt;ClientResponse&gt; responses = <span class="keyword">new</span> ArrayList&lt;&gt;(); <span class="comment">// 响应队列</span></span><br><span class="line">    <span class="comment">// 添加需要被丢弃的请求对应的响应到 responses 队列中，都是一些版本不匹配的请求</span></span><br><span class="line">    <span class="keyword">this</span>.handleAbortedSends(responses);</span><br><span class="line">    <span class="comment">// 对于发送成功且不期望服务端响应的请求，创建本地的响应对象添加到 responses 队列中</span></span><br><span class="line">    <span class="keyword">this</span>.handleCompletedSends(responses, updatedNow);</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 获取并解析服务端响应</span></span><br><span class="line"><span class="comment">     * - 如果是更新集群元数据对应的响应，则更新本地缓存的集群元数据信息</span></span><br><span class="line"><span class="comment">     * - 如果是更新 API 版本的响应，则更新本地缓存的目标节点支持的 API 版本信息</span></span><br><span class="line"><span class="comment">     * - 否则，获取 ClientResponse 添加到 responses 队列中</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">this</span>.handleCompletedReceives(responses, updatedNow);</span><br><span class="line">    <span class="comment">// 处理连接断开的请求，构建对应的 ClientResponse 添加到 responses 列表中，并标记需要更新集群元数据信息</span></span><br><span class="line">    <span class="keyword">this</span>.handleDisconnections(responses, updatedNow);</span><br><span class="line">    <span class="comment">// 处理 connections 列表，更新相应节点的连接状态</span></span><br><span class="line">    <span class="keyword">this</span>.handleConnections();</span><br><span class="line">    <span class="comment">// 如果需要更新本地的 API 版本信息，则创建对应的 ApiVersionsRequest 请求，并在下次执行 poll 操作时一并送出</span></span><br><span class="line">    <span class="keyword">this</span>.handleInitiateApiVersionRequests(updatedNow);</span><br><span class="line">    <span class="comment">// 遍历获取 inFlightRequests 中的超时请求，构建对应的 ClientResponse 添加到 responses 列表中，并标记需要更新集群元数据信息</span></span><br><span class="line">    <span class="keyword">this</span>.handleTimedOutRequests(responses, updatedNow);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历处理响应对应的 onComplete 方法</span></span><br><span class="line">    <span class="keyword">for</span> (ClientResponse response : responses) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// 本质上就是在调用注册的 RequestCompletionHandler#onComplete 方法</span></span><br><span class="line">            response.onComplete();</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Uncaught error in request completion:"</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> responses;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>整个方法的执行流程可以概括为 4 个步骤：</p>
<ol>
<li>检测是否需要更新本地缓存的集群元数据信息，如果需要则创建对应的 MetadataRequest 请求，并在下次 <code>Selector#poll</code> 操作时一并送出；</li>
<li>执行 <code>Selector#poll</code> 操作，向服务端发送网络请求；</li>
<li>处理服务端响应；</li>
<li>遍历应用注册的 <code>RequestCompletionHandler#onComplete</code> 方法。</li>
</ol>
<p>首先来看更新本地缓存的集群元数据信息的过程（ <strong>步骤 1</strong> ），前面曾多次提及到更新集群元数据的场景，而这些更新操作实际上都是标记集群元数据需要更新，真正执行更新的操作则发生在这里。实现位于 <code>DefaultMetadataUpdater#maybeUpdate</code> 方法中：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">maybeUpdate</span><span class="params">(<span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    <span class="comment">// 获取下次更新集群信息的时间戳</span></span><br><span class="line">    <span class="keyword">long</span> timeToNextMetadataUpdate = metadata.timeToNextUpdate(now);</span><br><span class="line">    <span class="comment">// 检查是否已经发送了 MetadataRequest 请求</span></span><br><span class="line">    <span class="keyword">long</span> waitForMetadataFetch = <span class="keyword">this</span>.metadataFetchInProgress ? requestTimeoutMs : <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 计算当前距离下次发送 MetadataRequest 请求的时间差</span></span><br><span class="line">    <span class="keyword">long</span> metadataTimeout = Math.max(timeToNextMetadataUpdate, waitForMetadataFetch);</span><br><span class="line">    <span class="keyword">if</span> (metadataTimeout &gt; <span class="number">0</span>) {</span><br><span class="line">        <span class="comment">// 如果时间还未到，则暂时不更新</span></span><br><span class="line">        <span class="keyword">return</span> metadataTimeout;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 寻找负载最小的可用节点，如果没有可用的节点则返回 null</span></span><br><span class="line">    Node node = leastLoadedNode(now);</span><br><span class="line">    <span class="keyword">if</span> (node == <span class="keyword">null</span>) {</span><br><span class="line">        log.debug(<span class="string">"Give up sending metadata request since no node is available"</span>);</span><br><span class="line">        <span class="keyword">return</span> reconnectBackoffMs;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查是否允许向目标节点发送请求，如果允许则创建 MetadataRequest 请求，并在下次执行 poll 操作时一并送出</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.maybeUpdate(now, node);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">maybeUpdate</span><span class="params">(<span class="keyword">long</span> now, Node node)</span> </span>{</span><br><span class="line">    String nodeConnectionId = node.idString();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果允许向该节点发送请求</span></span><br><span class="line">    <span class="keyword">if</span> (canSendRequest(nodeConnectionId)) {</span><br><span class="line">        <span class="comment">// 标识正在请求更新集群元数据信息</span></span><br><span class="line">        <span class="keyword">this</span>.metadataFetchInProgress = <span class="keyword">true</span>;</span><br><span class="line">        <span class="comment">// 创建集群元数据请求 MetadataRequest 对象</span></span><br><span class="line">        MetadataRequest.Builder metadataRequest;</span><br><span class="line">        <span class="keyword">if</span> (metadata.needMetadataForAllTopics()) {</span><br><span class="line">            <span class="comment">// 需要更新所有 topic 的元数据信息</span></span><br><span class="line">            metadataRequest = MetadataRequest.Builder.allTopics();</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 仅需更新指定 topic 的元数据信息</span></span><br><span class="line">            metadataRequest = <span class="keyword">new</span> MetadataRequest.Builder(<span class="keyword">new</span> ArrayList&lt;&gt;(metadata.topics()));</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将 MetadataRequest 包装成 ClientRequest 进行发送，在下次执行 poll 操作时一并发送</span></span><br><span class="line">        log.debug(<span class="string">"Sending metadata request {} to node {}"</span>, metadataRequest, node.id());</span><br><span class="line">        sendInternalMetadataRequest(metadataRequest, nodeConnectionId, now);</span><br><span class="line">        <span class="keyword">return</span> requestTimeoutMs;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 不允许向目标节点发送请求 */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果存在到目标节点的连接，则等待一会，无需再次尝试创建新的连接</span></span><br><span class="line">    <span class="keyword">if</span> (isAnyNodeConnecting()) {</span><br><span class="line">        <span class="keyword">return</span> reconnectBackoffMs;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* 如果不存在到目标节点连接 */</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果允许创建到目标节点的连接，则初始化连接</span></span><br><span class="line">    <span class="keyword">if</span> (connectionStates.canConnect(nodeConnectionId, now)) {</span><br><span class="line">        log.debug(<span class="string">"Initialize connection to node {} for sending metadata request"</span>, node.id());</span><br><span class="line">        initiateConnect(node, now); <span class="comment">// 初始化连接</span></span><br><span class="line">        <span class="keyword">return</span> reconnectBackoffMs;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Long.MAX_VALUE;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>方法首先会依据之前设置的标记，以及上次的更新时间决定是否需要更新集群元数据信息，如果需要则依据本地记录的已发往服务端的请求数目寻找集群中负载最小且可用的节点，并创建对应的 MetadataRequest 请求，但是这里的请求不是立即发出的，而是将请求包装成 ClientRequest 对象，并在下次 <code>Selector#poll</code> 操作时一并送出，也就是接下去即将执行的步骤 2。</p>
<p><strong>步骤 2</strong> 是真正发送网络请求的地方，这里的请求是异步的，客户端在发出请求之后继续执行步骤 3。 <strong>步骤 3</strong> 的逻辑主要是为每一个 ClientRequest 请求构造对应的 ClientResponse 响应对象，这些响应对象有的是依据服务端的响应进行构造，有的则是在本地伪造，因为不是所有的请求都需要等待服务端的响应，也不是所有的请求都能得到服务端的响应。这一步的实现对应了一系列的 <code>handle*</code> 方法：</p>
<blockquote>
<ul>
<li>handleAbortedSends</li>
<li>handleCompletedSends</li>
<li>handleCompletedReceives</li>
<li>handleDisconnections</li>
<li>handleConnections</li>
<li>handleInitiateApiVersionRequests</li>
<li>handleTimedOutRequests</li>
</ul>
</blockquote>
<p>下面逐一来看一下相应方法的实现。</p>
<ul>
<li><strong>handleAbortedSends</strong></li>
</ul>
<p>该方法的实现就是简单的将 <code>NetworkClient#abortedSends</code> 字段中记录的 ClientResponse 响应对象添加到结果集合中，并清空该字段。这些 ClientResponse 对象是在 <code>NetworkClient#doSend</code> 时添加的，添加的原因是本地请求与目标节点所支持的 API 版本不匹配。</p>
<ul>
<li><strong>handleCompletedSends</strong></li>
</ul>
<p>该方法会遍历客户端已经发送成功的请求，对于那些不期望服务端响应的请求可以直接创建对应的 ClientResponse 响应对象，并添加到结果集合中。实现如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleCompletedSends</span><span class="params">(List&lt;ClientResponse&gt; responses, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    <span class="keyword">for</span> (Send send : <span class="keyword">this</span>.selector.completedSends()) {</span><br><span class="line">        <span class="comment">// 获取缓存到 inFlightRequests 集合中的请求对象</span></span><br><span class="line">        InFlightRequest request = <span class="keyword">this</span>.inFlightRequests.lastSent(send.destination());</span><br><span class="line">        <span class="comment">// 检测请求是否期望响应</span></span><br><span class="line">        <span class="keyword">if</span> (!request.expectResponse) {</span><br><span class="line">            <span class="comment">// 当前请求不期望服务端响应，则从 inFlightRequests 集合中删除</span></span><br><span class="line">            <span class="keyword">this</span>.inFlightRequests.completeLastSent(send.destination());</span><br><span class="line">            <span class="comment">// 为当前请求生成 ClientResponse 对象</span></span><br><span class="line">            responses.add(request.completed(<span class="keyword">null</span>, now));</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<ul>
<li><strong>handleCompletedReceives</strong></li>
</ul>
<p>该方法会获取并解析服务端的响应结果，并依据响应类型分别处理。实现如下：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">handleCompletedReceives</span><span class="params">(List&lt;ClientResponse&gt; responses, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    <span class="keyword">for</span> (NetworkReceive receive : <span class="keyword">this</span>.selector.completedReceives()) {</span><br><span class="line">        <span class="comment">// 获取返回响应的节点 ID</span></span><br><span class="line">        String source = receive.source();</span><br><span class="line">        <span class="comment">// 从 inFlightRequests 集合中获取缓存的 ClientRequest 对象</span></span><br><span class="line">        InFlightRequest req = inFlightRequests.completeNext(source);</span><br><span class="line">        <span class="comment">// 解析响应</span></span><br><span class="line">        AbstractResponse body = parseResponse(receive.payload(), req.header);</span><br><span class="line">        log.trace(<span class="string">"Completed receive from node {}, for key {}, received {}"</span>, req.destination, req.header.apiKey(), body);</span><br><span class="line">        <span class="keyword">if</span> (req.isInternalRequest &amp;&amp; body <span class="keyword">instanceof</span> MetadataResponse) {</span><br><span class="line">            <span class="comment">// 如果是更新集群元数据对应的响应，则更新本地的缓存的集群元数据信息</span></span><br><span class="line">            metadataUpdater.handleCompletedMetadataResponse(req.header, now, (MetadataResponse) body);</span><br><span class="line">        } <span class="keyword">else</span> <span class="keyword">if</span> (req.isInternalRequest &amp;&amp; body <span class="keyword">instanceof</span> ApiVersionsResponse) {</span><br><span class="line">            <span class="comment">// 如果是更新 API 版本的响应，则更新本地缓存的目标节点支持的 API 版本信息</span></span><br><span class="line">            <span class="keyword">this</span>.handleApiVersionsResponse(responses, req, now, (ApiVersionsResponse) body);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 否则，获取 ClientResponse 响应对象添加到队列中</span></span><br><span class="line">            responses.add(req.completed(body, now));</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>如果当前是针对之前请求更新集群元数据信息的响应，则会调用 <code>DefaultMetadataUpdater#handleCompletedMetadataResponse</code> 方法解析响应内容，如果响应正常则会调用 <code>Metadata#update</code> 方法更新本地缓存的集群元数据信息。如果当前是针对请求更新本地 API 版本信息的响应，则会调用 <code>NetworkClient#handleApiVersionsResponse</code> 方法更新本地缓存的目标节点支持的 API 版本信息。对于其它类型的响应，则直接封装成 ClientResponse 对象添加到结果集合中。</p>
<ul>
<li><strong>handleDisconnections</strong></li>
</ul>
<p>该方法会调用 <code>Selector#disconnected</code> 方法获取断开连接的节点 ID 集合，并更新相应节点的连接状态为 <code>DISCONNECTED</code>，同时会清空本地缓存的与该节点相关的数据，最终创建一个 disconnected 类型的 ClientResponse 对象添加到结果集合中。如果这一步确实发现了已断开的连接，则标记需要更新本地缓存的节点元数据信息。</p>
<ul>
<li><strong>handleConnections</strong></li>
</ul>
<p>该方法会调用 <code>Selector#connected</code> 方法获取连接正常的节点 ID 集合，如果当前节点是第一次建立连接，则需要获取节点支持的 API 版本信息，方法会将当前节点的连接状态设置为 <code>CHECKING_API_VERSIONS</code>，并将节点 ID 添加到 <code>NetworkClient#nodesNeedingApiVersionsFetch</code> 集合中，对于其它节点，则更新相应连接状态为 <code>READY</code>。</p>
<ul>
<li><strong>handleInitiateApiVersionRequests</strong></li>
</ul>
<p>该方法用于处理 <code>NetworkClient#handleConnections</code> 方法中标记的需要获取支持的 API 版本信息的节点，即记录到 <code>NetworkClient#nodesNeedingApiVersionsFetch</code> 集合中的节点。方法会遍历处理集合中的节点，并在判断目标节点允许接收请求的情况下，构建 ApiVersionsRequest 请求以获取目标节点支持的 API 版本信息，该请求会被包装成 ClientRequest 对象，并在下次 <code>Selector#poll</code> 操作时一并送出。</p>
<ul>
<li><strong>handleTimedOutRequests</strong></li>
</ul>
<p>该方法会遍历缓存在 inFlightRequests 中已经超时的相关请求对应的节点集合，针对此类节点将其视作断开连接进行处理。方法会创建一个 disconnected 类型的 ClientResponse 对象添加到结果集合中，并标记需要更新本地缓存的集群元数据信息。</p>
<p>在完成了将各种类型请求对应的响应对象 ClientResponse 添加到结果集合中之后，会继续遍历该集合并应用 <code>ClientResponse#onComplete</code> 方法，该方法最终调用的是我们注册的 RequestCompletionHandler 对应的 <code>RequestCompletionHandler#onComplete</code> 方法。我们在分析 <code>Sender#sendProduceRequest</code> 方法时曾遇到过下面这一段代码：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RequestCompletionHandler callback = <span class="keyword">new</span> RequestCompletionHandler() {</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(ClientResponse response)</span> </span>{</span><br><span class="line">        handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></div></figure>
<p>实际上在调用 <code>ClientResponse#onComplete</code> 方法时本质上也就是在调用 <code>Sender#handleProduceResponse</code> 方法，该方法所做的工作就是区分当前的响应类型，并针对每一种响应类型设置对应的参数并回调 <code>Sender#completeBatch</code> 方法，区别仅在于方法的 response 参数设置：</p>
<ul>
<li>如果是 disconnected 类型的响应，则设置 <code>response=new ProduceResponse.PartitionResponse(Errors.NETWORK_EXCEPTION)</code>。</li>
<li>如果是 API 版本不匹配的响应，则设置 <code>response=new ProduceResponse.PartitionResponse(Errors.INVALID_REQUEST)</code>。</li>
<li>对于其它响应类型，如果存在响应体则以响应体作为 response 参数；如果不存在响应体则设置 <code>response=new ProduceResponse.PartitionResponse(Errors.NONE)</code>。</li>
</ul>
<p>下面来看一下 <code>Sender#completeBatch</code> 方法的具体实现：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">completeBatch</span><span class="params">(RecordBatch batch, ProduceResponse.PartitionResponse response, <span class="keyword">long</span> correlationId, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    Errors error = response.error;</span><br><span class="line">    <span class="comment">// 异常响应，但是允许重试</span></span><br><span class="line">    <span class="keyword">if</span> (error != Errors.NONE &amp;&amp; <span class="keyword">this</span>.canRetry(batch, error)) {</span><br><span class="line">        log.warn(<span class="string">"Got error produce response with correlation id {} on topic-partition {}, retrying ({} attempts left). Error: {}"</span>, correlationId, batch.topicPartition, retries - batch.attempts - <span class="number">1</span>, error);</span><br><span class="line">        <span class="comment">// 将消息重新添加到收集器中，等待再次发送</span></span><br><span class="line">        <span class="keyword">this</span>.accumulator.reenqueue(batch, now);</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 正常响应，或不允许重试的异常</span></span><br><span class="line">    <span class="keyword">else</span> {</span><br><span class="line">        RuntimeException exception;</span><br><span class="line">        <span class="keyword">if</span> (error == Errors.TOPIC_AUTHORIZATION_FAILED) {</span><br><span class="line">            <span class="comment">// 权限认证失败</span></span><br><span class="line">            exception = <span class="keyword">new</span> TopicAuthorizationException(batch.topicPartition.topic());</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 其他异常，如果是正常响应，则为 null</span></span><br><span class="line">            exception = error.exception();</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 将响应信息传递给用户，并释放 RecordBatch 占用的空间</span></span><br><span class="line">        batch.done(response.baseOffset, response.logAppendTime, exception);</span><br><span class="line">        <span class="keyword">this</span>.accumulator.deallocate(batch);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果是集群元数据异常，则标记需要更新集群元数据信息</span></span><br><span class="line">    <span class="keyword">if</span> (error.exception() <span class="keyword">instanceof</span> InvalidMetadataException) {</span><br><span class="line">        metadata.requestUpdate();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放已经处理完成的 topic 分区，对于需要保证消息强顺序性，以允许接收下一条消息</span></span><br><span class="line">    <span class="keyword">if</span> (guaranteeMessageOrder) {</span><br><span class="line">        <span class="keyword">this</span>.accumulator.unmutePartition(batch.topicPartition);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>上述方法会判断当前响应是否异常且可以需要重试，如果是则将 RecordBatch 重新添加到收集器 RecordAccumulator 中，等待再次发送。如果是正常响应或不允许重试，则调用 <code>RecordBatch#done</code> 方法结束本次发送消息的过程，并将响应结果传递给用户，同时释放 RecordBatch 占用的空间。下面来看一下方法 <code>RecordBatch#done</code> 的实现：</p>
<figure class="highlight java"><div class="table-container"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">done</span><span class="params">(<span class="keyword">long</span> baseOffset, <span class="keyword">long</span> logAppendTime, RuntimeException exception)</span> </span>{</span><br><span class="line">    log.trace(<span class="string">"Produced messages to topic-partition {} with base offset offset {} and error: {}."</span>, topicPartition, baseOffset, exception);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 标识当前 RecordBatch 已经处理完成</span></span><br><span class="line">    <span class="keyword">if</span> (completed.getAndSet(<span class="keyword">true</span>)) {</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Batch has already been completed"</span>);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置当前 RecordBatch 发送之后的状态</span></span><br><span class="line">    produceFuture.set(baseOffset, logAppendTime, exception);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 循环执行每个消息的 Callback 回调</span></span><br><span class="line">    <span class="keyword">for</span> (Thunk thunk : thunks) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// 消息处理正常</span></span><br><span class="line">            <span class="keyword">if</span> (exception == <span class="keyword">null</span>) {</span><br><span class="line">                <span class="comment">// RecordMetadata 是服务端返回的</span></span><br><span class="line">                RecordMetadata metadata = thunk.future.value();</span><br><span class="line">                thunk.callback.onCompletion(metadata, <span class="keyword">null</span>);</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// 消息处理异常</span></span><br><span class="line">            <span class="keyword">else</span> {</span><br><span class="line">                thunk.callback.onCompletion(<span class="keyword">null</span>, exception);</span><br><span class="line">            }</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Error executing user-provided callback on message for topic-partition '{}'"</span>, topicPartition, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 标记本次请求已经完成（正常响应、超时，以及关闭生产者）</span></span><br><span class="line">    produceFuture.done();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></div></figure>
<p>前面我们曾分析过在消息追加成功之后，如果在发送消息时指定了 Callback 回调函数，会将其封装成 Thunk 类对象，当时我们猜测 Sender 线程在向集群投递完消息并收到来自集群的响应时会循环遍历 thunks 集合，并应用 Callback 相应的回调方法，而上述方法的实现证实了我们的猜想。</p>
<p>方法中的变量 produceFuture 是一个 ProduceRequestResult 类型的对象，用于表示一次消息生产过程是否完成，该类基于 CountDownLatch 实现了类似 Future 的功能，在构造 ProduceRequestResult 对象时会创建一个大小为 1 的 CountDownLatch 对象，并在调用 <code>ProduceRequestResult#done</code> 方法时执行 <code>CountDownLatch#countDown</code> 操作，而 <code>ProduceRequestResult#completed</code> 方法判定消息发送是否完成的依据就是判定 CountDownLatch 对象值是否等于 0。</p>

        <h3 id="总结">
          <a href="#总结" class="heading-link"><i class="fas fa-link"></i></a>总结</h3>
      <p>本文我们介绍了 java 版本的 KafkaProducer 的使用，并深入分析了相关设计和实现。从执行流程上来说，Kafka 生产者运行机制在整体设计上还是比较简单和直观的，但不可否认在实现上也有很多需要注意的细节。Kafka 在老版本的 SDK 中默认使用同步的方式往服务端投递消息，因为采用异步的方式存在消息丢失的问题，直到 0.8.2.0 版本以后才修复了这一问题，并将异步提交设置为默认方式。</p>
<p>了解 KafkaProducer 的设计和实现能够帮助我们在实际开发中更好的使用 Kafka 生产者客户端，知晓如何能够保证消息的强顺序性，以及如何保证消息不丢失，甚至是利用其它编程语言自定义 SDK。下一篇，我们将继续分析消费者的运行机制。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="https://plotor.github.io">zhenchao</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="https://plotor.github.io/2019/06/18/kafka/kafka-producer/">https://plotor.github.io/2019/06/18/kafka/kafka-producer/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://plotor.github.io/tags/Kafka/">Kafka</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2019/06/19/kafka/kafka-consumer/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Kafka 源码解析：消费者运行机制</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2019/06/17/kafka/kafka-architecture/"><span class="paginator-prev__text">Kafka 源码解析：架构与核心概念</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div><div class="comments" id="comments"><div id="utterances-container"></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#KafkaProducer-%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.</span> <span class="toc-text">
          KafkaProducer 使用示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E6%94%B6%E9%9B%86%E4%B8%8E%E5%8F%91%E9%80%81%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90"><span class="toc-number">2.</span> <span class="toc-text">
          消息收集与发送过程分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B6%E9%9B%86%E5%BE%85%E5%8F%91%E9%80%81%E7%9A%84%E6%B6%88%E6%81%AF"><span class="toc-number">2.1.</span> <span class="toc-text">
          收集待发送的消息</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#KafkaProducer-%E7%9A%84%E5%AD%97%E6%AE%B5%E5%AE%9A%E4%B9%89%E4%B8%8E%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95"><span class="toc-number">2.1.1.</span> <span class="toc-text">
          KafkaProducer 的字段定义与构造方法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E6%94%B6%E9%9B%86%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">2.1.2.</span> <span class="toc-text">
          消息收集的过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%95%E9%80%92%E5%BE%85%E5%8F%91%E9%80%81%E7%9A%84%E6%B6%88%E6%81%AF"><span class="toc-number">2.2.</span> <span class="toc-text">
          投递待发送的消息</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.</span> <span class="toc-text">
          总结</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/author.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">追求技术深度，注重文章质量</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/plotor" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://weibo.com/" target="_blank" rel="noopener" data-popover="微博" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weibo"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="微信" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weixin"></i></span></a><a class="sidebar-ov-social-item" href="null" target="_blank" rel="noopener" data-popover="QQ" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-qq"></i></span></a><a class="sidebar-ov-social-item" href="https://twitter.com/" target="_blank" rel="noopener" data-popover="Twitter" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-twitter"></i></span></a></div><div class="sidebar-ov-feed"><span class="sidebar-ov-feed-rss"><a class="sidebar-ov-feed-rss__link" href="/atom.xml" target="_blank" rel="noopener"><span class="sidebar-ov-feed-rss__icon"><i class="fas fa-rss"></i></span><span>RSS 订阅</span></a></span></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">95</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">13</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">27</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2015~2024</span><span class="footer__devider"></span><span>Zhenchao All Rights Reserved</span><span class="footer__devider">|</span><span>浙ICP备 16010916 号</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.3.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/ribbon.js@latest/dist/ribbon.min.js" size="120" alpha="0.6" zIndex="-1"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script>function initSearch() {
  var isXML = true;
  var search_path = 'search.json';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;
  $.ajax({
    url: path,
    dataType: isXML ? 'xml' : 'json',
    async: true,
    success: function (res) {
      var datas = isXML ? $('entry', res).map(function () {
        // 将 XML 转为 JSON
        return {
          title: $('title', this).text(),
          content: $('content', this).text(),
          url: $('url', this).text()
        };
      }).get() : res;
      var $input = $('.search-input input');
      var $result = $('.search-results');
      // 搜索对象（标题、内容）的权重，影响显示顺序
      var WEIGHT = { title: 100, content: 1 };
      var searchPost = function () {
        var searchText = $input.val().toLowerCase().trim();
        // 根据空白字符分隔关键字
        var keywords = searchText.split(/[\s]+/);
        // 搜索结果
        var matchPosts = [];

        // 有多个关键字时，将原文字整个保存下来
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        // 防止未输入字符时搜索
        if (searchText.length > 0) {
          datas.forEach(function (data) {
            var isMatch  = false;
            // 没有标题的文章使用预设的 i18n 变量代替
            var title = (data.title && data.title.trim()) || '[ 文章无标题 ]';
            var titleLower = title && title.toLowerCase();
            // 删除 HTML 标签 和 所有空白字符
            var content = data.content && data.content.replace(/<[^>]+>/g, '');
            var contentLower = content && content.toLowerCase();
            // 删除重复的 /
            var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');
            // 标题中匹配到的关键词
            var titleHitSlice = [];
            // 内容中匹配到的关键词
            var contentHitSlice = [];

            keywords.forEach(function (keyword) {
              /**
              * 获取匹配的关键词的索引
              * @param {String} keyword 要匹配的关键字
              * @param {String} text 原文字
              * @param {Boolean} caseSensitive 是否区分大小写
              * @param {Number} weight 匹配对象的权重。权重大的优先显示
              * @return {Array}
              */
              function getIndexByword (word, text, caseSensitive, weight) {
                if (!word || !text) {
                  return [];
                };

                var startIndex = 0; // 每次匹配的开始索引
                var index = -1;     // 匹配到的索引值
                var result = [];    // 匹配结果

                if (!caseSensitive) {
                  word = word.toLowerCase();
                  text = text.toLowerCase();
                }

                while((index = text.indexOf(word, startIndex)) !== -1) {
                  var hasMatch = false;
                  // 索引位置相同的关键词，保留长度较长的
                  titleHitSlice.forEach(function (hit) {
                    if (hit.index === index && hit.word.length < word.length) {
                      hit.word = word;
                      hasMatch = true;
                    }
                  });
                  startIndex = index + word.length;
                  !hasMatch && result.push({ index: index, word: word, weight: weight });
                }
                return result;
              }
              titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
              contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
            });

            var hitTitle = titleHitSlice.length;
            var hitContent = contentHitSlice.length;

            if (hitTitle > 0 || hitContent > 0) {
              isMatch = true;
            }
            if (isMatch) {
              ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                // 按照匹配文字的索引的递增顺序排序
                hit.sort(function (left, right) {
                  return left.index - right.index;
                });
              });
              /**
              * 给文本中匹配到的关键词添加标记，从而进行高亮显示
              * @param {String} text 原文本
              * @param {Array} hitSlice 匹配项的索引信息
              * @param {Number} start 开始索引
              * @param {Number} end 结束索引
              * @return {String}
              */
              function highlightKeyword (text, hitSlice, start, end) {
                if (!text || !hitSlice || !hitSlice.length) {
                  return;
                }

                var result = '';
                var startIndex = start;
                var endIndex = end;
                hitSlice.forEach(function (hit) {
                  if (hit.index < startIndex) {
                    return;
                  }

                  var hitWordEnd = hit.index + hit.word.length;
                  result += text.slice(startIndex, hit.index);
                  result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                  startIndex = hitWordEnd;
                });
                result += text.slice(startIndex, endIndex);
                return result;
              }

              var postData = {};
              // 文章总的搜索权重
              var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
              // 标记匹配关键词后的标题
              var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
              // 标记匹配关键词后的内容
              var postContent;
              // 显示内容的长度
              var SHOW_WORD_LENGTH = 200;
              // 命中关键词前的字符显示长度
              var SHOW_WORD_FRONT_LENGTH = 20;
              var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

              // 截取匹配的第一个字符，前后共 200 个字符来显示
              if (contentHitSlice.length > 0) {
                var firstIndex = contentHitSlice[0].index;
                var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                var end = firstIndex + SHOW_WORD_END_LENGTH;
                postContent = highlightKeyword(content, contentHitSlice, start, end);
              } else { // 未匹配到内容，直接截取前 200 个字符来显示
                postContent = content.slice(0, SHOW_WORD_LENGTH);
              }
              postData.title = postTitle;
              postData.content = postContent;
              postData.url = postURL;
              postData.weight = postWeight;
              matchPosts.push(postData);
            }
          });
        }

        var resultInnerHtml = '';
        if (matchPosts.length) {
          // 按权重递增的顺序排序，使权重大的优先显示
          matchPosts.sort(function (left, right) {
            return right.weight - left.weight;
          });
          resultInnerHtml += '<ul>';
          matchPosts.forEach(function (post) {
            resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
            resultInnerHtml += post.title;
            resultInnerHtml += '</a><div class="search-results-content">';
            resultInnerHtml += post.content;
            resultInnerHtml += '</div></li>';
          });
          resultInnerHtml += '</ul>';
        } else {
          resultInnerHtml += '<div class="search-results-none"><i class="far fa-meh"></i></div>';
        }
        $result.html(resultInnerHtml);
      };
      $input.on('input', searchPost);
      $input.on('keyup', function (e) {
        if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
          searchPost();
        }
      });
    }
  });
}

function closeSearch () {
  $('body').css({ overflow: 'auto' });
  $('.search-popup').css({ display: 'none' });
  $('.search-mask').css({ display: 'none' });
}

window.addEventListener('DOMContentLoaded', function () {
  Stun.utils.pjaxReloadLocalSearch = function () {
    $('.header-nav-search').on('click', function (e) {
      e.stopPropagation();
      $('body').css('overflow', 'hidden');
      $('.search-popup')
        .velocity('stop')
        .velocity('transition.expandIn', {
          duration: 300,
          complete: function () {
            $('.search-popup input').focus();
          }
        });
      $('.search-mask')
        .velocity('stop')
        .velocity('transition.fadeIn', {
          duration: 300
        });

      initSearch();
    });
    $('.search-mask, .search-close').on('click', function () {
      closeSearch();
    });
    $(document).on('keydown', function (e) {
      // Escape <=> 27
      if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
        closeSearch();
      }
    });
  };

  Stun.utils.pjaxReloadLocalSearch();
}, false);

function safeOpenUrl(url) {
  var newTab = window.open();
  newTab.opener = null;
  newTab.location = url;
}

function extSearch(engine) {
  var engines = {
    google: 'https://www.google.com/search?q=',
    bing: 'https://cn.bing.com/search?q=',
    baidu: 'https://www.baidu.com/s?ie=UTF-8&wd=',
  };
  var host = window.location.host;
  var query = $('.search-input input').val().toLowerCase().trim();
  var uri = engines[engine] + query + ' site:' + host;

  if (query) {
    safeOpenUrl(uri);
  } else {
    Stun.utils.popAlert('warning', '请输入字符');
  }
}

var assistSearchList = window.CONFIG.assistSearch;

if (Array.isArray(assistSearchList)) {
  assistSearchList.forEach(function (name) {
    document.querySelector('.search-btns-item--' + name).addEventListener('click', function () {
      extSearch(name);
    }, false);
  });
}</script><script src="https://cdn.jsdelivr.net/npm/pjax@latest/pjax.min.js"></script><script>window.addEventListener('DOMContentLoaded', function () {
  var pjax = new Pjax({"selectors":["head title","#main",".pjax-reload",".header-inner"],"history":true,"scrollTo":false,"scrollRestoration":false,"cacheBust":false,"debug":false,"currentUrlFullReload":false,"timeout":0});
  // 加载进度条的计时器
  var loadingTimer = null;

  // 重置页面 Y 方向上的滚动偏移量
  document.addEventListener('pjax:send', function () {
    $('.header-nav-menu').removeClass('show');
    if (CONFIG.pjax && CONFIG.pjax.avoidBanner) {
      $('html').velocity('scroll', {
        duration: 500,
        offset: $('#header').height(),
        easing: 'easeInOutCubic'
      });
    }

    var loadingBarWidth = 20;
    var MAX_LOADING_WIDTH = 95;

    $('.loading-bar').addClass('loading');
    $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    clearInterval(loadingTimer);
    loadingTimer = setInterval(function () {
      loadingBarWidth += 3;
      if (loadingBarWidth > MAX_LOADING_WIDTH) {
        loadingBarWidth = MAX_LOADING_WIDTH;
      }
      $('.loading-bar__progress').css('width', loadingBarWidth + '%');
    }, 500);
  }, false);

  window.addEventListener('pjax:complete', function () {
    clearInterval(loadingTimer);
    $('.loading-bar__progress').css('width', '100%');
    $('.loading-bar').removeClass('loading');
    setTimeout(function () {
      $('.loading-bar__progress').css('width', '0');
    }, 400);
    $('link[rel=prefetch], script[data-pjax-rm]').each(function () {
      $(this).remove();
    });
    $('script[data-pjax], #pjax-reload script').each(function () {
      $(this).parent().append($(this).remove());
    });

    if (Stun.utils.pjaxReloadBoot) {
      Stun.utils.pjaxReloadBoot();
    }
    if (Stun.utils.pjaxReloadScroll) {
      Stun.utils.pjaxReloadScroll();
    }
    if (Stun.utils.pjaxReloadSidebar) {
      Stun.utils.pjaxReloadSidebar();
    }
    if (true) {
      if (Stun.utils.pjaxReloadHeader) {
        Stun.utils.pjaxReloadHeader();
      }
      if (Stun.utils.pjaxReloadScrollIcon) {
        Stun.utils.pjaxReloadScrollIcon();
      }
      if (Stun.utils.pjaxReloadLocalSearch) {
        Stun.utils.pjaxReloadLocalSearch();
      }
    }
  }, false);
}, false);</script><div id="pjax-reload"></div><script data-pjax="">function loadUtterances() {
  var d = document, s = d.createElement('script');
  var container = d.getElementById('utterances-container');

  if (!container) {
    return;
  }
  s.src = 'https://utteranc.es/client.js';
  s.setAttribute('repo', 'plotor/hexo-comments');
  s.setAttribute('issue-term', 'title');
  s.setAttribute('label', 'utterances');
  s.setAttribute('theme', 'github-light');
  s.setAttribute('crossorigin', 'anonymous');
  s.setAttribute('async', '');
  if (true) {
    s.setAttribute('data-pjax-rm', '');
  }
  container.append(s);
}

if (true) {
  loadUtterances();
} else {
  window.addEventListener('DOMContentLoaded', loadUtterances, false);
}</script><script src="/js/utils.js?v=2.6.1"></script><script src="/js/stun-boot.js?v=2.6.1"></script><script src="/js/scroll.js?v=2.6.1"></script><script src="/js/header.js?v=2.6.1"></script><script src="/js/sidebar.js?v=2.6.1"></script><script type="application/json" src="/search.json"></script></body></html>